{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK3alCdFflQX"
      },
      "source": [
        "### CNN on CIFR Assignment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHCYMwwXflQd"
      },
      "source": [
        "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
        "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
        "3.  You cannot use DropOut layers.\n",
        "4.  You MUST use Image Augmentation Techniques.\n",
        "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
        "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
        "7.  You cannot use test images for training the model.\n",
        "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
        "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
        "10. You cannot have more than 1 Million parameters in total\n",
        "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
        "12. You can use any optimization algorithm you need. \n",
        "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLVcyNYKflQi"
      },
      "source": [
        "# Load necessary libraries\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from numpy import expand_dims\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "l = 40\n",
        "num_filter = 12\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2"
      ],
      "metadata": {
        "id": "b2crE_2q8ha3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "\n",
        "(X_train, y_train),(X_test ,y_test ) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5hQ9IMc9De-",
        "outputId": "b140ee31-1602-4f44-ffb5-29ac8b98da90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHECKING THE SHAPE OF DATA WE HAVE LOADED**"
      ],
      "metadata": {
        "id": "6EsynAV9Vw3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape,X_test.shape , X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npCZ-Mwm9Dh4",
        "outputId": "816628f2-930f-4383-b867-be06d735cd12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 32, 32, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **converting image pixel in the range of 0 to 1**"
      ],
      "metadata": {
        "id": "YjSLWMsG_QpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_pixels(train, test):\n",
        "    '''\n",
        "    Normalize data into range of 0 to 1\n",
        "    '''\n",
        "    return train.astype('float32')/255, test.astype('float32')/255"
      ],
      "metadata": {
        "id": "iLIYFrVP9DlQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# normalizing the pixel values "
      ],
      "metadata": {
        "id": "4V9-Xs8nFGhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test=normalize_pixels(X_train,X_test)\n"
      ],
      "metadata": {
        "id": "uAC3cCUt9DoW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first we will try the model with dense layer"
      ],
      "metadata": {
        "id": "xmHa25Ny_rWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
        "def model_summarize(history):\n",
        "    '''\n",
        "    Summarize model i.e. print train and test loss\n",
        "    '''\n",
        "    # plot loss\n",
        "    pyplot.subplot(125)\n",
        "    pyplot.title('Cross Entropy Loss')\n",
        "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    pyplot.show()\n"
      ],
      "metadata": {
        "id": "RQSyIkCC9Dr9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_z6UNKTTKtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denseblock(input, num_filter = 64, dropout_rate = 0):\n",
        "    '''\n",
        "    Create dense block\n",
        "    '''\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_5_5 = layers.Conv2D(int(num_filter*compression), (5,5),kernel_initializer=\"he_uniform\" ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_5_5 = layers.Dropout(dropout_rate)(Conv2D_5_5)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_5_5])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "def transition(input, num_filter = 32, dropout_rate = 0):\n",
        "    '''\n",
        "    Create transition block\n",
        "    '''\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (5,5), kernel_initializer=\"he_uniform\" ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        "\n",
        "def output_layer(input):\n",
        "    '''\n",
        "    define output layer\n",
        "    '''\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "_NrAGeKgSwSa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_filter = 12\n",
        "dropout_rate = 0\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(32, (5,5), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D,10, dropout_rate)\n",
        "First_Transition = transition(First_Block, 64, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, 10, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, 32, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, 32, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "metadata": {
        "id": "GXqlZDW4SwU2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al9UCSn7SwXm",
        "outputId": "8c24d906-bc86-4aa6-fe03-0cefe027bece"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 32)   2400        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 5)    4005        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 37)   0           ['conv2d[0][0]',                 \n",
            "                                                                  'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 37)  148         ['concatenate[0][0]']            \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 37)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 5)    4630        ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32, 32, 42)   0           ['concatenate[0][0]',            \n",
            "                                                                  'conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 42)  168         ['concatenate_1[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 42)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 5)    5255        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 32, 32, 47)   0           ['concatenate_1[0][0]',          \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 47)  188         ['concatenate_2[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 47)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 5)    5880        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 32, 32, 52)   0           ['concatenate_2[0][0]',          \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 52)  208         ['concatenate_3[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 52)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 5)    6505        ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 32, 32, 57)   0           ['concatenate_3[0][0]',          \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 57)  228         ['concatenate_4[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 57)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 5)    7130        ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 32, 62)   0           ['concatenate_4[0][0]',          \n",
            "                                                                  'conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 62)  248         ['concatenate_5[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 62)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 5)    7755        ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 32, 32, 67)   0           ['concatenate_5[0][0]',          \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 67)  268         ['concatenate_6[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 67)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 5)    8380        ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 32, 32, 72)   0           ['concatenate_6[0][0]',          \n",
            "                                                                  'conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 32, 72)  288         ['concatenate_7[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 32, 32, 72)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 5)    9005        ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 32, 32, 77)   0           ['concatenate_7[0][0]',          \n",
            "                                                                  'conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 32, 77)  308         ['concatenate_8[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 32, 32, 77)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 5)    9630        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 32, 32, 82)   0           ['concatenate_8[0][0]',          \n",
            "                                                                  'conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 32, 82)  328         ['concatenate_9[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32, 32, 82)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 5)    10255       ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 32, 32, 87)   0           ['concatenate_9[0][0]',          \n",
            "                                                                  'conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32, 32, 87)  348         ['concatenate_10[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 32, 32, 87)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 5)    10880       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 32, 32, 92)   0           ['concatenate_10[0][0]',         \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 32, 32, 92)  368         ['concatenate_11[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 32, 32, 92)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 32)   73632       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 16, 16, 32)  0           ['conv2d_13[0][0]']              \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 32)  128         ['average_pooling2d[0][0]']      \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 5)    4005        ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 16, 16, 37)   0           ['average_pooling2d[0][0]',      \n",
            "                                                                  'conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 37)  148         ['concatenate_12[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 37)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 5)    4630        ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 16, 16, 42)   0           ['concatenate_12[0][0]',         \n",
            "                                                                  'conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 42)  168         ['concatenate_13[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 16, 16, 42)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 5)    5255        ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 16, 16, 47)   0           ['concatenate_13[0][0]',         \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 47)  188         ['concatenate_14[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 16, 16, 47)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 5)    5880        ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 16, 16, 52)   0           ['concatenate_14[0][0]',         \n",
            "                                                                  'conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 52)  208         ['concatenate_15[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 16, 16, 52)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 5)    6505        ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 16, 16, 57)   0           ['concatenate_15[0][0]',         \n",
            "                                                                  'conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 16, 16, 57)  228         ['concatenate_16[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 16, 16, 57)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 5)    7130        ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 16, 16, 62)   0           ['concatenate_16[0][0]',         \n",
            "                                                                  'conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16, 16, 62)  248         ['concatenate_17[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 16, 16, 62)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 5)    7755        ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 16, 16, 67)   0           ['concatenate_17[0][0]',         \n",
            "                                                                  'conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 16, 16, 67)  268         ['concatenate_18[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 16, 16, 67)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 5)    8380        ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_18[0][0]',         \n",
            "                                                                  'conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 16, 16, 72)  288         ['concatenate_19[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 5)    9005        ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 16, 16, 77)   0           ['concatenate_19[0][0]',         \n",
            "                                                                  'conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 16, 16, 77)  308         ['concatenate_20[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 16, 16, 77)   0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 16, 16, 5)    9630        ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 16, 16, 82)   0           ['concatenate_20[0][0]',         \n",
            "                                                                  'conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 16, 16, 82)  328         ['concatenate_21[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 16, 16, 82)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 16, 16, 5)    10255       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 16, 16, 87)   0           ['concatenate_21[0][0]',         \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 16, 16, 87)  348         ['concatenate_22[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 16, 16, 87)   0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 16, 16, 5)    10880       ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 16, 16, 92)   0           ['concatenate_22[0][0]',         \n",
            "                                                                  'conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 16, 16, 92)  368         ['concatenate_23[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 16, 16, 92)   0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 16, 16, 16)   36816       ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 8, 8, 16)    0           ['conv2d_26[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 8, 8, 16)    64          ['average_pooling2d_1[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 8, 8, 16)     0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 8, 8, 6)      2406        ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 8, 8, 22)     0           ['average_pooling2d_1[0][0]',    \n",
            "                                                                  'conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 8, 8, 22)    88          ['concatenate_24[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 8, 8, 22)     0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 8, 8, 6)      3306        ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 8, 8, 28)     0           ['concatenate_24[0][0]',         \n",
            "                                                                  'conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 8, 8, 28)    112         ['concatenate_25[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 8, 8, 28)     0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 8, 8, 6)      4206        ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, 8, 8, 34)     0           ['concatenate_25[0][0]',         \n",
            "                                                                  'conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 8, 8, 34)    136         ['concatenate_26[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 8, 8, 34)     0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 8, 8, 6)      5106        ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenate)   (None, 8, 8, 40)     0           ['concatenate_26[0][0]',         \n",
            "                                                                  'conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 8, 8, 40)    160         ['concatenate_27[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 8, 8, 40)     0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 8, 8, 6)      6006        ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenate)   (None, 8, 8, 46)     0           ['concatenate_27[0][0]',         \n",
            "                                                                  'conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 8, 8, 46)    184         ['concatenate_28[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 8, 8, 46)     0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 8, 8, 6)      6906        ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_29 (Concatenate)   (None, 8, 8, 52)     0           ['concatenate_28[0][0]',         \n",
            "                                                                  'conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 8, 8, 52)    208         ['concatenate_29[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 8, 8, 52)     0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 8, 8, 6)      7806        ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenate)   (None, 8, 8, 58)     0           ['concatenate_29[0][0]',         \n",
            "                                                                  'conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 8, 8, 58)    232         ['concatenate_30[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 8, 8, 58)     0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 8, 8, 6)      8706        ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenate)   (None, 8, 8, 64)     0           ['concatenate_30[0][0]',         \n",
            "                                                                  'conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 8, 8, 64)    256         ['concatenate_31[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 8, 8, 6)      9606        ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenate)   (None, 8, 8, 70)     0           ['concatenate_31[0][0]',         \n",
            "                                                                  'conv2d_35[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 8, 8, 70)    280         ['concatenate_32[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 8, 8, 70)     0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 8, 8, 6)      10506       ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenate)   (None, 8, 8, 76)     0           ['concatenate_32[0][0]',         \n",
            "                                                                  'conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 8, 8, 76)    304         ['concatenate_33[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 8, 8, 76)     0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 8, 8, 6)      11406       ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenate)   (None, 8, 8, 82)     0           ['concatenate_33[0][0]',         \n",
            "                                                                  'conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 8, 8, 82)    328         ['concatenate_34[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 8, 8, 82)     0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 8, 8, 6)      12306       ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenate)   (None, 8, 8, 88)     0           ['concatenate_34[0][0]',         \n",
            "                                                                  'conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 8, 8, 88)    352         ['concatenate_35[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 8, 8, 88)     0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 8, 8, 16)     35216       ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 4, 4, 16)    0           ['conv2d_39[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 4, 4, 16)    64          ['average_pooling2d_2[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 4, 4, 16)     0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 4, 4, 6)      2406        ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 4, 4, 22)     0           ['average_pooling2d_2[0][0]',    \n",
            "                                                                  'conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 4, 4, 22)    88          ['concatenate_36[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 4, 4, 22)     0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 4, 4, 6)      3306        ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 4, 4, 28)     0           ['concatenate_36[0][0]',         \n",
            "                                                                  'conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 4, 4, 28)    112         ['concatenate_37[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 4, 4, 28)     0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 4, 4, 6)      4206        ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 4, 4, 34)     0           ['concatenate_37[0][0]',         \n",
            "                                                                  'conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 4, 4, 34)    136         ['concatenate_38[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 4, 4, 34)     0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 4, 4, 6)      5106        ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 4, 4, 40)     0           ['concatenate_38[0][0]',         \n",
            "                                                                  'conv2d_43[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 4, 4, 40)    160         ['concatenate_39[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 4, 4, 40)     0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 4, 4, 6)      6006        ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 4, 4, 46)     0           ['concatenate_39[0][0]',         \n",
            "                                                                  'conv2d_44[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 4, 4, 46)    184         ['concatenate_40[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 4, 4, 46)     0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 4, 4, 6)      6906        ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 4, 4, 52)     0           ['concatenate_40[0][0]',         \n",
            "                                                                  'conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 4, 4, 52)    208         ['concatenate_41[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 4, 4, 52)     0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 4, 4, 6)      7806        ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 4, 4, 58)     0           ['concatenate_41[0][0]',         \n",
            "                                                                  'conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 4, 4, 58)    232         ['concatenate_42[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 4, 4, 58)     0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 4, 4, 6)      8706        ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 4, 4, 64)     0           ['concatenate_42[0][0]',         \n",
            "                                                                  'conv2d_47[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 4, 4, 64)    256         ['concatenate_43[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 4, 4, 64)     0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 4, 4, 6)      9606        ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 4, 4, 70)     0           ['concatenate_43[0][0]',         \n",
            "                                                                  'conv2d_48[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 4, 4, 70)    280         ['concatenate_44[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 4, 4, 70)     0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 4, 4, 6)      10506       ['activation_48[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 4, 4, 76)     0           ['concatenate_44[0][0]',         \n",
            "                                                                  'conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 4, 4, 76)    304         ['concatenate_45[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 4, 4, 76)     0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 4, 4, 6)      11406       ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 4, 4, 82)     0           ['concatenate_45[0][0]',         \n",
            "                                                                  'conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 4, 4, 82)    328         ['concatenate_46[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 4, 4, 82)     0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 4, 4, 6)      12306       ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_47 (Concatenate)   (None, 4, 4, 88)     0           ['concatenate_46[0][0]',         \n",
            "                                                                  'conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 4, 4, 88)    352         ['concatenate_47[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 4, 4, 88)     0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 2, 2, 88)    0           ['activation_51[0][0]']          \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 352)          0           ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           3530        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 518,614\n",
            "Trainable params: 512,686\n",
            "Non-trainable params: 5,928\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=22,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode = 'nearest',\n",
        "    zoom_range=0.01)\n",
        "data_gen.fit(X_train)"
      ],
      "metadata": {
        "id": "GWskhJP-SwaU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ywq-eF1dSwc4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience= 5,\n",
        "                              min_lr=0.000001)\n",
        "filepath = \"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks = [checkpoint, reduce_lr]"
      ],
      "metadata": {
        "id": "4MqJDgGLTC0c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit_generator(data_gen.flow(X_train, y_train, batch_size=50),\n",
        "                    steps_per_epoch = (len(X_train) /50), epochs=50, validation_data=(X_test, y_test),callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Y5ZMd9TC3P",
        "outputId": "a7ba0fa1-2a1f-4452-d8df-36bf532c6662"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-516e8d78b8f2>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history=model.fit_generator(data_gen.flow(X_train, y_train, batch_size=50),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 1.5547 - accuracy: 0.4281\n",
            "Epoch 1: val_loss improved from -inf to 1.45744, saving model to best_model.hdf5\n",
            "1000/1000 [==============================] - 158s 105ms/step - loss: 1.5547 - accuracy: 0.4281 - val_loss: 1.4574 - val_accuracy: 0.4763 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 1.1627 - accuracy: 0.5808\n",
            "Epoch 2: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 1.1627 - accuracy: 0.5808 - val_loss: 1.0920 - val_accuracy: 0.6103 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.9652 - accuracy: 0.6574\n",
            "Epoch 3: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 103s 103ms/step - loss: 0.9652 - accuracy: 0.6574 - val_loss: 1.0561 - val_accuracy: 0.6337 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.8632 - accuracy: 0.6962\n",
            "Epoch 4: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.8632 - accuracy: 0.6962 - val_loss: 0.9584 - val_accuracy: 0.6866 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.7802 - accuracy: 0.7287\n",
            "Epoch 5: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.7802 - accuracy: 0.7287 - val_loss: 0.8416 - val_accuracy: 0.7108 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.7201 - accuracy: 0.7500\n",
            "Epoch 6: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 106s 106ms/step - loss: 0.7201 - accuracy: 0.7500 - val_loss: 0.7571 - val_accuracy: 0.7435 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.6738 - accuracy: 0.7671\n",
            "Epoch 7: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.6738 - accuracy: 0.7671 - val_loss: 0.6633 - val_accuracy: 0.7716 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.7793\n",
            "Epoch 8: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.6355 - accuracy: 0.7793 - val_loss: 0.7586 - val_accuracy: 0.7421 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.6070 - accuracy: 0.7889\n",
            "Epoch 9: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.6070 - accuracy: 0.7889 - val_loss: 0.6621 - val_accuracy: 0.7661 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.7953\n",
            "Epoch 10: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 100ms/step - loss: 0.5810 - accuracy: 0.7953 - val_loss: 0.6618 - val_accuracy: 0.7759 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5560 - accuracy: 0.8072\n",
            "Epoch 11: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.5560 - accuracy: 0.8072 - val_loss: 0.5572 - val_accuracy: 0.8087 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5309 - accuracy: 0.8151\n",
            "Epoch 12: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 100s 100ms/step - loss: 0.5309 - accuracy: 0.8151 - val_loss: 1.0384 - val_accuracy: 0.6868 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.8207\n",
            "Epoch 13: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 100s 100ms/step - loss: 0.5173 - accuracy: 0.8207 - val_loss: 0.6266 - val_accuracy: 0.7935 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4997 - accuracy: 0.8276\n",
            "Epoch 14: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 106s 106ms/step - loss: 0.4997 - accuracy: 0.8276 - val_loss: 0.5368 - val_accuracy: 0.8172 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4821 - accuracy: 0.8324\n",
            "Epoch 15: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 100ms/step - loss: 0.4821 - accuracy: 0.8324 - val_loss: 0.5886 - val_accuracy: 0.8005 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.8377\n",
            "Epoch 16: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.4660 - accuracy: 0.8377 - val_loss: 0.7507 - val_accuracy: 0.7667 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8420\n",
            "Epoch 17: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 100ms/step - loss: 0.4568 - accuracy: 0.8420 - val_loss: 0.6016 - val_accuracy: 0.8065 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.8461\n",
            "Epoch 18: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.4416 - accuracy: 0.8461 - val_loss: 0.6688 - val_accuracy: 0.7891 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.8514\n",
            "Epoch 19: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.4293 - accuracy: 0.8514 - val_loss: 0.5124 - val_accuracy: 0.8261 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8532\n",
            "Epoch 20: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.4202 - accuracy: 0.8532 - val_loss: 0.4834 - val_accuracy: 0.8344 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.8600\n",
            "Epoch 21: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.4088 - accuracy: 0.8600 - val_loss: 0.6037 - val_accuracy: 0.8076 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4016 - accuracy: 0.8599\n",
            "Epoch 22: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.4016 - accuracy: 0.8599 - val_loss: 0.4820 - val_accuracy: 0.8393 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3916 - accuracy: 0.8634\n",
            "Epoch 23: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.3916 - accuracy: 0.8634 - val_loss: 0.5826 - val_accuracy: 0.8145 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.8674\n",
            "Epoch 24: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.3812 - accuracy: 0.8674 - val_loss: 0.5524 - val_accuracy: 0.8182 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3753 - accuracy: 0.8689\n",
            "Epoch 25: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.3753 - accuracy: 0.8689 - val_loss: 0.4828 - val_accuracy: 0.8402 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8715\n",
            "Epoch 26: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 107s 107ms/step - loss: 0.3678 - accuracy: 0.8715 - val_loss: 0.7035 - val_accuracy: 0.7848 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8734\n",
            "Epoch 27: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.3641 - accuracy: 0.8734 - val_loss: 0.5080 - val_accuracy: 0.8296 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8948\n",
            "Epoch 28: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.3011 - accuracy: 0.8948 - val_loss: 0.3875 - val_accuracy: 0.8706 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.9018\n",
            "Epoch 29: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 106s 106ms/step - loss: 0.2836 - accuracy: 0.9018 - val_loss: 0.3776 - val_accuracy: 0.8738 - lr: 1.0000e-04\n",
            "Epoch 30/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9012\n",
            "Epoch 30: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.2787 - accuracy: 0.9012 - val_loss: 0.3903 - val_accuracy: 0.8722 - lr: 1.0000e-04\n",
            "Epoch 31/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9059\n",
            "Epoch 31: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.2729 - accuracy: 0.9059 - val_loss: 0.3707 - val_accuracy: 0.8774 - lr: 1.0000e-04\n",
            "Epoch 32/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9075\n",
            "Epoch 32: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 101s 101ms/step - loss: 0.2626 - accuracy: 0.9075 - val_loss: 0.3780 - val_accuracy: 0.8747 - lr: 1.0000e-04\n",
            "Epoch 33/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9084\n",
            "Epoch 33: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 107s 107ms/step - loss: 0.2606 - accuracy: 0.9084 - val_loss: 0.3799 - val_accuracy: 0.8743 - lr: 1.0000e-04\n",
            "Epoch 34/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9104\n",
            "Epoch 34: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 107s 107ms/step - loss: 0.2560 - accuracy: 0.9104 - val_loss: 0.3799 - val_accuracy: 0.8752 - lr: 1.0000e-04\n",
            "Epoch 35/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9098\n",
            "Epoch 35: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 103s 103ms/step - loss: 0.2566 - accuracy: 0.9098 - val_loss: 0.3947 - val_accuracy: 0.8710 - lr: 1.0000e-04\n",
            "Epoch 36/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9112\n",
            "Epoch 36: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 103s 103ms/step - loss: 0.2542 - accuracy: 0.9112 - val_loss: 0.3893 - val_accuracy: 0.8738 - lr: 1.0000e-04\n",
            "Epoch 37/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.9139\n",
            "Epoch 37: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.2460 - accuracy: 0.9139 - val_loss: 0.3816 - val_accuracy: 0.8740 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9127\n",
            "Epoch 38: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 108s 108ms/step - loss: 0.2488 - accuracy: 0.9127 - val_loss: 0.3809 - val_accuracy: 0.8760 - lr: 1.0000e-05\n",
            "Epoch 39/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9140\n",
            "Epoch 39: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 107s 107ms/step - loss: 0.2462 - accuracy: 0.9140 - val_loss: 0.3807 - val_accuracy: 0.8761 - lr: 1.0000e-05\n",
            "Epoch 40/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9137\n",
            "Epoch 40: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 103s 103ms/step - loss: 0.2477 - accuracy: 0.9137 - val_loss: 0.3792 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
            "Epoch 41/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.9151\n",
            "Epoch 41: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 107s 107ms/step - loss: 0.2470 - accuracy: 0.9151 - val_loss: 0.3791 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
            "Epoch 42/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9148\n",
            "Epoch 42: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 108s 108ms/step - loss: 0.2451 - accuracy: 0.9148 - val_loss: 0.3815 - val_accuracy: 0.8753 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.9150\n",
            "Epoch 43: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 103s 102ms/step - loss: 0.2418 - accuracy: 0.9150 - val_loss: 0.3798 - val_accuracy: 0.8761 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.9147\n",
            "Epoch 44: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 108s 108ms/step - loss: 0.2410 - accuracy: 0.9147 - val_loss: 0.3795 - val_accuracy: 0.8756 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.9158\n",
            "Epoch 45: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 108s 108ms/step - loss: 0.2438 - accuracy: 0.9158 - val_loss: 0.3793 - val_accuracy: 0.8763 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9147\n",
            "Epoch 46: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 103s 103ms/step - loss: 0.2436 - accuracy: 0.9147 - val_loss: 0.3805 - val_accuracy: 0.8753 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9132\n",
            "Epoch 47: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 108s 108ms/step - loss: 0.2462 - accuracy: 0.9132 - val_loss: 0.3786 - val_accuracy: 0.8757 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.9137\n",
            "Epoch 48: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 102s 102ms/step - loss: 0.2445 - accuracy: 0.9137 - val_loss: 0.3789 - val_accuracy: 0.8756 - lr: 1.0000e-06\n",
            "Epoch 49/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.9134\n",
            "Epoch 49: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 108s 108ms/step - loss: 0.2460 - accuracy: 0.9134 - val_loss: 0.3769 - val_accuracy: 0.8763 - lr: 1.0000e-06\n",
            "Epoch 50/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9135\n",
            "Epoch 50: val_loss did not improve from 1.45744\n",
            "1000/1000 [==============================] - 108s 108ms/step - loss: 0.2455 - accuracy: 0.9135 - val_loss: 0.3771 - val_accuracy: 0.8768 - lr: 1.0000e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-x2qSL2TC7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fI-EMFZlSwgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AS YOU CAN SEE THAT WE HAVE GOT 96% ACCURACY**"
      ],
      "metadata": {
        "id": "Ob_QdTeIVJr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOW WE WILL TRY WITHOUT DENSE LAYER**"
      ],
      "metadata": {
        "id": "7OcY85gyVPuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    '''\n",
        "    Create Dense Block\n",
        "    '''\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)        \n",
        "        Conv2D_5_5 = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)        \n",
        "        if dropout_rate>0:\n",
        "            Conv2D_5_5 = layers.Dropout(dropout_rate)(Conv2D_5_5)          \n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_5_5])       \n",
        "        temp = concat       \n",
        "    return temp\n",
        "\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    '''\n",
        "    Create transition block\n",
        "    '''\n",
        "    global compression   \n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)   \n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)    \n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)            \n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        "\n",
        "def output_layer(input):\n",
        "    '''\n",
        "    Define output layer\n",
        "    '''\n",
        "    global compression    \n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    # as you can see we have removed the dense layer \n",
        "    AvgPooling = layers. MaxPooling2D(pool_size=(2,2))(relu)   \n",
        "    output = layers.Conv2D(filters=10,kernel_size=(2,2),activation='softmax')(AvgPooling)  \n",
        "    # we are doing the flattened the output layer to apply softmax fucntion on output layr \n",
        "    flat = layers.Flatten()(output)   \n",
        "    return flat"
      ],
      "metadata": {
        "id": "FJvgTGJcCZXT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AS YOU CAN SEE THAT WE HAVE REMOVED THE DENSE LAYER FROM THE OUTPUT LAYER IN ABOVE CODE AND AT THAT PLACE WE HAVE USED CONV2D LAYER**"
      ],
      "metadata": {
        "id": "SHHRRXxmVZmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_filter = 12\n",
        "dropout_rate = 0\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(32, (5,5), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D,10, dropout_rate)\n",
        "First_Transition = transition(First_Block, 64, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, 10, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, 32, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, 32, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "metadata": {
        "id": "h7hNMBxZNoFR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vRs3IFqVNoIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa507c4-4582-402e-e535-c81b858e3bad"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, 32, 32, 32)   2400        ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_156 (Batch  (None, 32, 32, 32)  128         ['conv2d_158[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_156 (Activation)    (None, 32, 32, 32)   0           ['batch_normalization_156[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, 32, 32, 5)    4000        ['activation_156[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_144 (Concatenate)  (None, 32, 32, 37)   0           ['conv2d_158[0][0]',             \n",
            "                                                                  'conv2d_159[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_157 (Batch  (None, 32, 32, 37)  148         ['concatenate_144[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_157 (Activation)    (None, 32, 32, 37)   0           ['batch_normalization_157[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, 32, 32, 5)    4625        ['activation_157[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_145 (Concatenate)  (None, 32, 32, 42)   0           ['concatenate_144[0][0]',        \n",
            "                                                                  'conv2d_160[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_158 (Batch  (None, 32, 32, 42)  168         ['concatenate_145[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_158 (Activation)    (None, 32, 32, 42)   0           ['batch_normalization_158[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_161 (Conv2D)            (None, 32, 32, 5)    5250        ['activation_158[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_146 (Concatenate)  (None, 32, 32, 47)   0           ['concatenate_145[0][0]',        \n",
            "                                                                  'conv2d_161[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_159 (Batch  (None, 32, 32, 47)  188         ['concatenate_146[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_159 (Activation)    (None, 32, 32, 47)   0           ['batch_normalization_159[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_162 (Conv2D)            (None, 32, 32, 5)    5875        ['activation_159[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_147 (Concatenate)  (None, 32, 32, 52)   0           ['concatenate_146[0][0]',        \n",
            "                                                                  'conv2d_162[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_160 (Batch  (None, 32, 32, 52)  208         ['concatenate_147[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_160 (Activation)    (None, 32, 32, 52)   0           ['batch_normalization_160[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_163 (Conv2D)            (None, 32, 32, 5)    6500        ['activation_160[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_148 (Concatenate)  (None, 32, 32, 57)   0           ['concatenate_147[0][0]',        \n",
            "                                                                  'conv2d_163[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_161 (Batch  (None, 32, 32, 57)  228         ['concatenate_148[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_161 (Activation)    (None, 32, 32, 57)   0           ['batch_normalization_161[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_164 (Conv2D)            (None, 32, 32, 5)    7125        ['activation_161[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_149 (Concatenate)  (None, 32, 32, 62)   0           ['concatenate_148[0][0]',        \n",
            "                                                                  'conv2d_164[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_162 (Batch  (None, 32, 32, 62)  248         ['concatenate_149[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_162 (Activation)    (None, 32, 32, 62)   0           ['batch_normalization_162[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_165 (Conv2D)            (None, 32, 32, 5)    7750        ['activation_162[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_150 (Concatenate)  (None, 32, 32, 67)   0           ['concatenate_149[0][0]',        \n",
            "                                                                  'conv2d_165[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_163 (Batch  (None, 32, 32, 67)  268         ['concatenate_150[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_163 (Activation)    (None, 32, 32, 67)   0           ['batch_normalization_163[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_166 (Conv2D)            (None, 32, 32, 5)    8375        ['activation_163[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_151 (Concatenate)  (None, 32, 32, 72)   0           ['concatenate_150[0][0]',        \n",
            "                                                                  'conv2d_166[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_164 (Batch  (None, 32, 32, 72)  288         ['concatenate_151[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_164 (Activation)    (None, 32, 32, 72)   0           ['batch_normalization_164[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_167 (Conv2D)            (None, 32, 32, 5)    9000        ['activation_164[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_152 (Concatenate)  (None, 32, 32, 77)   0           ['concatenate_151[0][0]',        \n",
            "                                                                  'conv2d_167[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_165 (Batch  (None, 32, 32, 77)  308         ['concatenate_152[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_165 (Activation)    (None, 32, 32, 77)   0           ['batch_normalization_165[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_168 (Conv2D)            (None, 32, 32, 5)    9625        ['activation_165[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_153 (Concatenate)  (None, 32, 32, 82)   0           ['concatenate_152[0][0]',        \n",
            "                                                                  'conv2d_168[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_166 (Batch  (None, 32, 32, 82)  328         ['concatenate_153[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_166 (Activation)    (None, 32, 32, 82)   0           ['batch_normalization_166[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_169 (Conv2D)            (None, 32, 32, 5)    10250       ['activation_166[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_154 (Concatenate)  (None, 32, 32, 87)   0           ['concatenate_153[0][0]',        \n",
            "                                                                  'conv2d_169[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_167 (Batch  (None, 32, 32, 87)  348         ['concatenate_154[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_167 (Activation)    (None, 32, 32, 87)   0           ['batch_normalization_167[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_170 (Conv2D)            (None, 32, 32, 5)    10875       ['activation_167[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_155 (Concatenate)  (None, 32, 32, 92)   0           ['concatenate_154[0][0]',        \n",
            "                                                                  'conv2d_170[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_168 (Batch  (None, 32, 32, 92)  368         ['concatenate_155[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_168 (Activation)    (None, 32, 32, 92)   0           ['batch_normalization_168[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_171 (Conv2D)            (None, 32, 32, 32)   73600       ['activation_168[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_9 (AveragePo  (None, 16, 16, 32)  0           ['conv2d_171[0][0]']             \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_169 (Batch  (None, 16, 16, 32)  128         ['average_pooling2d_9[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_169 (Activation)    (None, 16, 16, 32)   0           ['batch_normalization_169[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_172 (Conv2D)            (None, 16, 16, 5)    4000        ['activation_169[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_156 (Concatenate)  (None, 16, 16, 37)   0           ['average_pooling2d_9[0][0]',    \n",
            "                                                                  'conv2d_172[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_170 (Batch  (None, 16, 16, 37)  148         ['concatenate_156[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_170 (Activation)    (None, 16, 16, 37)   0           ['batch_normalization_170[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_173 (Conv2D)            (None, 16, 16, 5)    4625        ['activation_170[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_157 (Concatenate)  (None, 16, 16, 42)   0           ['concatenate_156[0][0]',        \n",
            "                                                                  'conv2d_173[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_171 (Batch  (None, 16, 16, 42)  168         ['concatenate_157[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_171 (Activation)    (None, 16, 16, 42)   0           ['batch_normalization_171[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_174 (Conv2D)            (None, 16, 16, 5)    5250        ['activation_171[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_158 (Concatenate)  (None, 16, 16, 47)   0           ['concatenate_157[0][0]',        \n",
            "                                                                  'conv2d_174[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_172 (Batch  (None, 16, 16, 47)  188         ['concatenate_158[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_172 (Activation)    (None, 16, 16, 47)   0           ['batch_normalization_172[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_175 (Conv2D)            (None, 16, 16, 5)    5875        ['activation_172[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_159 (Concatenate)  (None, 16, 16, 52)   0           ['concatenate_158[0][0]',        \n",
            "                                                                  'conv2d_175[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_173 (Batch  (None, 16, 16, 52)  208         ['concatenate_159[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_173 (Activation)    (None, 16, 16, 52)   0           ['batch_normalization_173[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_176 (Conv2D)            (None, 16, 16, 5)    6500        ['activation_173[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_160 (Concatenate)  (None, 16, 16, 57)   0           ['concatenate_159[0][0]',        \n",
            "                                                                  'conv2d_176[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_174 (Batch  (None, 16, 16, 57)  228         ['concatenate_160[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_174 (Activation)    (None, 16, 16, 57)   0           ['batch_normalization_174[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_177 (Conv2D)            (None, 16, 16, 5)    7125        ['activation_174[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_161 (Concatenate)  (None, 16, 16, 62)   0           ['concatenate_160[0][0]',        \n",
            "                                                                  'conv2d_177[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_175 (Batch  (None, 16, 16, 62)  248         ['concatenate_161[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_175 (Activation)    (None, 16, 16, 62)   0           ['batch_normalization_175[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_178 (Conv2D)            (None, 16, 16, 5)    7750        ['activation_175[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_162 (Concatenate)  (None, 16, 16, 67)   0           ['concatenate_161[0][0]',        \n",
            "                                                                  'conv2d_178[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_176 (Batch  (None, 16, 16, 67)  268         ['concatenate_162[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_176 (Activation)    (None, 16, 16, 67)   0           ['batch_normalization_176[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_179 (Conv2D)            (None, 16, 16, 5)    8375        ['activation_176[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_163 (Concatenate)  (None, 16, 16, 72)   0           ['concatenate_162[0][0]',        \n",
            "                                                                  'conv2d_179[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_177 (Batch  (None, 16, 16, 72)  288         ['concatenate_163[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_177 (Activation)    (None, 16, 16, 72)   0           ['batch_normalization_177[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_180 (Conv2D)            (None, 16, 16, 5)    9000        ['activation_177[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_164 (Concatenate)  (None, 16, 16, 77)   0           ['concatenate_163[0][0]',        \n",
            "                                                                  'conv2d_180[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_178 (Batch  (None, 16, 16, 77)  308         ['concatenate_164[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_178 (Activation)    (None, 16, 16, 77)   0           ['batch_normalization_178[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_181 (Conv2D)            (None, 16, 16, 5)    9625        ['activation_178[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_165 (Concatenate)  (None, 16, 16, 82)   0           ['concatenate_164[0][0]',        \n",
            "                                                                  'conv2d_181[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_179 (Batch  (None, 16, 16, 82)  328         ['concatenate_165[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_179 (Activation)    (None, 16, 16, 82)   0           ['batch_normalization_179[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_182 (Conv2D)            (None, 16, 16, 5)    10250       ['activation_179[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_166 (Concatenate)  (None, 16, 16, 87)   0           ['concatenate_165[0][0]',        \n",
            "                                                                  'conv2d_182[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_180 (Batch  (None, 16, 16, 87)  348         ['concatenate_166[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_180 (Activation)    (None, 16, 16, 87)   0           ['batch_normalization_180[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_183 (Conv2D)            (None, 16, 16, 5)    10875       ['activation_180[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_167 (Concatenate)  (None, 16, 16, 92)   0           ['concatenate_166[0][0]',        \n",
            "                                                                  'conv2d_183[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_181 (Batch  (None, 16, 16, 92)  368         ['concatenate_167[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_181 (Activation)    (None, 16, 16, 92)   0           ['batch_normalization_181[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_184 (Conv2D)            (None, 16, 16, 16)   36800       ['activation_181[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_10 (AverageP  (None, 8, 8, 16)    0           ['conv2d_184[0][0]']             \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " batch_normalization_182 (Batch  (None, 8, 8, 16)    64          ['average_pooling2d_10[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_182 (Activation)    (None, 8, 8, 16)     0           ['batch_normalization_182[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_185 (Conv2D)            (None, 8, 8, 6)      2400        ['activation_182[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_168 (Concatenate)  (None, 8, 8, 22)     0           ['average_pooling2d_10[0][0]',   \n",
            "                                                                  'conv2d_185[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_183 (Batch  (None, 8, 8, 22)    88          ['concatenate_168[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_183 (Activation)    (None, 8, 8, 22)     0           ['batch_normalization_183[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_186 (Conv2D)            (None, 8, 8, 6)      3300        ['activation_183[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_169 (Concatenate)  (None, 8, 8, 28)     0           ['concatenate_168[0][0]',        \n",
            "                                                                  'conv2d_186[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_184 (Batch  (None, 8, 8, 28)    112         ['concatenate_169[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_184 (Activation)    (None, 8, 8, 28)     0           ['batch_normalization_184[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_187 (Conv2D)            (None, 8, 8, 6)      4200        ['activation_184[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_170 (Concatenate)  (None, 8, 8, 34)     0           ['concatenate_169[0][0]',        \n",
            "                                                                  'conv2d_187[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_185 (Batch  (None, 8, 8, 34)    136         ['concatenate_170[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_185 (Activation)    (None, 8, 8, 34)     0           ['batch_normalization_185[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_188 (Conv2D)            (None, 8, 8, 6)      5100        ['activation_185[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_171 (Concatenate)  (None, 8, 8, 40)     0           ['concatenate_170[0][0]',        \n",
            "                                                                  'conv2d_188[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_186 (Batch  (None, 8, 8, 40)    160         ['concatenate_171[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_186 (Activation)    (None, 8, 8, 40)     0           ['batch_normalization_186[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_189 (Conv2D)            (None, 8, 8, 6)      6000        ['activation_186[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_172 (Concatenate)  (None, 8, 8, 46)     0           ['concatenate_171[0][0]',        \n",
            "                                                                  'conv2d_189[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_187 (Batch  (None, 8, 8, 46)    184         ['concatenate_172[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_187 (Activation)    (None, 8, 8, 46)     0           ['batch_normalization_187[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_190 (Conv2D)            (None, 8, 8, 6)      6900        ['activation_187[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_173 (Concatenate)  (None, 8, 8, 52)     0           ['concatenate_172[0][0]',        \n",
            "                                                                  'conv2d_190[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_188 (Batch  (None, 8, 8, 52)    208         ['concatenate_173[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_188 (Activation)    (None, 8, 8, 52)     0           ['batch_normalization_188[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_191 (Conv2D)            (None, 8, 8, 6)      7800        ['activation_188[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_174 (Concatenate)  (None, 8, 8, 58)     0           ['concatenate_173[0][0]',        \n",
            "                                                                  'conv2d_191[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_189 (Batch  (None, 8, 8, 58)    232         ['concatenate_174[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_189 (Activation)    (None, 8, 8, 58)     0           ['batch_normalization_189[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_192 (Conv2D)            (None, 8, 8, 6)      8700        ['activation_189[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_175 (Concatenate)  (None, 8, 8, 64)     0           ['concatenate_174[0][0]',        \n",
            "                                                                  'conv2d_192[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_190 (Batch  (None, 8, 8, 64)    256         ['concatenate_175[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_190 (Activation)    (None, 8, 8, 64)     0           ['batch_normalization_190[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_193 (Conv2D)            (None, 8, 8, 6)      9600        ['activation_190[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_176 (Concatenate)  (None, 8, 8, 70)     0           ['concatenate_175[0][0]',        \n",
            "                                                                  'conv2d_193[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_191 (Batch  (None, 8, 8, 70)    280         ['concatenate_176[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_191 (Activation)    (None, 8, 8, 70)     0           ['batch_normalization_191[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_194 (Conv2D)            (None, 8, 8, 6)      10500       ['activation_191[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_177 (Concatenate)  (None, 8, 8, 76)     0           ['concatenate_176[0][0]',        \n",
            "                                                                  'conv2d_194[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_192 (Batch  (None, 8, 8, 76)    304         ['concatenate_177[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_192 (Activation)    (None, 8, 8, 76)     0           ['batch_normalization_192[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_195 (Conv2D)            (None, 8, 8, 6)      11400       ['activation_192[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_178 (Concatenate)  (None, 8, 8, 82)     0           ['concatenate_177[0][0]',        \n",
            "                                                                  'conv2d_195[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_193 (Batch  (None, 8, 8, 82)    328         ['concatenate_178[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_193 (Activation)    (None, 8, 8, 82)     0           ['batch_normalization_193[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_196 (Conv2D)            (None, 8, 8, 6)      12300       ['activation_193[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_179 (Concatenate)  (None, 8, 8, 88)     0           ['concatenate_178[0][0]',        \n",
            "                                                                  'conv2d_196[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_194 (Batch  (None, 8, 8, 88)    352         ['concatenate_179[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_194 (Activation)    (None, 8, 8, 88)     0           ['batch_normalization_194[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_197 (Conv2D)            (None, 8, 8, 16)     35200       ['activation_194[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_11 (AverageP  (None, 4, 4, 16)    0           ['conv2d_197[0][0]']             \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " batch_normalization_195 (Batch  (None, 4, 4, 16)    64          ['average_pooling2d_11[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_195 (Activation)    (None, 4, 4, 16)     0           ['batch_normalization_195[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_198 (Conv2D)            (None, 4, 4, 6)      2400        ['activation_195[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_180 (Concatenate)  (None, 4, 4, 22)     0           ['average_pooling2d_11[0][0]',   \n",
            "                                                                  'conv2d_198[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_196 (Batch  (None, 4, 4, 22)    88          ['concatenate_180[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_196 (Activation)    (None, 4, 4, 22)     0           ['batch_normalization_196[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_199 (Conv2D)            (None, 4, 4, 6)      3300        ['activation_196[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_181 (Concatenate)  (None, 4, 4, 28)     0           ['concatenate_180[0][0]',        \n",
            "                                                                  'conv2d_199[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_197 (Batch  (None, 4, 4, 28)    112         ['concatenate_181[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_197 (Activation)    (None, 4, 4, 28)     0           ['batch_normalization_197[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_200 (Conv2D)            (None, 4, 4, 6)      4200        ['activation_197[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_182 (Concatenate)  (None, 4, 4, 34)     0           ['concatenate_181[0][0]',        \n",
            "                                                                  'conv2d_200[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_198 (Batch  (None, 4, 4, 34)    136         ['concatenate_182[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_198 (Activation)    (None, 4, 4, 34)     0           ['batch_normalization_198[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_201 (Conv2D)            (None, 4, 4, 6)      5100        ['activation_198[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_183 (Concatenate)  (None, 4, 4, 40)     0           ['concatenate_182[0][0]',        \n",
            "                                                                  'conv2d_201[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_199 (Batch  (None, 4, 4, 40)    160         ['concatenate_183[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_199 (Activation)    (None, 4, 4, 40)     0           ['batch_normalization_199[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_202 (Conv2D)            (None, 4, 4, 6)      6000        ['activation_199[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_184 (Concatenate)  (None, 4, 4, 46)     0           ['concatenate_183[0][0]',        \n",
            "                                                                  'conv2d_202[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_200 (Batch  (None, 4, 4, 46)    184         ['concatenate_184[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_200 (Activation)    (None, 4, 4, 46)     0           ['batch_normalization_200[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_203 (Conv2D)            (None, 4, 4, 6)      6900        ['activation_200[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_185 (Concatenate)  (None, 4, 4, 52)     0           ['concatenate_184[0][0]',        \n",
            "                                                                  'conv2d_203[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_201 (Batch  (None, 4, 4, 52)    208         ['concatenate_185[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_201 (Activation)    (None, 4, 4, 52)     0           ['batch_normalization_201[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_204 (Conv2D)            (None, 4, 4, 6)      7800        ['activation_201[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_186 (Concatenate)  (None, 4, 4, 58)     0           ['concatenate_185[0][0]',        \n",
            "                                                                  'conv2d_204[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_202 (Batch  (None, 4, 4, 58)    232         ['concatenate_186[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_202 (Activation)    (None, 4, 4, 58)     0           ['batch_normalization_202[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_205 (Conv2D)            (None, 4, 4, 6)      8700        ['activation_202[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_187 (Concatenate)  (None, 4, 4, 64)     0           ['concatenate_186[0][0]',        \n",
            "                                                                  'conv2d_205[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_203 (Batch  (None, 4, 4, 64)    256         ['concatenate_187[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_203 (Activation)    (None, 4, 4, 64)     0           ['batch_normalization_203[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_206 (Conv2D)            (None, 4, 4, 6)      9600        ['activation_203[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_188 (Concatenate)  (None, 4, 4, 70)     0           ['concatenate_187[0][0]',        \n",
            "                                                                  'conv2d_206[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_204 (Batch  (None, 4, 4, 70)    280         ['concatenate_188[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_204 (Activation)    (None, 4, 4, 70)     0           ['batch_normalization_204[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_207 (Conv2D)            (None, 4, 4, 6)      10500       ['activation_204[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_189 (Concatenate)  (None, 4, 4, 76)     0           ['concatenate_188[0][0]',        \n",
            "                                                                  'conv2d_207[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_205 (Batch  (None, 4, 4, 76)    304         ['concatenate_189[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_205 (Activation)    (None, 4, 4, 76)     0           ['batch_normalization_205[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_208 (Conv2D)            (None, 4, 4, 6)      11400       ['activation_205[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_190 (Concatenate)  (None, 4, 4, 82)     0           ['concatenate_189[0][0]',        \n",
            "                                                                  'conv2d_208[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_206 (Batch  (None, 4, 4, 82)    328         ['concatenate_190[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_206 (Activation)    (None, 4, 4, 82)     0           ['batch_normalization_206[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)            (None, 4, 4, 6)      12300       ['activation_206[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_191 (Concatenate)  (None, 4, 4, 88)     0           ['concatenate_190[0][0]',        \n",
            "                                                                  'conv2d_209[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_207 (Batch  (None, 4, 4, 88)    352         ['concatenate_191[0][0]']        \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_207 (Activation)    (None, 4, 4, 88)     0           ['batch_normalization_207[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 2, 2, 88)    0           ['activation_207[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)            (None, 1, 1, 10)     3530        ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 10)           0           ['conv2d_210[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 518,286\n",
            "Trainable params: 512,358\n",
            "Non-trainable params: 5,928\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=22,\n",
        "    width_shift_range=0.125,\n",
        "    height_shift_range=0.125,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode = 'nearest',\n",
        "    zoom_range=0.01)\n",
        "data_gen.fit(X_train)\n"
      ],
      "metadata": {
        "id": "qBAvO21x2lWm"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Rv0RqcuoNoLw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience= 5,\n",
        "                              min_lr=0.000001)\n",
        "filepath = \"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks = [checkpoint, reduce_lr]"
      ],
      "metadata": {
        "id": "gqdsLfBP1TrN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit_generator(data_gen.flow(X_train, y_train, batch_size=50),\n",
        "                    steps_per_epoch = (len(X_train) /50), epochs=50, validation_data=(X_test, y_test),callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA6ZK7jV1TuE",
        "outputId": "995126e0-3ef1-4d9a-b661-2ba5bd9fc41a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-516e8d78b8f2>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history=model.fit_generator(data_gen.flow(X_train, y_train, batch_size=50),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/1000 [==============================] - ETA: 0s - loss: 0.9228 - accuracy: 0.6776\n",
            "Epoch 1: val_loss improved from -inf to 0.88508, saving model to drive/My Drive/best_model.hdf5\n",
            "1000/1000 [==============================] - 217s 98ms/step - loss: 0.9228 - accuracy: 0.6776 - val_loss: 0.8851 - val_accuracy: 0.6931 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.8238 - accuracy: 0.7139\n",
            "Epoch 2: val_loss improved from 0.88508 to 0.95265, saving model to drive/My Drive/best_model.hdf5\n",
            "1000/1000 [==============================] - 97s 97ms/step - loss: 0.8238 - accuracy: 0.7139 - val_loss: 0.9526 - val_accuracy: 0.6818 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.7409 - accuracy: 0.7400\n",
            "Epoch 3: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.7409 - accuracy: 0.7400 - val_loss: 0.9366 - val_accuracy: 0.6847 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.7620\n",
            "Epoch 4: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.6872 - accuracy: 0.7620 - val_loss: 0.7948 - val_accuracy: 0.7289 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.6506 - accuracy: 0.7758\n",
            "Epoch 5: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.6506 - accuracy: 0.7758 - val_loss: 0.9146 - val_accuracy: 0.7024 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.7872\n",
            "Epoch 6: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.6135 - accuracy: 0.7872 - val_loss: 0.6884 - val_accuracy: 0.7699 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.7969\n",
            "Epoch 7: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 97s 97ms/step - loss: 0.5825 - accuracy: 0.7969 - val_loss: 0.7580 - val_accuracy: 0.7504 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.8080\n",
            "Epoch 8: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 97s 97ms/step - loss: 0.5573 - accuracy: 0.8080 - val_loss: 0.6861 - val_accuracy: 0.7739 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.8134\n",
            "Epoch 9: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 97s 97ms/step - loss: 0.5355 - accuracy: 0.8134 - val_loss: 0.6981 - val_accuracy: 0.7704 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.8212\n",
            "Epoch 10: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.5159 - accuracy: 0.8212 - val_loss: 0.7232 - val_accuracy: 0.7644 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.8261\n",
            "Epoch 11: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.4971 - accuracy: 0.8261 - val_loss: 0.5998 - val_accuracy: 0.8023 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.8332\n",
            "Epoch 12: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.4801 - accuracy: 0.8332 - val_loss: 0.6156 - val_accuracy: 0.8029 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4694 - accuracy: 0.8368\n",
            "Epoch 13: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.4694 - accuracy: 0.8368 - val_loss: 0.6029 - val_accuracy: 0.8022 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8420\n",
            "Epoch 14: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.4513 - accuracy: 0.8420 - val_loss: 0.5592 - val_accuracy: 0.8176 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.8479\n",
            "Epoch 15: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.4393 - accuracy: 0.8479 - val_loss: 0.5432 - val_accuracy: 0.8157 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8535\n",
            "Epoch 16: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.4289 - accuracy: 0.8535 - val_loss: 0.5003 - val_accuracy: 0.8345 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4152 - accuracy: 0.8565\n",
            "Epoch 17: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.4152 - accuracy: 0.8565 - val_loss: 0.4969 - val_accuracy: 0.8332 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4085 - accuracy: 0.8582\n",
            "Epoch 18: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.4085 - accuracy: 0.8582 - val_loss: 0.5163 - val_accuracy: 0.8335 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8614\n",
            "Epoch 19: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.4012 - accuracy: 0.8614 - val_loss: 0.4867 - val_accuracy: 0.8372 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3864 - accuracy: 0.8653\n",
            "Epoch 20: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.3864 - accuracy: 0.8653 - val_loss: 0.4772 - val_accuracy: 0.8451 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.8670\n",
            "Epoch 21: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 97s 97ms/step - loss: 0.3819 - accuracy: 0.8670 - val_loss: 0.7654 - val_accuracy: 0.7713 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8711\n",
            "Epoch 22: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 97s 97ms/step - loss: 0.3684 - accuracy: 0.8711 - val_loss: 0.4558 - val_accuracy: 0.8477 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.8733\n",
            "Epoch 23: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.3631 - accuracy: 0.8733 - val_loss: 0.5436 - val_accuracy: 0.8255 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.8742\n",
            "Epoch 24: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.3612 - accuracy: 0.8742 - val_loss: 0.5452 - val_accuracy: 0.8286 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.8774\n",
            "Epoch 25: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.3485 - accuracy: 0.8774 - val_loss: 0.6404 - val_accuracy: 0.8038 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8801\n",
            "Epoch 26: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.3420 - accuracy: 0.8801 - val_loss: 0.5145 - val_accuracy: 0.8339 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.8813\n",
            "Epoch 27: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 97s 97ms/step - loss: 0.3386 - accuracy: 0.8813 - val_loss: 0.4543 - val_accuracy: 0.8517 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.8847\n",
            "Epoch 28: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.3310 - accuracy: 0.8847 - val_loss: 0.5456 - val_accuracy: 0.8341 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.8857\n",
            "Epoch 29: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.3282 - accuracy: 0.8857 - val_loss: 0.4403 - val_accuracy: 0.8576 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.8886\n",
            "Epoch 30: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.3243 - accuracy: 0.8886 - val_loss: 0.4887 - val_accuracy: 0.8432 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.8872\n",
            "Epoch 31: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.3199 - accuracy: 0.8872 - val_loss: 0.4321 - val_accuracy: 0.8589 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.8928\n",
            "Epoch 32: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.3084 - accuracy: 0.8928 - val_loss: 0.5217 - val_accuracy: 0.8349 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8908\n",
            "Epoch 33: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.3052 - accuracy: 0.8908 - val_loss: 0.3934 - val_accuracy: 0.8707 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.8923\n",
            "Epoch 34: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.3053 - accuracy: 0.8923 - val_loss: 0.4752 - val_accuracy: 0.8478 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8930\n",
            "Epoch 35: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.3014 - accuracy: 0.8930 - val_loss: 0.4568 - val_accuracy: 0.8539 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.8970\n",
            "Epoch 36: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.2946 - accuracy: 0.8970 - val_loss: 0.5935 - val_accuracy: 0.8181 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2920 - accuracy: 0.8982\n",
            "Epoch 37: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.2920 - accuracy: 0.8982 - val_loss: 0.5720 - val_accuracy: 0.8246 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8992\n",
            "Epoch 38: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.2891 - accuracy: 0.8992 - val_loss: 0.4168 - val_accuracy: 0.8693 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9172\n",
            "Epoch 39: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.2396 - accuracy: 0.9172 - val_loss: 0.3507 - val_accuracy: 0.8866 - lr: 1.0000e-04\n",
            "Epoch 40/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9214\n",
            "Epoch 40: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.2215 - accuracy: 0.9214 - val_loss: 0.3475 - val_accuracy: 0.8887 - lr: 1.0000e-04\n",
            "Epoch 41/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9264\n",
            "Epoch 41: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.2125 - accuracy: 0.9264 - val_loss: 0.3464 - val_accuracy: 0.8906 - lr: 1.0000e-04\n",
            "Epoch 42/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9279\n",
            "Epoch 42: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.2074 - accuracy: 0.9279 - val_loss: 0.3662 - val_accuracy: 0.8847 - lr: 1.0000e-04\n",
            "Epoch 43/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9277\n",
            "Epoch 43: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.2077 - accuracy: 0.9277 - val_loss: 0.3548 - val_accuracy: 0.8884 - lr: 1.0000e-04\n",
            "Epoch 44/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9290\n",
            "Epoch 44: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.2032 - accuracy: 0.9290 - val_loss: 0.3519 - val_accuracy: 0.8916 - lr: 1.0000e-04\n",
            "Epoch 45/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9312\n",
            "Epoch 45: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.2002 - accuracy: 0.9312 - val_loss: 0.3505 - val_accuracy: 0.8912 - lr: 1.0000e-04\n",
            "Epoch 46/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1971 - accuracy: 0.9300\n",
            "Epoch 46: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.1971 - accuracy: 0.9300 - val_loss: 0.3456 - val_accuracy: 0.8946 - lr: 1.0000e-04\n",
            "Epoch 47/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9299\n",
            "Epoch 47: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 96s 96ms/step - loss: 0.1983 - accuracy: 0.9299 - val_loss: 0.3529 - val_accuracy: 0.8906 - lr: 1.0000e-04\n",
            "Epoch 48/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9326\n",
            "Epoch 48: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.1939 - accuracy: 0.9326 - val_loss: 0.3532 - val_accuracy: 0.8903 - lr: 1.0000e-04\n",
            "Epoch 49/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9321\n",
            "Epoch 49: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 95s 95ms/step - loss: 0.1948 - accuracy: 0.9321 - val_loss: 0.3424 - val_accuracy: 0.8928 - lr: 1.0000e-04\n",
            "Epoch 50/50\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9335\n",
            "Epoch 50: val_loss did not improve from 0.95265\n",
            "1000/1000 [==============================] - 94s 94ms/step - loss: 0.1920 - accuracy: 0.9335 - val_loss: 0.3491 - val_accuracy: 0.8926 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Print the training and validation accuracy and loss values at the end of each epoch\n",
        "for epoch in range(1, len(history.history['loss']) + 1):\n",
        "    print(f'Epoch {epoch}')\n",
        "    print(f'Training Accuracy: {history.history[\"accuracy\"][epoch-1]:.4f}')\n",
        "    print(f'Training Loss: {history.history[\"loss\"][epoch-1]:.4f}')\n",
        "    print(f'Validation Accuracy: {history.history[\"val_accuracy\"][epoch-1]:.4f}')\n",
        "    print(f'Validation Loss: {history.history[\"val_loss\"][epoch-1]:.4f}')\n",
        "    print()\n",
        "\n",
        "# Plot the training and validation accuracy and loss curves\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
        "\n",
        "ax[0].plot(history.history['accuracy'])\n",
        "ax[0].plot(history.history['val_accuracy'])\n",
        "ax[0].set_title('Model Accuracy')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "ax[1].plot(history.history['loss'])\n",
        "ax[1].plot(history.history['val_loss'])\n",
        "ax[1].set_title('Model Loss')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E_VIg0OB1Txo",
        "outputId": "7598a3fe-c079-44d8-b084-b948c584199d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Training Accuracy: 0.6776\n",
            "Training Loss: 0.9228\n",
            "Validation Accuracy: 0.6931\n",
            "Validation Loss: 0.8851\n",
            "\n",
            "Epoch 2\n",
            "Training Accuracy: 0.7139\n",
            "Training Loss: 0.8238\n",
            "Validation Accuracy: 0.6818\n",
            "Validation Loss: 0.9526\n",
            "\n",
            "Epoch 3\n",
            "Training Accuracy: 0.7400\n",
            "Training Loss: 0.7409\n",
            "Validation Accuracy: 0.6847\n",
            "Validation Loss: 0.9366\n",
            "\n",
            "Epoch 4\n",
            "Training Accuracy: 0.7620\n",
            "Training Loss: 0.6872\n",
            "Validation Accuracy: 0.7289\n",
            "Validation Loss: 0.7948\n",
            "\n",
            "Epoch 5\n",
            "Training Accuracy: 0.7758\n",
            "Training Loss: 0.6506\n",
            "Validation Accuracy: 0.7024\n",
            "Validation Loss: 0.9146\n",
            "\n",
            "Epoch 6\n",
            "Training Accuracy: 0.7872\n",
            "Training Loss: 0.6135\n",
            "Validation Accuracy: 0.7699\n",
            "Validation Loss: 0.6884\n",
            "\n",
            "Epoch 7\n",
            "Training Accuracy: 0.7969\n",
            "Training Loss: 0.5825\n",
            "Validation Accuracy: 0.7504\n",
            "Validation Loss: 0.7580\n",
            "\n",
            "Epoch 8\n",
            "Training Accuracy: 0.8080\n",
            "Training Loss: 0.5573\n",
            "Validation Accuracy: 0.7739\n",
            "Validation Loss: 0.6861\n",
            "\n",
            "Epoch 9\n",
            "Training Accuracy: 0.8134\n",
            "Training Loss: 0.5355\n",
            "Validation Accuracy: 0.7704\n",
            "Validation Loss: 0.6981\n",
            "\n",
            "Epoch 10\n",
            "Training Accuracy: 0.8212\n",
            "Training Loss: 0.5159\n",
            "Validation Accuracy: 0.7644\n",
            "Validation Loss: 0.7232\n",
            "\n",
            "Epoch 11\n",
            "Training Accuracy: 0.8261\n",
            "Training Loss: 0.4971\n",
            "Validation Accuracy: 0.8023\n",
            "Validation Loss: 0.5998\n",
            "\n",
            "Epoch 12\n",
            "Training Accuracy: 0.8332\n",
            "Training Loss: 0.4801\n",
            "Validation Accuracy: 0.8029\n",
            "Validation Loss: 0.6156\n",
            "\n",
            "Epoch 13\n",
            "Training Accuracy: 0.8368\n",
            "Training Loss: 0.4694\n",
            "Validation Accuracy: 0.8022\n",
            "Validation Loss: 0.6029\n",
            "\n",
            "Epoch 14\n",
            "Training Accuracy: 0.8420\n",
            "Training Loss: 0.4513\n",
            "Validation Accuracy: 0.8176\n",
            "Validation Loss: 0.5592\n",
            "\n",
            "Epoch 15\n",
            "Training Accuracy: 0.8479\n",
            "Training Loss: 0.4393\n",
            "Validation Accuracy: 0.8157\n",
            "Validation Loss: 0.5432\n",
            "\n",
            "Epoch 16\n",
            "Training Accuracy: 0.8535\n",
            "Training Loss: 0.4289\n",
            "Validation Accuracy: 0.8345\n",
            "Validation Loss: 0.5003\n",
            "\n",
            "Epoch 17\n",
            "Training Accuracy: 0.8565\n",
            "Training Loss: 0.4152\n",
            "Validation Accuracy: 0.8332\n",
            "Validation Loss: 0.4969\n",
            "\n",
            "Epoch 18\n",
            "Training Accuracy: 0.8582\n",
            "Training Loss: 0.4085\n",
            "Validation Accuracy: 0.8335\n",
            "Validation Loss: 0.5163\n",
            "\n",
            "Epoch 19\n",
            "Training Accuracy: 0.8614\n",
            "Training Loss: 0.4012\n",
            "Validation Accuracy: 0.8372\n",
            "Validation Loss: 0.4867\n",
            "\n",
            "Epoch 20\n",
            "Training Accuracy: 0.8653\n",
            "Training Loss: 0.3864\n",
            "Validation Accuracy: 0.8451\n",
            "Validation Loss: 0.4772\n",
            "\n",
            "Epoch 21\n",
            "Training Accuracy: 0.8670\n",
            "Training Loss: 0.3819\n",
            "Validation Accuracy: 0.7713\n",
            "Validation Loss: 0.7654\n",
            "\n",
            "Epoch 22\n",
            "Training Accuracy: 0.8711\n",
            "Training Loss: 0.3684\n",
            "Validation Accuracy: 0.8477\n",
            "Validation Loss: 0.4558\n",
            "\n",
            "Epoch 23\n",
            "Training Accuracy: 0.8733\n",
            "Training Loss: 0.3631\n",
            "Validation Accuracy: 0.8255\n",
            "Validation Loss: 0.5436\n",
            "\n",
            "Epoch 24\n",
            "Training Accuracy: 0.8742\n",
            "Training Loss: 0.3612\n",
            "Validation Accuracy: 0.8286\n",
            "Validation Loss: 0.5452\n",
            "\n",
            "Epoch 25\n",
            "Training Accuracy: 0.8774\n",
            "Training Loss: 0.3485\n",
            "Validation Accuracy: 0.8038\n",
            "Validation Loss: 0.6404\n",
            "\n",
            "Epoch 26\n",
            "Training Accuracy: 0.8801\n",
            "Training Loss: 0.3420\n",
            "Validation Accuracy: 0.8339\n",
            "Validation Loss: 0.5145\n",
            "\n",
            "Epoch 27\n",
            "Training Accuracy: 0.8813\n",
            "Training Loss: 0.3386\n",
            "Validation Accuracy: 0.8517\n",
            "Validation Loss: 0.4543\n",
            "\n",
            "Epoch 28\n",
            "Training Accuracy: 0.8847\n",
            "Training Loss: 0.3310\n",
            "Validation Accuracy: 0.8341\n",
            "Validation Loss: 0.5456\n",
            "\n",
            "Epoch 29\n",
            "Training Accuracy: 0.8857\n",
            "Training Loss: 0.3282\n",
            "Validation Accuracy: 0.8576\n",
            "Validation Loss: 0.4403\n",
            "\n",
            "Epoch 30\n",
            "Training Accuracy: 0.8886\n",
            "Training Loss: 0.3243\n",
            "Validation Accuracy: 0.8432\n",
            "Validation Loss: 0.4887\n",
            "\n",
            "Epoch 31\n",
            "Training Accuracy: 0.8872\n",
            "Training Loss: 0.3199\n",
            "Validation Accuracy: 0.8589\n",
            "Validation Loss: 0.4321\n",
            "\n",
            "Epoch 32\n",
            "Training Accuracy: 0.8928\n",
            "Training Loss: 0.3084\n",
            "Validation Accuracy: 0.8349\n",
            "Validation Loss: 0.5217\n",
            "\n",
            "Epoch 33\n",
            "Training Accuracy: 0.8908\n",
            "Training Loss: 0.3052\n",
            "Validation Accuracy: 0.8707\n",
            "Validation Loss: 0.3934\n",
            "\n",
            "Epoch 34\n",
            "Training Accuracy: 0.8923\n",
            "Training Loss: 0.3053\n",
            "Validation Accuracy: 0.8478\n",
            "Validation Loss: 0.4752\n",
            "\n",
            "Epoch 35\n",
            "Training Accuracy: 0.8930\n",
            "Training Loss: 0.3014\n",
            "Validation Accuracy: 0.8539\n",
            "Validation Loss: 0.4568\n",
            "\n",
            "Epoch 36\n",
            "Training Accuracy: 0.8970\n",
            "Training Loss: 0.2946\n",
            "Validation Accuracy: 0.8181\n",
            "Validation Loss: 0.5935\n",
            "\n",
            "Epoch 37\n",
            "Training Accuracy: 0.8982\n",
            "Training Loss: 0.2920\n",
            "Validation Accuracy: 0.8246\n",
            "Validation Loss: 0.5720\n",
            "\n",
            "Epoch 38\n",
            "Training Accuracy: 0.8992\n",
            "Training Loss: 0.2891\n",
            "Validation Accuracy: 0.8693\n",
            "Validation Loss: 0.4168\n",
            "\n",
            "Epoch 39\n",
            "Training Accuracy: 0.9172\n",
            "Training Loss: 0.2396\n",
            "Validation Accuracy: 0.8866\n",
            "Validation Loss: 0.3507\n",
            "\n",
            "Epoch 40\n",
            "Training Accuracy: 0.9214\n",
            "Training Loss: 0.2215\n",
            "Validation Accuracy: 0.8887\n",
            "Validation Loss: 0.3475\n",
            "\n",
            "Epoch 41\n",
            "Training Accuracy: 0.9264\n",
            "Training Loss: 0.2125\n",
            "Validation Accuracy: 0.8906\n",
            "Validation Loss: 0.3464\n",
            "\n",
            "Epoch 42\n",
            "Training Accuracy: 0.9279\n",
            "Training Loss: 0.2074\n",
            "Validation Accuracy: 0.8847\n",
            "Validation Loss: 0.3662\n",
            "\n",
            "Epoch 43\n",
            "Training Accuracy: 0.9277\n",
            "Training Loss: 0.2077\n",
            "Validation Accuracy: 0.8884\n",
            "Validation Loss: 0.3548\n",
            "\n",
            "Epoch 44\n",
            "Training Accuracy: 0.9290\n",
            "Training Loss: 0.2032\n",
            "Validation Accuracy: 0.8916\n",
            "Validation Loss: 0.3519\n",
            "\n",
            "Epoch 45\n",
            "Training Accuracy: 0.9312\n",
            "Training Loss: 0.2002\n",
            "Validation Accuracy: 0.8912\n",
            "Validation Loss: 0.3505\n",
            "\n",
            "Epoch 46\n",
            "Training Accuracy: 0.9300\n",
            "Training Loss: 0.1971\n",
            "Validation Accuracy: 0.8946\n",
            "Validation Loss: 0.3456\n",
            "\n",
            "Epoch 47\n",
            "Training Accuracy: 0.9299\n",
            "Training Loss: 0.1983\n",
            "Validation Accuracy: 0.8906\n",
            "Validation Loss: 0.3529\n",
            "\n",
            "Epoch 48\n",
            "Training Accuracy: 0.9326\n",
            "Training Loss: 0.1939\n",
            "Validation Accuracy: 0.8903\n",
            "Validation Loss: 0.3532\n",
            "\n",
            "Epoch 49\n",
            "Training Accuracy: 0.9321\n",
            "Training Loss: 0.1948\n",
            "Validation Accuracy: 0.8928\n",
            "Validation Loss: 0.3424\n",
            "\n",
            "Epoch 50\n",
            "Training Accuracy: 0.9335\n",
            "Training Loss: 0.1920\n",
            "Validation Accuracy: 0.8926\n",
            "Validation Loss: 0.3491\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAJcCAYAAAC8DwN/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAC2dElEQVR4nOzdd3zUVbrH8c9J74Uk1AChdwQFpNkbVmxrb7uurmtZ123XLXfX9W5xi+vqrq6997X3DlgABRSQXkILLYV00nPuH2cCISRkksxkZpLv+/XKa2Z+5cwzjIaHU55jrLWIiIiISHAIC3QAIiIiIrKfkjMRERGRIKLkTERERCSIKDkTERERCSJKzkRERESCiJIzERERkSCi5ExEQpYxJssYY40xEV5ce5Ux5vPOiEtEpCOUnIlIpzDGbDbGVBtj0psc/8aTYGUFKLTGsSQYY8qMMe8GOhYR6b6UnIlIZ9oEXNzwwhgzDogLXDgHOQ+oAk4yxvTuzDf2pvdPRLoHJWci0pmeAq5o9PpK4MnGFxhjko0xTxpj8owxW4wxvzHGhHnOhRtj/m6MyTfGZAOnN3PvI8aYncaY7caYPxhjwtsQ35XA/cBy4LImbc80xsw3xhQZY7YZY67yHI81xtzpibXYGPO559ixxpicJm1sNsac6Hl+mzHmJWPM08aYEuAqY8wUY8wCz3vsNMb82xgT1ej+McaYD40xe4wxu40xvzLG9DbG7DXGpDW67nDPn19kGz67iAQJJWci0pkWAknGmFGepOki4Okm1/wLSAYGA8fgkrnves5dA5wBTAQmAec3ufdxoBYY6rnmZOD73gRmjBkIHAs84/m5osm5dz2xZQATgKWe038HjgCmAz2AXwD13rwnMBt4CUjxvGcdcAuQDkwDTgCu98SQCHwEvAf09XzGj621u4C5wAWN2r0ceN5aW+NlHCISRJSciUhna+g9OwlYDWxvONEoYfultbbUWrsZuBOXbIBLQP5prd1mrd0D/LnRvb2A04AfW2vLrbW5wF2e9rxxObDcWrsKeB4YY4yZ6Dl3CfCRtfY5a22NtbbAWrvU06P3PeBma+12a22dtXa+tbbKy/dcYK19zVpbb62tsNYusdYutNbWej77A7gEFVxSustae6e1ttLz5/Ol59wTeHr6PH+GF+P+nEUkBGmOg4h0tqeAT4FBNBnSxPUYRQJbGh3bAvTzPO8LbGtyrsFAz707jTENx8KaXH8oVwAPAVhrtxtj5uGGOb8B+gMbm7knHYhp4Zw3DojNGDMc+AeuVzAO9zt6ied0SzEAvA7cb4wZBIwAiq21X7UzJhEJMPWciUinstZuwS0MOA14pcnpfKAGl2g1GMD+3rWduCSl8bkG23CT+dOttSmenyRr7ZjWYjLGTAeGAb80xuwyxuwCjgQu8UzU3wYMaebWfKCyhXPlNFrs4OnRymhyjW3y+j/AGmCYtTYJ+BXQkGluww31HsRaWwm8iOs9uxz1momENCVnIhIIVwPHW2vLGx+01tbhkow/GmMSPXO9fsL+eWkvAj8yxmQaY1KBWxvduxP4ALjTGJNkjAkzxgwxxhxD664EPgRG4+aTTQDGArHAqbj5YCcaYy4wxkQYY9KMMROstfXAo8A/jDF9PQsWphljooF1QIwx5nTPxPzfANGtxJEIlABlxpiRwA8bnXsL6GOM+bExJtrz53Nko/NPAlcBZ6HkTCSkKTkTkU5nrd1orV3cwumbcL1O2cDnwLO4BAjcsOP7wDLgaw7uebsCiAJWAYW4yfZ9DhWLMSYGN5ftX9baXY1+NuGSnCuttVtxPX0/BfbgFgMc5mniZ8C3wCLPub8AYdbaYtxk/odxPX/lwAGrN5vxM9z8tlLPZ32h4YS1thQ3T+9MYBewHjiu0fkvcAsRvvb0TopIiDLWNu1VFxGRUGSM+QR41lr7cKBjEZH2U3ImItIFGGMm44Zm+3t62UQkRGlYU0QkxBljnsDVQPuxEjOR0KeeMxEREZEgop4zERERkSDSZYrQpqen26ysrECHISIiItKqJUuW5Ftrm9Y+BLpQcpaVlcXixS2tzBcREREJHsaYFkveaFhTREREJIgoORMREREJIkrORERERIJIl5lz1pyamhpycnKorKwMdCh+FxMTQ2ZmJpGRkYEORURERDqgSydnOTk5JCYmkpWVhTEm0OH4jbWWgoICcnJyGDRoUKDDERERkQ7o0sOalZWVpKWldenEDMAYQ1paWrfoIRQREenqunRyBnT5xKxBd/mcIiIiXV2XT85EREREQomSMz8rKirivvvua/N9p512GkVFRb4PSERERA5SWVPH5vxy5m/IZ966vIDG0qUXBASDhuTs+uuvP+B4bW0tEREt//G/8847/g5NREQkJFlr2ZRfzuLNhazPLSUqIoy4qAhiI8OJiwonNirc8zyC2Ch3LCYynMK91ewoqmBnUSXbiyrYWVzBjqJKdhZXkF9Wva/9wenxfPKzYwP2+ZSc+dmtt97Kxo0bmTBhApGRkcTExJCamsqaNWtYt24dZ599Ntu2baOyspKbb76Za6+9Fti/HVVZWRmnnnoqM2fOZP78+fTr14/XX3+d2NjYAH8yERHp7mrr6snOL2dncSVp8VH0TIwmLSGa8DDfzoOurq1nxY5iFm/ew+LNhSzZUkhBuUumoiLCqKu31NXbNrWZEB1B35QY+iTHMrZfMn2TY+ibEkuflBgyU+J8Gn9bdZvk7PdvrmTVjhKftjm6bxK/O3PMIa+54447WLFiBUuXLmXu3LmcfvrprFixYl/Ji0cffZQePXpQUVHB5MmTOe+880hLSzugjfXr1/Pcc8/x0EMPccEFF/Dyyy9z2WWX+fSziIiIHMqe8mrW7Cxh1c4S1uwqZfXOEtbnllFdW3/AdWEG0hKi6ZnofjISo+mZGEPPpGgyEqKJiQwnLMwQEWYIM4aIcPcY3uRYTuFeFm8uZPHmQpblFFHleZ+BaXEcO6Ink7JSmZyVyuD0BIyB6rp6Kqrr2Ov5qaxpeF5LRXUdFTV1pMRF0jcllr4psSTFBG9d0G6TnAWLKVOmHFCL7J577uHVV18FYNu2baxfv/6g5GzQoEFMmDABgCOOOILNmzd3VrgiItKNVFTXsaPYDfvtKK4gO6+c1TtLWLOrhN0lVfuuS0+IZlSfRK6ansWoPon0S4ljT3k1eaWV5JZWkVdaRW5pFbmllazaWUJ+WXWbe7YAIsIMY/omcdnUgUwamMoRWan0TIxp9troiHCiI8IJcKeXT3Sb5Ky1Hq7OEh8fv+/53Llz+eijj1iwYAFxcXEce+yxzdYqi46O3vc8PDycioqKTolVRESCl7WWPeXV5BRWkFNYwZ7yqn09UuFhYZ5HT2/UvuMGYwz5pVVuvlVxJbuKK908rOJKiitqDniPyHDD0J6JzBiazqjeSYzqk8SI3olkJEa3EFXz6uqtJ3mrorquft8wZMNPbX099dZSW2fdY72lR3wUE/qnEBfVbVKVfbrfJ+5kiYmJlJaWNnuuuLiY1NRU4uLiWLNmDQsXLuzk6EREJJiVV9WSnVfOtsK95BTu3ZeINTzfW13XofZ7xEfROymGzNRYJmf1oHdyzL55WH2S3WNURMcLO4SHGTI8Q5zSOiVnfpaWlsaMGTMYO3YssbGx9OrVa9+5WbNmcf/99zNq1ChGjBjB1KlTAxipiIgEi7W7SnlywWZe/Wb7AQlYUkwEmalxZKXFM3NoBpmpsfTvEUdmaixpCVFgoXZfb5Slrr5+3+uGY/X1lrSEaPokxxATGR7ATyktMda2fQw4GE2aNMkuXrz4gGOrV69m1KhRAYqo83W3zysi0pXU1tXz4ardPLFgMwuz9xAdEcbsCX05YVQv+qfG0S81luTY4J3ELm1jjFlirZ3U3Dn1nImIiARQflkVz3+1lWe+3MrO4koyU2P55akjuWBSf1LjowIdngSAkjMREZEAWLqtiCfnb+at5TuprqvnqGHp3D57LMeP7OnzOmESWpSciYiIdJLC8mreWr6Dl5bksCynmIToCC45cgCXTR3I0J4JgQ5PgoSSMxERET+qqq3jk9W5vPLNduauzaWmzjKydyK3zx7DuYdnkhCtv4rlQPovQkRExMestSzaXMir32zn7eU7KKmspWdiNFdNz+KciZmM7psU6BAliCk5ExER8ZHsvDJe/WY7r36znZzCCmIjw5k1tjfnTOzHjKHpmksmXlFyFmQSEhIoKysLdBgiItJG//p4PXd+uI4wAzOGpvOTk4ZzypjexGvYUtpI/8WIiIh00Lc5xfzz4/WcOrY3t501hl5Jze//KOINJWd+duutt9K/f39uuOEGAG677TYiIiKYM2cOhYWF1NTU8Ic//IHZs2cHOFIREWmP6tp6fv7SMtLio7jj3PEkx6lQrHRM90nO3r0Vdn3r2zZ7j4NT7zjkJRdeeCE//vGP9yVnL774Iu+//z4/+tGPSEpKIj8/n6lTp3LWWWdhjOYiiIiEmv/M3ciaXaU8dMUkJWbiE90nOQuQiRMnkpuby44dO8jLyyM1NZXevXtzyy238OmnnxIWFsb27dvZvXs3vXv3DnS4IiLSBmt2lfDvOes567C+nDS6V+s3iHih+yRnrfRw+dN3vvMdXnrpJXbt2sWFF17IM888Q15eHkuWLCEyMpKsrCwqKysDFp+IiLRdbV09P//vcpJiIrntrDGBDke6kO6TnAXQhRdeyDXXXEN+fj7z5s3jxRdfpGfPnkRGRjJnzhy2bNkS6BBFRKSNHvpsE99uL+beSw6nh/bAFB9SctYJxowZQ2lpKf369aNPnz5ceumlnHnmmYwbN45JkyYxcuTIQIcoIiJtsCG3jLs+WsesMb05bZympIhvKTnrJN9+u38xQnp6OgsWLGj2OtU4ExEJbnX1ll+8tIy4qHBuP3uMFnOJz4UFOgAREZFQ8vj8zXy9tYjfnTmanomqZya+p+RMRETES1sKyvnb+2s4fmRPzp7QL9DhSBfV5ZMza22gQ+gU3eVziogESn295X9eXk5kWBh/PGeshjPFb7p0chYTE0NBQUGXT1ystRQUFBATo+51ERF/eearrSzM3sOvTx9Fn+TYQIcjXZhfFwQYY2YBdwPhwMPW2juanB8IPApkAHuAy6y1OZ5zVwK/8Vz6B2vtE219/8zMTHJycsjLy+vApwgNMTExZGZmBjoMEZEuKadwL3e8s5qZQ9O5cHL/QIcjXZzfkjNjTDhwL3ASkAMsMsa8Ya1d1eiyvwNPWmufMMYcD/wZuNwY0wP4HTAJsMASz72FbYkhMjKSQYMG+eLjiIhIN2Wt5ZevfIsF/nzuOA1nit/5c1hzCrDBWpttra0Gngea7u49GvjE83xOo/OnAB9aa/d4ErIPgVl+jFVEROQAe6tr2bZnLw9/tonP1udz66kj6d8jLtBhSTfgz2HNfsC2Rq9zgCObXLMMOBc39HkOkGiMSWvh3oOWxRhjrgWuBRgwYIDPAhcRka6porqOXSWV7Pb8FJRVU1BeRX6p57HR64qaun33TRnUg8uOHBjAyKU7CXQR2p8B/zbGXAV8CmwH6g55RyPW2geBBwEmTZrUtWf9i4hIq3YVV7J0WxG5pZXsKq5kd0nVvkRsV0klpZW1B90THmZIi48iLSGa9IQoBqXH73udlhBFekIU0wanExam4UzpHP5MzrYDjWdNZnqO7WOt3YHrOcMYkwCcZ60tMsZsB45tcu9cP8YqIiIhaldxJe98u5N3vt3J4i37pyZHhBl6JkbTMymGIRkJTB+SRq/kGHolxtA7OYaeidFkJEaTFBOpxEuCij+Ts0XAMGPMIFxSdhFwSeMLjDHpwB5rbT3wS9zKTYD3gT8ZY1I9r0/2nBcREWFXcSXvrtjJ28v3J2Qjeyfy05OGc/TwDPqmxJIWH6WkK5hYC8XbIKE3RGij+EPxW3Jmra01xtyIS7TCgUettSuNMbcDi621b+B6x/5sjLG4Yc0bPPfuMcb8Hy7BA7jdWrvHX7GKiEjw211yYA+ZtfsTstPG92FIRkKgQ5Smqspg06ew4UNY/6FLzmJSYNSZMOYcGHQMhAd6hlXwMV2lQOukSZPs4sWLAx2GiEiXtG3PXp75ciuR4YZhvRIZ3iuBwekJREW0b9G/tZb8smp2FldQXFFDaWUtpZU1lFR4Hitr9x0rraylcG81a3eX7kvIThvXh9PG9WFoTyVkQcVaKNgI6z9wP1u+gLpqiEqAwcdC1kzY8Q2seQeqSyEuDUadBWPPhYEzICzct/HU18OejbB9CexYCrWVEBbhfsI9j2GRTV5HQFw6HHahb2NpwhizxFo7qblzSldFRKRF63eX8p+5G3l92Q4MUG8t9Z5/00eEGbLS4xneK4FhPRMZ7knastLjiQgzFFfUsG1PBdsK95JTuJdteyrcY6F7rKypb/F9E6MjSIyJIDEmkqTYCPqmxCoh6yzWwu6VkD0HyvMgIsb9RMZCRDRExEJkzIHHK4thw8cuISvc5NpJHwFTroVhJ8OAaQcOZdZUwIaPYMUrsPwFWPIYJPSC0bNhzLnQ/0gIa0fiX5brErGcxZ6E7GsXG0BkHETFQ30t1Ne5x7oaqK85uJ2MkX5Pzg5FPWciInKQb3OKuXfOBt5buYvYyHAuPXIA1xw9mOTYSLLzylmfW8q63aWs213G+t2lbNmzl4a/TiLDDdER4ZRVHbgyMikmgv494shMjaV/qnvsmxJLSlwUiTERJMVGkhgTQUJURPedK1ZfB0VbIG8d5K2BgvWQNgwmXw3Rif5739JdsHGOS8g2zoHyXHc8PBrqqrxrIyIWBh8Dw06CoSdBqpelR6rLYd37sPJVl9zVVkJSP8ic5JK/8EgXR3iUS/DCozyvI12yWFvpesW2L3HDpgAmHHqNhn5HQL9J7jFjRMs9c/X1nqStxj1aC7Ep3sXfTofqOVNyJiIigBtq/GrTHv49ZwOfrc8nKSaCq6ZncdWMQfSIP/QE7sqaOjbklnmStjIqquvITI0lMzWO/j3cY3JsZCd9khBQW+WG//LXukQsfy3krYWCDS7ZaBCXBnsLILYHTL8JplzjmyStei9smQ8bP3EJWa5n8564dBhyHAw+zj0m9XWJS12V6+2qrYLaCqipdI+1nuPhUZA52fWodURVKax9zyVqBevdkGhttXts+Kmtwm0e1EjKQE8idoRL6nqPh6jgLhis5ExEpBvIL6viq017WJhdwMLsAkora+mXEku/1FgyU2PplxLX6HksMZGuF8Fay9x1edz7yQYWbykkPSGKq2cO5rKpA0iMUULVIdZCcY4bJty9wiVBu1dC/nqwDWU9DaQMcD076cPdY8ZISB8GsaluiG7uHW5SfUeStKJtsOZtWPsObF3gEp3waBg4zZOMHQ+9xrZvOLEzWet6GOuq3GcwYRCTHOio2kzJmYhIF5RfVsWX2fuTsfW5ZQDERYVzxMBUMhKi2V5UwfaiCnYWV1JXf+Dv+/SEKPqlxFJRU8e63WX0TY7hB8cM4cLJ/fclbtIG9fWQs+jAJGz3Kqgq3n9NygDoOcYNuWWMgozhbtjSm16etiZp1ro41rwNa96Cncvc8fQRbuhxyPFuLliQ9zB1VUrORERCVFVtHYXlNRSUV1FYXkNuaSXfbC06KBmblNWDqYN7MHVwGuP6JRMZfmDvR21dPbtLq9heWMH2or3k7KnYl7hVVNdxweT+nD2hX7tXXwrw6nWw7Dn3PDoJeo6GXmP2//Qc5ZsenkMlafV1sO0rl4ytedszOd+4IcdRZ8CI0yF9aMdjkA5TciYiEqRKK2v4YkM+X28tIr+sisLyavaUV7NnbzWF5TUHTaoHiN+XjKUxdXAPxjaTjEknW/U6vHgFTL0epv4QkvuD8fOihqZJ2pDjXE2x8jxXHmLwMTDyDBhxKiT29m8s0mYqpSEiEiSstWzMK2fu2lw+WZPLos17qKmzREWEkR4fRY+EKFLj3P6OqfFRpMVHkRofRY+4KHrER5GWEMXAtHglY8GkLBfeugX6TICTbnerCDtD5iS47CXYtgjm3eEm9w8+zvWQDT0JYpI6Jw7xOSVnIiJ+VllTx8LsAuauzeOTNbls3bMXgOG9EvjejEEcN7InRwxMVcIViqx1iVlVGZxzf+clZo31nwyXvdz57yt+o+RMRMRHyqpq2VVcSW5JJbtLK9lVXMWSLXv4YkMBFTV1xESGMX1IOtccPZhjh2fQv4cmYoe8Zc+7+V0n/Z+bUybiA0rORES8VFNXz+qdJSzbVkROUQW5JVXsKnaJ2O7iSsqr6w66JzM1lu9MyuS4kT2ZNjhNqyC7kuIcePd/YMB0mHZDoKORLkTJmYhIC4oravh6ayFLNheyZEshS7cVUVHjErCo8DB6JkXTKymGUb2TOGZ4Br2TYui178edi4/Wr9kuyVp4/UZXTf7se32/J6R0a/qtISKCm6i/pWAvS7YUsnhLIUu27GF9bhnWQniYYXSfJC6c3J8jBqZy+MBU+ibHYPy9Gk+C1+JHXGX90/8BPQYHOhrpYpSciUi3lVtayfwNBXy+IZ8vNuSzs9htm5MYE8ERA1M5c3xfjshK5bDMFPWAyX4FG+GD/3VFXCd9L9DRSBek3zYi0m2UV9Xy1aY9fL4hn8/X57N2dykAKXGRzBiSzvShaUzO6sHQjITuu/G2HFp9Hbx2vasjdta//V/LTLolJWci0qVYa9lbXcee8moK91ZTUF7N8m3FnkKvhdTWW6IjwpgyqAfnHN6PmUPTGd0nScmYeGfBv2HbQjjnQUjuF+hopItSciYiIcNay7Y9FazaWcK63aXkl1XtS8L2lNe46vp7q6murT/gPmNgXL9krjl6MEcNTefwgalaNSltl7saPvmDq7o//oJARyNdmJIzEQlKlTV1rN9dxqqdxazeWcqqHSWs3llCaaPtjFLiIukR5yro90uJZVy/pH3V9Pc/RjIkI4GUuKgAfhoJeXU18OoP3J6ZZ/xTw5niV0rORCQolFXV8v6KXXy2Po9VO0vYmFdOXb3b+zc+KpyRfZI4e2I/RvdNYnSfJIb3SiQ2Sr1fQal6L9RVQWxqoCPxnU//DjuXwYVPQ0JGoKORLk7JmYgETFVtHfPW5vH6sh18tGo3VbX1ZCRGM65fMieP7r0vERvQI05zwkLJ6ze4TblvXASRMYGOpuN2fAOf/g3GXwSjzgx0NNINKDkTkU5VX2/5ctMe3li2nXe+3UVxRQ094qO4cHJ/Zk/oy+EDUlU/LJRVlcHad6C2EpY8DlOvC3REHffmzZDQC079S6AjkW5CyZmI+J21lpU7Snh96XbeXLaTXSWVxEWFc8qY3pw1oS8zh6Zr0++uYv37LjFL7Auf3QmHXwFRIbyHaHGOG8485U8QmxLoaKSbUHImIj5TWVPH1j172ZxfzuaCcjYXuOfZeeXsKqkkIsxw7IgMfnX6KE4a1UtzxrqiVa+7XqbzHoLHT4dFD8GMmwMdVftlz3OPg48NaBjSvSg5E5F22ZhXxpw1uWzMK2dLQTmb88vZWVKJtfuvSYmLJCstnqmDezB5UA9OG9uH1Hitmuyyqsth3Qcw8VLImglDToDP/wlHfBdikgIdXftkz4X4DOg5OtCRSDei5ExEvLYht4x3vt3JO9/uZM0uV10/NS6SrPR4jhycxsC0OAalxzMwLZ6stDiVr+hu1n8ItRUw+mz3+vhfw0PHw5f3wzG/CGho7WItbJoHg45R6QzpVErOROSQNuSW8vbyXbzz7c592x1NGpjKb88YzayxvembEhvgCCVorHod4tJh4HT3ut8RMOJ0mP9vmHJN6JXWyFsDZbs1pCmdTsmZiBykuYRsclYqvztzNKeO7UPv5C5QHkF8q6YC1r3vKueHNZpLeNyv4P4ZLkE74X8DF1977Jtvdkxg45BuR8mZiFBYXs2C7AI+35DP/A35bC7YizGuh0wJmXhlw0dQUw5jzj7weO+xMOZcWPgfmPpDiE8PSHjtkj0XegyGlAGBjkS6GSVnIt1QZU0dizbv4fMN+XyxIZ+VO0qwFhKiIzhyUA++O2MQs8b2pleSEjLx0qrXIbYHDJx58LljfwmrXoPP74JT/tjpobVLXS1s/hzGnR/oSKQbUnIm0sVZaykoryY7r9wlZOvzWbK1kOraeiLDDRMHpPLjE4Yzc1ga4zNTVG+su9kyH764B86+D+J6tK+NmkpY+x6MPQfCm/lrJWM4jL8QFj0M026EpD4di7kz7Pgaqks130wCQsmZSBdRvLeGTZ6SFps8P5sL3GNp5f7Nwkf1SeLKaQOZPjSdKVk9iI/Wr4GQUVcDH98OY86Bfod3vL3S3fDilVCeCwvubf+csI2fuESmYZVmc475BXz7X1eY9vS/t+99OlP2PMDAoKMDHYl0Q/qtLBKCyqpqWbR5Dws3FrBkSyHZ+eXsKa/ed94Y6Jscy+CMeM6e0I+s9HgGpccxPjOF9IToAEYuHbLsOZh/j3u8Zg6k9G9/W/V18Mr3oaoU+h8JXz0I029s34rKVa9DTMqhE5keg2HiZW5Lpxk/Cv55XNlzoc/49vcminSAkjOREFBRXceSLYXM35jPguwClucUU1dviQw3jM9M4ZQxvchKiycrPZ7B6fH07xFHTKSq73cpdTVu8+30EVC6E56/BL73fvu3Rpr3V9j0Kcy+F/ocBvfPhC8fgGNvbVs7tVVuL81RZ0F45KGvPfrnsPRZ9znO+lf74u4M1eWw7Uu3gEEkAJSciQSh6tp6lmwpZEF2AQs3FvDNtkJq6izhYYbxmcn84OjBTBuSxqSBPbQFUnex9Fko2gqXvAgYePYCeP16OP+xthdIzZ4L8/4Ch10MEy519488Axbe5xKSmOS2tVVVcvAqzeYkZ8Kk78FXD8GMH0PaEO/eo67GJYD9JkFyP+9ja6+tC6C+RvPNJGCUnIkEkb3VtTz75VYe+iyb3SVVhBkY0zeZ780YxNQhaUzO6kGC5oh1P7XV8Nnfoe/hMOxkl0ydeBt89DvoNcb1SHmrdDe8fA2kD4fT79yf2B39M1jzlhvebEt7q16H6GRXRd8bM38CS55wyeG5Dx762vo6+PYlmPtnKNzkSnJ85zHvY2uv7LkQHgUDpvn/vUSa4dff8saYWcDdQDjwsLX2jibnBwBPACmea2611r5jjMkCVgNrPZcutNZe589YRQKpuKKGJ+dv5tEvNlG4t4YjB/Xg92eNYdqQdJJjWxkqkuC1+XP3l3z/KR1rZ5mn1+y0RsnUjJth90r45A9u38eRp7feTn0dvHy1m2d25RsQFb//XN+JMOwUtzDgyOsgOrH19mqrXUI38jSI8HKrrsRecOS1boXozJ9Az5HNxFkPa96EOX9yVfp7j3Pz2da974rdRvp5V4rseW4eXnuHjEU6yG9r5o0x4cC9wKnAaOBiY0zTnWN/A7xorZ0IXATc1+jcRmvtBM+PEjPpkvLLqvjLe2uYcccn3PnhOib0T+Gl66bxwg+mMWtsHyVmoaxgIzx9Hjx1LhRta387tdXw6Z1uK6RhJ+0/bgycdY9Lql65Fnavar2teX+BzZ+5HrOeow4+f8wvoKIQFj3iXWybPoXK4kOv0mzO9JtdYjj3zwcet9YlYA8eAy9e4V5/5wm49lOXyNWUw4aP2/ZebVVeALuWe98TKOIH/ixoNAXYYK3NttZWA88Ds5tcY4Ekz/NkYIcf4xEJGjuKKrjtjZXMuOMT7p+3kWOGZ/D2j2by2HenMClLq8NCXn09vH4DhEeDrYc3bnSJRnssexaKt7pCrk3nlkXGwkXPukTn+Yth756W29k4xy0CmHApTLy0+WsyJ8GQE2D+v9yk+Naseg2iEmHIcV5/HADi02Dq9e7+ncvdsex58MjJbi5dVQmc8wBcv8DNZQsLg6yZrsjtqtfa9l5ttflT96j5ZhJA/hzW7Ac0/udiDnBkk2tuAz4wxtwExAMnNjo3yBjzDVAC/MZa+1nTNzDGXAtcCzBgQJAvyxYBNuWX85+5G3j1m+1YC2dP7McPjx3CkIyEQIcmvvTVg25S+dn/ccNwb/8EljzmJsO3RW01fPp3NxF+6InNX5PUFy58Bh4/zfU2Xf7qwasmS3fBK9dAxgg47W+Hfs9jfgGPngKLH3OlNVpSV+OGNEecChHtKM8y7Qb46gF49xcu3k2fQlI/OOOfruRG088QHumGble+5oreRvpp94rsuRCd5HokRQIk0KXALwYet9ZmAqcBTxljwoCdwADPcOdPgGeNMUlNb7bWPmitnWStnZSRkdGpgYu0RXZeGT95YSkn3DmX15fu4JIpA5j782P5+3cOU2LW2OLH4L5pUL030JG0355s+Pj3MPQktxpy0vfcENkH/wuFW9rW1tJnoHhb871mjfWfDGfe7YYs3//Vgefq6+Dl77uesO88ceA8s+YMmOrmd31xt0ssW7L5MzcE6s0qzebEpsD0H7kkNnc1zLoDbvoaJn235ZIcY852xW6z57TvPb2RPc/10jW304FIJ/FncrYdaFwhMdNzrLGrgRcBrLULgBgg3VpbZa0t8BxfAmwEhvsxVhG/aEjKTvzHPN5ZsZOrZw7is/85jt/PHktmqiYbH2TTPMhd5XpUQlF9Pbx+E4RFuGTJGPcz+9/u/Bs3umu8UVvtqulnToahJ7R+/YRL3NZIXz3oCr02mHtHo3lmzUy+b84x/+N2DVjyRMvXrHodohJgyPHetdmcGTe7Xr+bl7kSHq31hg06xhW7Xfla+9/zUAq3uFWhmm8mAebP5GwRMMwYM8gYE4Wb8P9Gk2u2AicAGGNG4ZKzPGNMhmdBAcaYwcAwINuPsYr4VLNJ2S+O59enj6ZnYhfZTHzbIjevqqLQd23mr3ePn9/l23Y7y+JHYMvncMqfDqzHlTIATv6DG7pb8qh3bS192tNrdqv3dcxO/L2bM/b2z9yemRs/cQVfJ1zmkjdvZc2EgTPgi3+6IcSm6mph9Zsw/JSOrZwMj4RRZ7Tem9f4+pGnw9p3XfFbX9s0zz1qvpkEmN+SM2ttLXAj8D6uLMaL1tqVxpjbjTFneS77KXCNMWYZ8BxwlbXWAkcDy40xS4GXgOustYeY6SoSHA6VlGUkdpFtk2oq4cPfwqMnwzdP+271XH09FGxwPTGVJW5YLZQUboYPf+eSo4mXHXz+iKtg8HHwwW/dtYdSW+VWaGZOce15KzwCzn8UUgfCC5e7VZwZI1ufZ9acY37hdiJY+vTB57Z8AXsL2r5K0xdGnw1VxZ69L30sey4k9HZz80QCyK+D6tbad4B3mhz7baPnq4AZzdz3MvCyP2MT8aXsvDL+/ckGXlu6naiIMK6eOYhrjx7SdRKyBjlL4LUfQv5amHi5mxOVv843bRdvg9pKGD0b4jNg4f0w5QeQ1Mc37fuTtfDGTWDC9g9nNmWM27Lovmnw+o1wxRtuFWJzvnkaSnJcqYy2Vv+PTYGLn4eHjof6WrjyrfbV6xp0jEsOP7sLJl5xYB2zVa9DZFzLixT8afAxrujtqtdg+Mm+a7e+3iV8Q09o+5+5iI8FekGASMiy1rJkyx6uf2ZJ1+4pA9eT89Ft8MiJUF0Gl73s5lGlZkHe2tbu9k6BZ0gzfbibAF9fC5/+1Tdt+9uSx9yQ5cn/d+jNyFP6w6w/uTlgix5u/praKvjsH64Ianvnc6UPc/tuXvW29/PMmjLGzT0ryXHlPBrU17khzWEnB6ZIa0S0WyG65i03L89XclfB3nwNaUpQ0HIUkTaqqavn3RW7eOTzTSzbVkRSTATXHD2Y788c3LUSsgbbv4bXroe81a637JQ/7t97MX2E73rOGuabpQ2DhAy3am/RI26Su7d7MAZC0Va3EnPwsW7osjUTL3c9Tx/9DoadCD0GH3j+m6dcQjT7Xx3rwenVtOZ3Oww9wW0Z9dmdrj5aeKRbXVme2/5Vmr4w5mxY/ryrSear3rvsue5RiwEkCKjnTMRLxXtr+M/cjRz91zn86LlvKKmo4fbZY1jwyxP45amjul5iVlsFH/8fPHyiqwJ/6Uuut6zxptgZw908sbrajr9f/nq3Ei8+3b0++ucQEQNz/tjxtv3FWnjjR+7xTC+HII1x14ZFuuHNxqs39/WaTXXz0wKtofesaCssf8EdW/U6RMS6UiGBMvg4V/zWl6s2N81z/zDojI3VRVqh5EykFdl5ZfzvayuY+ueP+ct7a8hKi+fhKybx8U+O4YppWcR3xY3IdyyFB491m20fdpGr1D6smb+M00dAXTUUtbF+V3Py17nhuIYEJ6EnTLseVrwMO5d1vH1/+PpJV3Pr5NvdJHxvJfeDWX92E+u/arT599dPQsn2tq3Q9Lfhp0Dv8a73rLYaVr3hevyiA1ifLzIGRsxyQ5t1NR1vr7YaNn/h5rOJBIEu+LeKSMdV1tTx+fp8nvtqKx+vySUqPIwzD+vL92ZmMaZvcusNhKKaCld6YfWbsPxFNyn/khfdX84taVjVlre240OPDSs1G5t+k5ub9fHtbp5bMCnOgfd/DVlHwRFtrPwPrrTFqtfdXL5hJ0Fypus1GzAtuOY9GeNWbr5wmavmX7YrMKs0mxo9G779r9tcvq3bRzW1fYnbtzOY/tylW1NyJuKRW1rJJ6tz+Wj1bj7fkE9lTT094qP40fFDuWzawK5Tn6yxvXvcRtNr3nKJWc1eN2x5xJVwwm8hNvXQ96cPc4/5a3GbfLRTZYkr25A29MDjMclw1E/hg9/Aps9g0FHtfw9fshbevBlsnVuB2dKqy0MxBs78J9w31c3pG3sulO6Ac/4TPL1mDUacDj3HuIUP4dGHTtg7y9ATITLerdrsaHKWPdettM2a6YvIRDpMyZl0W9ZaVu8s5ePVu/lo9W6W5RQD0C8llgsm9eeEUb2YOrgH0RHhnR9c/nrY9iUk9nH7DSb1hZiDdjBrn6JtsPYdl5Bt/sIlGIl93YTvkad7tq5pYfucpmKSXYx5HVwUULDBPaY3sxHI5O/Dwv+4LZGu/jA4Epelz8CGj+DUv0GPQe1vJ6kvzPoLvHYdbF8MA6YH54T0sDA45ufw36tcUhSdGOiIXPHb4afA6rfgtDs7tt3SpnnQZ0Lr/xgR6SRKzqRbsdayILuA91bs4uPVuWwvcnsHHtY/hZ+eNJwTR/diZO9ETCATgJoKeOb8gwuVRiW6uUpJfT0//dxPQk+w9W4yeV0N1FW5eWC11e6x4aemwg0B7Vzq2ssYCTN/DCPPcJs8t/czpw/v+IrNhpWaDT1xjUXGujlYb9zkksqRp3fsvTpq7x5471eugv7k73e8vcMucsOb694NrrlmTY2a7erOjTs/0JHsN3o2rHwFts53+4G2R1Up5CxyQ+giQULJmXQbm/PLue3Nlcxdm0dMZBgzh2bwoxOGctzInsE1ZDnvry4x+84TkNDLTRAv2Q4lO/Y/7l4FZbsB62WjxtWH6j3ObfEz8gxIH9r6bd5IH+5W8lnb/sQifx2YcEhtoRfqsEvgi3vc3LPhsyAsAL2ZDZa/4CrUz7qjfcOZTRkD5z7o5j21N8HoDGFhcFqQ1Z0bdrIrhrvytfb/2W2Z72rqab6ZBBElZ9LlVdbUcd+cDdw/L5uoiDB+c/ooLps6kJjIAP4F35Ldq2D+PS4Zaa2OVF0NlO5yNafCIiA8av9PRLQbmgyP9hzz4//qGSOgqsTF0t5q/gXrXUHbxlXoGwuPgON/A/+90iVHbdkn0pesdSsq+x4Ofcb7rt2YpI7Pm+qOouLcYorVb7otqtqTtGfPc/+f9D/S9/GJtJOSM+nSPlq1m9veXElOYQWzJ/TlV6eNoldSEPWSNVZfD2/9GKKT3CbZrQmPdBXnD1WRvjM0zBPLX9v+5Cx/ffPzzRobPdvNC5rzJxh7nktAO9v2Ja6S/Jkhtu9nVzZ6thsW3roQsg7aDbB12XNhwNSObeAu4mOqcyZd0taCvVz9+CK+/+RiYiPDee6aqdx90cTgTcwAvn7cLQI45Y8QnxboaLy3r5xGO+ed1ddBwcbWh1mNgRNvc3twLn7Uu7atdW1XFrcvtqaWPO5WCI49zzftSccNO8UVK171etvvLcuF3JWqbyZBRz1n0qVU1tTxwLxs7pu7gYgww69PG8VVM7KIDA/yf4eU7oIPb3M1sw67ONDRtE1CL7cRdX4799gs2uoWMbTWcwZu6G/QMfDp32DiZc2vGize7lbfZc9zj6U73XykK99sX3wNqkphxSsw9pzgWK0oTnSCW0G6+o22zwPc9Kl71HwzCTKtJmfGmDOBt6219a1dKxJIc9bk8rs3VrJ1z17OGN+H35w+mt7JQdxT1th7v4TaSjjjn8G7Wq8lxrhtnNq7AXpDGY20ZlZqNufE38FDx8OCe93qxopCtwo1e65LyBo2UI9Ld0lZeKSbp7btK+g/pX0xgkvMasrh8Kva34b4x+izXWmYnK/cEKW3sue6cjB9JvgpMJH28abn7ELgn8aYl4FHrbVr/ByTiNcqa+p4e/lOnlq4haXbihiSEc8z3z+SGUPTAx2a99Z/6MoBHPdr362g7GzpI2DDh+27t6EMhzc9ZwD9joBRZ7nVm+vec1s72Xo33Jg1w20+PvgYVzQ1LAyqy92f8Wf/gEueb1+MAF8/ARmjIHNS+9sQ/xh+ipvUv+p175Mza10yn3VUYFf/ijSj1eTMWnuZMSYJuBh43BhjgceA56y1pf4OUKQ5Wwv28syXW3hx8TYK99YwOCOe3581hounDCAqIsiHMBurLoe3fuKSmxk3Bzqa9ssYDkufhooiiE1p2735613xz7bMszvht64nLCLGbcw96BiXtDW32jMqHqb+0G2gvmsF9B7btvjA3bd9CZzy59Dr2ewOYpJg6AkuOTv5j60PbVrrVngWb4UZP+qcGEXawKs5Z9baEmPMS0As8GPgHODnxph7rLX/8mN8IvvU1Vvmrs3lqYVbmLcujzBjOGVMLy6bOpBpg9MCWzi2vebe4f6C+O67gVl96CvpnkUB+evaPnTozUrNg95vGPysDcOoU66BL+6Gz++C8x9p23sBfPOUK0ly2EVtv1c6x+jZrkjx9iXQf3LL1+1cBh/8r5uP2GOIu08kyHgz5+ws4LvAUOBJYIq1NtcYEwesApSciV/ll1Xx4uJtPLNwK9uLKuiZGM2Pjh/GxVMGhM6csubs+tbNmzr8Chg4PdDRdEyGJ7nKW9v25KxgPQw9yfcxNRabCpO+Bwv+Dcf/GnoM9v7emkpY9jyMOhPievgvRumY4bMgLNLttdlccla0DT75g5t/GJvqts2a9L2Wa+uJBJA3PWfnAXdZaz9tfNBau9cYc7V/whKBksoa7nh3DS8tzqG6rp7pQ9L49emjOGl0r+Bffdma+jq3cXZcD1exP9SlDHRzftq6YrOy2O100Ny2Tb427Qb48gHXg9aWOmWr34TKIpdES/CKTYEhx8OqN1ydwIae9MpiN99w4X/c6xk3w1E/cQsBRIKUN8nZbcDOhhfGmFigl7V2s7X2Y38FJt3bnLW5/OqVb9ldUsklRw7gqulZDO3ZhcoXLH7UDb+c+3DX6I0JC4e0ofv3yPRWfsOG552QnCX2duU3vnnKzVNL6uvdfV8/4ZLPrCDeWkmc0bNh/fuw42voNc79fzbvL25F7/gL3S4TgS7aLOIFb5Kz/wKNx1zqPMcOMagv0j7FFTX84a1V/HdJDsN6JvCf62cwoX9KoMPyrZId8NHv3b/yg2kT6Y7KGA47lrbtnrau1OyoGT9yhWQX3OuK/bamYCNs/gyO/1/f7KMp/jXiVLeV2Sd/hMJNsCfbLRY5+f+gz2GBjk7Ea94kZxHW2uqGF9baamOMBunF5+asyeWXr3xLXlkV1x87hB+dMCw4979sqmQnLHkM4jNckpExwhVmbWmBwrv/A/U1cPqdXWvlX/oIt1quphIivZwLWLDe/WWamuXX0PZJzXIJ8eLH4Kiftt5r+c1TYMJgwqWdEp50UFwPl4xt/Bh6joZLX3IFarvS/2fSLXiTnOUZY86y1r4BYIyZDeT7NyzpTooravi/t1bx0pIchvdK4MErjmB8Zkqgw/JObTW8cKkbomwsJtklKxnDPY8j3fPdK10l8xN+17ZJ6aEgY7irN1awwftyFfnrIHWQKxTbWWbe4iaFf/kAHPfLlq+rq4Glz7rtgdq7Z6h0vtP+BrtXwMgzVL9MQpY3ydl1wDPGmH8DBtgGaGas+MQna3bzy1e+Jb+smhuPG8pNJwwlOiKEfqF+8BuXmF3wFGROdhPi8zw/+etg3QfwzdMH3tNzNEy/KTDx+tO+chpr25Ccbeic+WaN9Rzl/uL+8n6YfmPLWzGt/8AtVjjiys6NTzombYj7EQlh3hSh3QhMNcYkeF6X+T0q6fKK99Zw+1urePnrHEb0SuThKyYzLjPEVk+teBm+egCm3Qijz3LHkvocvE/f3j0uUctbC3s2wviLOrenqLOkDXVDgN5ugF5f5/48hp/s37iaM/MnbrufxY+1XIR0yROQ0Nv/ZT5ERJrwqgitMeZ0YAwQ01Do01p7ux/jki6qpLKG577cykOfbaJwbzU3HT+UG48Psd4ycAnIGz+C/kfCibcd+tq4Hm5Lmbbs+ReKImPcqkZvy2kUbYG6au/31PSlzCPc3KQF98KUaw+eI1e83W1HNfMWCPfq16SIiM94U4T2fiAOOA54GDgf+MrPcUkXs6u4kse+2MSzX26ltKqW6UPS+NVpoxjbL8R6y8BtufTiFW7roO883jV7wdorY4T3PWcNZTc6a6VmU0f9FJ48C5Y964qRNrb0WTd/buJlgYlNRLo1b/5JON1aO94Ys9xa+3tjzJ3Au/4OTLqGtbtKefDTbN5Ytp26esvp4/ty7VGDQ28Is4G18NYtkLcGLn/V+1pZ3UX6cNg4xw1ZtjYZe19yFoCeM4BBR0O/SfD5P2HiFft7yOrr4ZsnXc9aV1u0ISIhwZvkrNLzuNcY0xcoALR0SVpkrWVh9h4e/HQjc9bmERsZzqVHDuTqmYPo3yMu0OF1zJLH3Uq/434NQ44LdDTBJ2ME1FVB4ebWJ2Xnr4O4tMAV4TXGVYp//hJY+QqMv8Ad3zQXira6FbUiIgHgTXL2pjEmBfgb8DVggYf8GZSEprp6y3srdvHApxtZnlNMWnwUPzlpOJdPHUhqfBcojbfjG3j3FzDkBDjqZ4GOJjg1DFHmr2s9OSvYEJj5Zo0NPxUyRrkN0cee7wrNfv2k23tx5BmBjU1Euq1DJmfGmDDgY2ttEfCyMeYtIMZaW9wZwUnoqKyp46bnvuHDVbvJSovjj+eM5bzDM0OjiKw3KgrhxSshviec+5CqxbckvdEG6CNOPfS1+evcZtWBFBbmes9euQbWvec2bV/9Fky5xvtCuiIiPnbI5MxaW2+MuReY6HldBVR1RmASOvZW13Ltk0v4fEM+vzl9FN+dMYjwsC5Ukbu+Hl79odt26bvvQnxaoCMKXrEpbneE1vbYrCiE8rzAzTdrbMy58Mkf4LM7YczZbveGiZcHOioR6ca8+ef/x8aY84zR/hdysOKKGi5/5Cvmb8zn7985jO8fNbhrJWYA8++Bde+6vRj7a0vZVqUPb72cxr4NzwO0UrOx8AiYcTNsXwyf/s0VE+41OtBRiUg35k1y9gPcRudVxpgSY0ypMabEz3FJCMgvq+LiBxeyPKeIey85nPOPyAx0SL63+XP4+HYYc46rhyWtayinYW3L1xR4etYCPeeswYRLXY9fZTEcrh0BRCSwvNkhoIW9TaQ721lcwaUPf8mOogoeumISx47oGdiArIWCjVBZBFWlUF0GVWWexyavq8tdjbKYJIhOavSY7HnuebQWXvqeK6dw1r+0ebK30kdAVbHb+iixd/PX5K+DsEhIHdi5sbUkMgaO+R+3MGDMOYGORkS6OW+K0B7d3HFr7ade3DsLuBsIBx621t7R5PwA4AkgxXPNrdbadzznfglcDdQBP7LWvt/a+0nn2FJQzqUPf0nR3hqe/N6RTBkUoFIIjX39JLzZwjY8ACYcohMgKhGi4qC2EipLoKrEFRttSUQsXP5ay/svysEyGi0KaDE5Ww89OnnD89ZMvtr9iIgEmDelNH7e6HkMMAVYAhx/qJuMMeHAvcBJQA6wyBjzhrV2VaPLfgO8aK39jzFmNPAOkOV5fhFuy6i+wEfGmOHW2jovP5f4ybrdpVz28JfU1NXz7DVHMj4zJdAhOStfgdRBcOpfPUlYwv5kLDrB9ZQ11/NlretNa0jU9j0Wu8d+R2j+UVvt2wB9HQw+pvlr8tcHx3wzEZEg5M2w5pmNXxtj+gP/9KLtKcAGa222577ngdlA4+TMAkme58nADs/z2cDzntWhm4wxGzztLfDifcVPvs0p5opHvyQyPIwXfjCN4b2CpDepotDNDZt2Y9s30TbG9YpFJwL9/BJet5PY2w0T57WwKKCuFvZkw8jTOjcuEZEQ0Z5iTTnAKC+u6wdsa3Jf07/9bgMuM8bk4HrNbmrDvRhjrjXGLDbGLM7Ly/MuemmXrzbt4eKHFhIfHcF/rwuixAxg/YdQX6uiocHCmEOv2Cza4spVBMtiABGRIOPNnLN/4Xq4wCVzE3A7BfjCxcDj1to7jTHTgKeMMWO9vdla+yDwIMCkSZMOsTRMOmLu2lyue3oJ/VJiefr7R9InOTbQIR1ozdtupV2/IwIdiTTIGAEbPm7+XKA3PBcRCXLezDlb3Oh5LfCctfYLL+7bDvRv9DrTc6yxq4FZANbaBcaYGCDdy3vFz6y1PD5/M394ezUjeiXy5NVTSE+IDnRYB6qtgg0fwbjzVbU/mKQPg6XPuLl7MU02uc9f57lmaOfHJSISArxJzl4CKhsm4xtjwo0xcdbava3ctwgYZowZhEusLgIuaXLNVuAE4HFjzCjcgoM84A3gWWPMP3ALAoYBX3n5mcQHqmrr+O1rK3lh8TZOGt2Luy6cQEK0N/+5dLJNn7oJ/RrSDC4NiwLy1h1cuDd/HcRnuP0rRUTkIF7tEAA0HseKBT5q7SZrbS1wI/A+sBq3KnOlMeZ2Y8xZnst+ClxjjFkGPAdcZZ2VwIu4xQPvATdopWbnyS+r4tKHvuSFxdu46fihPHDZEcGZmAGsecutzBzUbMUXCZSMhhWbzcw7C4YNz0VEgpg3f+PGWGvLGl5Ya8uMMXHeNO6pWfZOk2O/bfR8FTCjhXv/CPzRm/cR31mxvZhrn1zMnr3V/PuSiZwxvm+gQ2pZfT2sfReGnggRQTbc2t2lDITwqP1DmI3lr1NPp4jIIXjTc1ZujDm84YUx5gigwn8hSaC8vXwn598/H4CXrpse3IkZwPYlrgq9/qIPPuERkDbUDWs2tncP7C0Ijg3PRUSClDc9Zz8G/muM2QEYoDdwoT+Dks5VX2+566N1/OuTDUwamMp/LjuCjMQQ6Ila8xaERcCwkwIdiTQnfTjsWn7gMa3UFBFplTdFaBcZY0YCnkkkrLXW1vg3LOksZVW13PLCUj5ctZsLJ/Xn9rPHEB0RHuiwvLPmbciaCbEpgY5EmpMxAla/ATWVbu9KaLThuVZqioi0pNVhTWPMDUC8tXaFtXYFkGCMud7/oYm/bS3Yy3n3zeeTNbncduZo7jhvXOgkZnnr3F/0GtIMXunD3b6lezbuP5a/zs1FSwmSDc9FRIKQN3POrrHWFjW8sNYWAtf4LSLpFMu2FTH73s/ZVVLJk9+bwlUzBmGa23syWK192z2OODWwcUjLGlZsNt7GKX8D9Bjs5qSJiEizvPkNGW6MMdZaC/s2NI/yb1jiT0u27OGqRxeRGh/Fk9+bQlZ6fKBDars170CfCZCcGehIpCVpQwFz4IrN/HXQc2TAQhIRCQXe9Jy9B7xgjDnBGHMCrh7Zu/4NS/xlYXYBlz/yFRmJ0bzwg6mhmZiV7oKcRRrSDHaRsZAyYH/PWV0NFG5SjTMRkVZ403P2P8C1wHWe18txKzYlxHy2Po9rnlxM/9Q4nrnmSHomxgQ6pPZZ+y5gYeRpgY5EWpMxYn/PWeFmt0G9VmqKiBxSqz1n1tp64EtgMzAFOB5X8V9CyCdrdnP1E4sZlJ7A89dODd3EDGDtO5CaBT1HBzoSaU36cFc+o76uURkN9ZyJiBxKiz1nxpjhwMWen3zgBQBr7XGdE5r4ynsrdnHTc18zsncST109hZS4EJ4yWFUK2XNh8jUQSgsYuquMEVBXBUVb9vegqYyGiMghHWpYcw3wGXCGtXYDgDHmlk6JSnzmzWU7+PELSzksM5nHvzeFpJjIQIfUMRs+grpqGHl6oCMRbzTeAL1gPcT3VF06EZFWHGpY81xgJzDHGPOQZzGAuipCyMtLcrj5+W84YmAqT159ZOgnZuBWacalQf8jAx2JeCPDM78sf50b1tR8MxGRVrWYnFlrX7PWXgSMBObgtnHqaYz5jzHm5E6KT9rpua+28rOXljF9SDpPfHcKCdFdoK5UXQ2sfx+Gz1KdrFARm+p6y/LXepIzDWmKiLTGmwUB5dbaZ621ZwKZwDe4FZwSpJ6Yv5lfvvItxwzP4OErJxEbFSJV/1uz5QuoLNaQZqjJGAFbFkDFHvWciYh4wZs6Z/tYawuttQ9aa0/wV0DSMU8t2Mzv3ljJSaN78cDlRxAT2UUSM3B7aUbEwmCtSQkp6cP3b+GkGmciIq1qU3Imwe2TNbv53RsrOXFUL+679PDO3yezrhYKNoLbTMK3rHXzzYYcD1Fxvm9f/KdhGydQGQ0RES9o4k4XsXJHMTc++w2j+yZxz8UTiAwPQN79ye3wxd2Q2BeGn+Lmhg062jfJ1M5lUJIDx/2q421J52oYygyPdjsGiIjIISk56wJ2FVdy9eOLSY6N5JErJxMXFYCvtSwXvnwQso5ypRK+/S8seQwiYmDQMZ5k7ZT274W55m0wYS7hk9DSkJylDYGwLjTMLiLiJ0rOQlx5VS1XP7GI0soa/nvddHolBajy/xd3u2KjZ97t/hKurXIT+Ne977ZbWv8+vA30Gre/Vy1zkveFZNe8DQOmQXyaXz+G+EFSX4hKVPFZEREvac5ZCKurt9z8/FJW7yzh35cczui+Sd7fXL0X3rzZ7XfYUaW7YdEjMP5Cl5gBRES7+WGn/gVuXgY3fAUn3Q4xSfD5XfDIifDQcbD+w9bnqO3ZBLkrtUozVBkD5/wHjvlFoCMREQkJ6jkLYX96ZzUfrd7N788aw3Eje7bt5m0LYcnjLjm7/LWObYX0xd2uav/RP2/+vDFuUnjGCJhxM+zdA6vfgM/uhGfOh8zJbi7Z4OOaj2PtO+5xhDY6D1mjzgx0BCIiIUM9ZyHqqYVbeOTzTVw1PYsrp2e1vYHdq9xj9lxY9Vr7AyndDYub9Jq1Jq4HHHEV3LjEDYOW7ISnzoHHToVNnx58/Zp3oOcY6DGo/XGKiIiECCVnIWjeujxue2Mlx4/syf+eMbp9jeSuhvgM6D0O3vuV21C8Pb6421XuP/pnbb83IsolaT/6Gk77u+vFe+JMePwMV7QUoLwAts7XkKaIiHQbSs5CzJpdJdzwzNcM75XIPRdPJDysncORuSuh1xg4/R9QugPm/aXtbZTucr1mh13kfa9ZcyKiYco18KOlMOsvkLcWHpsFT54Nn/8DbD2M1JCmiIh0D0rOQkhuqSuZERcVzqNXTWr/fpn1dZC7BnqOhv5TYOLlsPA/rjetLTrSa9acyBiYep1bQHDyH2DXt7Dg35DUD/pM8M17iIiIBDklZyGiorqOa55YzJ7yah65cjJ9kmPb31jhZqitcMkZwIm/h6gEePtn3lf3L90Fix+Fwy6GHoPbH0tzouJg+k0uSTvlz653ryMLFkREREKIkrMQ8YuXl7N8ezF3XzSBcZnJHWss17MYoJcnOYtPgxN/B1s+d8VjvfH5Pz29Zj/tWCyHEp0A066HESo8KyIi3YeSsxCwMLuAN5ft4McnDOfkMb073uDuVYCBjJH7jx1+JfQ9HN7/NVQWH/r+kp2u12yCH3rNREREujklZ0HOWssd766hd1IMPzjGR4lQ7kpIzYKo+P3HwsLh9DuhPA/m/OnQ93/xT7B1cJSP5pqJiIjIPkrOgtz7K3exdFsRPzlpODGRPtqXMHe1W6nZVL/DYdL34KsHYefy5u8t2QGLH/PMNVPdMREREV9TchbEauvq+ev7axnWM4FzD+/nm0ZrKqFg4/7FAE2d8L8Qmwrv/Azq6w8+//k/Xa+Zr1ZoioiIyAGUnAWx/y7JITuvnJ+fMoKIcB99VflrXXLVq4XkLDbV7YG57UtY9uyB50p2uC2fJlzihkVFRETE55ScBamK6jru+nAdRwxM5aTRvXzXcMO2TS31nAEcdgn0PxI+/K3bB7PB53d55pr5cYWmiIhIN6fkLEg9+sUmckuruPXUkRhf1vjKXQXh0dDjEBX9w8Lc4oCKQvjk/9wx9ZqJiIh0CiVnQaiwvJr7523kxFE9mZzVw7eN566CjOEQ3sruAr3HwZQfuMn/25fAZ55tlLRCU0RExK/8mpwZY2YZY9YaYzYYY25t5vxdxpilnp91xpiiRufqGp17w59xBpv75m6gvKqWn58ysvWL22r3qkMPaTZ23C8hoSe8fiN8/QRMuBRSB/o+JhEREdmnnZszts4YEw7cC5wE5ACLjDFvWGtXNVxjrb2l0fU3ARMbNVFhrZ3gr/iC1faiCp6Yv4XzDs9kRO9E3zZeUeg2Ofc2OYtJdntcvnINhEVohaaIiEgn8FtyBkwBNlhrswGMMc8Ds4FVLVx/MfA7P8YTEu76cB0YuOWk4b5vvGFj8+ZqnLVk3Hcge67bCSBlgO9jEhERkQP4MznrB2xr9DoHOLK5C40xA4FBwCeNDscYYxYDtcAd1trXmrnvWuBagAEDQj9xWLOrhJe/zuGaowbTN6UDG5u3ZPdK9+htzxm4DcfPvs/3sYiIiEizgmVBwEXAS9baukbHBlprJwGXAP80xhy0vNBa+6C1dpK1dlJGRkZnxeo3f3tvLQnREVx/7CFWUnZE7mqIToakvv5pX0RERDrMn8nZdqB/o9eZnmPNuQh4rvEBa+12z2M2MJcD56N1OV9t2sPHa3K5/tihpMRF+edNcle54rO+LM0hIiIiPuXP5GwRMMwYM8gYE4VLwA5adWmMGQmkAgsaHUs1xkR7nqcDM2h5rlrIc5ubr6ZXUjRXTc/y15u0baWmiIiIBITfkjNrbS1wI/A+sBp40Vq70hhzuzHmrEaXXgQ8b621jY6NAhYbY5YBc3BzzrpscvbBqt18vbWIW04cTmyUjzY3b6pkO1QVQ89R/mlfREREfMKfCwKw1r4DvNPk2G+bvL6tmfvmA+P8GVuwqK2r56/vrWFIRjznH5Hpvzdqz0pNERER6XTBsiCg23r56xw25pXzi1kjfbe5eXP2rdRUz5mIiEgwU3IWQJU1ddz14XoOH5DCyb7c3Lw5uasgqR/Epvr3fURERKRDlJwF0H8Xb2NXSSU/P8XHm5s3J3eVes1ERERCgJKzAKmrtzz8+SYmDkhh6mAfb25+0JvVQt46rdQUEREJAUrOAuTDVbvZUrCXa44a7P9esz0boa5KiwFERERCgJKzAHnos2z694jllDG9/f9mWgwgIiISMpScBcCSLYUs2VLI1TMGER7WCdX6c1eDCYf0Ef5/LxEREekQJWcB8PBn2STHRvKdSf1bv9gXcldB2hCIjOmc9xMREZF2U3LWybYUlPPeyl1cNnUA8dF+rQG83+6VWgwgIiISIpScdbJHPt9EZFgYV07L6pw3rC6Hws1KzkREREKEkrNOVFhezX8X5zB7Ql96JnXSEGPeGsBCLyVnIiIioUDJWSd65sstVNTUcc3Rg9t+8xd3w9p3237fbs9+8eo5ExERCQmdNOlJKmvqeHz+Fo4ZnsHwXoltu7mqFD6+HRJ6w9CTILwNX1vuKoiIhdRBbXtPERERCQj1nHWSN5buIL+simvb02u2+Quor4WSHFj9etvuzV0FPUdCmL5qERGRUKC/sTtBfb3lwc+yGd0nielD0treQPbc/b1f8/8N1np/7+5V0FM7A4iIiIQKJWedYN66PDbklnHN0YPat1VT9lwYOB2m3QA7voZtX3p3X3k+lOdqMYCIiEgIUXLWCR76LJveSTGcMb5v228u3QV5q2HwsTDhEohJgQX/9u7e3IbFANq2SUREJFQoOfOzFduLmb+xgO/NzCIyvB1/3Nlz3ePgYyEqHiZ9D1a/BXuyW79330pNDWuKiIiECiVnfvbQZ9kkREdw0ZQB7Wsgey7EpUOvse71lGshLAK+fKD1e3NXQlwaJPRs33uLiIhIp1Ny5kc7iip4a/lOLprcn6SYyLY3YK1LzgYfs3+1ZVIfGHsefP0UVBQd+v7c1a6+WXvmuYmIiEhAKDnzo8e+2ATAd2e2s8ZY3loo3emGNBubdj3UlMPXT7Z8b339/uRMREREQoaSMz8pqazhua+2cfq4PvRLiW1fI43nmzXW5zDIOsoNbdbVNH9v8VaoLtNKTRERkRCj5MxPXvhqG2VVtVxzVDuKzjbIngs9hkBKM/PVpt3oitKuaqEorRYDiIiIhCQlZ35QU1fPo19sYurgHozLTG5fI3U1sPnzg3vNGgw7GdKGurIazRWl3VdGY2T73l9EREQCQsmZH7y9fCc7iyvbt1VTg+1LoLq05eQsLAymXg87voGtCw8+n7vK9bhFt3EfTxEREQkoJWd+8NKSHAamxXHs8A6UsMieCyYMBh3V8jWHXQyxqc0XpdW2TSIiIiFJyZmPFZZXsyC7gNPH9SEsrAMlLLLnQt+JLvlqSVQcTLoa1rx9YFHa2mooWK/FACIiIiFIyZmPfbhqN3X1llPH9ml/I1WlkLOo5SHNxqZc44rSLrx//7GC9VBfqzIaIiIiIUjJmY+9u2InmamxjO2X1P5GNn/hkitvkrPE3jDufPjmaagodMf2rdRUciYiIhJqlJz5UEllDZ9vyGfWmN6YjlTlz54LEbHQ/0jvrp/qKUq75An3OnclhEVC+rD2xyAiIiIBoeTMhz5ZnUtNneXUcb071lD2HBg4HSKivbu+z3gYdPT+orS5qyF9OIS3Y8soERERCSglZz707oqd9EqKZmL/Q0zib03JTshb492QZmPTboTSHbDyNc9KzVHtj0FEREQCRsmZj+ytrmXeujxOGdO7Y6s0N81zj0OOa9t9Q0+CtGHw2d/d1k1aqSkiIhKSlJz5yNy1eVTW1DNrbEeHNOdCXHrba5SFhbkN0fPWuNeqcSYiIhKS/JqcGWNmGWPWGmM2GGNubeb8XcaYpZ6fdcaYokbnrjTGrPf8XOnPOH3h3RW7SIuPYkpWj/Y3Yi1snAODj3HJVluNvwhiPe+vYU0REZGQFOGvho0x4cC9wElADrDIGPOGtXZVwzXW2lsaXX8TMNHzvAfwO2ASYIElnnsL/RVvR1TW1PHJ6t2cNaEvEeEdyHfz1kLZrrbPN2sQFQczboavn2h+s3QREREJev7sOZsCbLDWZltrq4HngdmHuP5i4DnP81OAD621ezwJ2YfALD/G2iGfrc+nvLqOWR0pPAtuSBNgcBvnmzU242a46WvoSCkPERERCRh/Jmf9gG2NXud4jh3EGDMQGAR80pZ7jTHXGmMWG2MW5+Xl+STo9nh3xU6SYiKYNjitYw1lz4EeQyClf/vbMEaJmYiISAgLlgUBFwEvWWvr2nKTtfZBa+0ka+2kjIwMP4V2aNW19Xy0ajcnju5FVEQH/jjramDz5+0f0hQREZEuwZ/J2XagcRdQpudYcy5i/5BmW+8NqAXZBZRU1nZsL02A7UuguqztJTRERESkS/FncrYIGGaMGWSMicIlYG80vcgYMxJIBRY0Ovw+cLIxJtUYkwqc7DkWdN5bsZP4qHCOGpbesYY2zgETBlkzfROYiIiIhCS/rda01tYaY27EJVXhwKPW2pXGmNuBxdbahkTtIuB5a61tdO8eY8z/4RI8gNuttXv8FWt71dVbPli5m+NG9iQmMrxjjWXPhb4TIbYDuwuIiIhIyPNbcgZgrX0HeKfJsd82eX1bC/c+Cjzqt+DaY8PH0OcwiHe9ZF9t2kNBeXXHhzQrSyBnEcz8ccdjFBERkZAWLAsCgl9FEfz3u/D46VC6C3CrNGMiwzh2RAcXI2yZD7auYyU0REREpEtQcuat2BS46Bko2gaPnUp94VbeW7GLY4ZnEB/dwQ7I7DkQEQv9p/gkVBEREQldSs7aYtBRcMXrUF5AzcOnEFu2peNDmuDmmw2cDhHRHW9LREREQpqSs7bqPxmuepPaqr28GHU7J2Z0cEepkp1us3KV0BARERGUnLWL7T2ea8NuJzoijIRnz4Kdy9rf2L4tm471RWgiIiIS4pSctcOK7SV8UZLO/KOecnPFnjgTcha3r7HsuRCXDj3H+DRGERERCU1Kztrh3RU7CQ8zTJs8Bb73LsT2gCdnu+2X2sJal5wNPhbC9FWIiIiIkrM2s9by7opdTB3cg9T4KEgZAN99F5L6wdPnw4aPDt1AfR3sXA4L/wPPXwJluzSkKSIiIvv4tQhtV7R2dymb8su5euag/QeT+sB334GnzobnLobvPA4jT3fn6mph1zLY/AVs+QK2LoDKYncuNQsmXQ1jz+vkTyEiIiLBSslZG7377S6MgZPH9DrwRHw6XPmm6z174XI48geQvw62fgnVpe6atKEw+my3f+bA6ZCc2enxi4iISHBTctZG763YxeSBPeiZGHPwydhUuOI113u28D7IGAnjL4CsGTBwBiT27vR4RUREJLQoOWuD7Lwy1u4u5bdnjG75ouhE14NWVQoxSZ0XnIiIiHQJWhDQBu+ucHtqzhrbSg+YMUrMREREpF2UnLXBeyt2cVj/FPqmxAY6FBEREemiNKzppcqaOhKiIzh+ZM9AhyIiIiJdmJIzL8VEhvPctVOx1gY6FBEREenCNKzZRsaYQIcgIiIiXZiSMxEREZEgouRMREREJIgoORMREREJIkrORERERIKIkjMRERGRIKLkTERERCSIKDkTERERCSJKzkRERESCiOkqFe+NMXnAlk54q3QgvxPeR9pO301w0/cTvPTdBDd9P8GrI9/NQGttRnMnukxy1lmMMYuttZMCHYccTN9NcNP3E7z03QQ3fT/By1/fjYY1RURERIKIkjMRERGRIKLkrO0eDHQA0iJ9N8FN30/w0ncT3PT9BC+/fDeacyYiIiISRNRzJiIiIhJElJyJiIiIBBElZ14yxswyxqw1xmwwxtwa6Hi6O2PMo8aYXGPMikbHehhjPjTGrPc8pgYyxu7KGNPfGDPHGLPKGLPSGHOz57i+nyBgjIkxxnxljFnm+X5+7zk+yBjzped33AvGmKhAx9pdGWPCjTHfGGPe8rzWdxMkjDGbjTHfGmOWGmMWe475/HebkjMvGGPCgXuBU4HRwMXGmNGBjarbexyY1eTYrcDH1tphwMee19L5aoGfWmtHA1OBGzz/v+j7CQ5VwPHW2sOACcAsY8xU4C/AXdbaoUAhcHXgQuz2bgZWN3qt7ya4HGetndCovpnPf7cpOfPOFGCDtTbbWlsNPA/MDnBM3Zq19lNgT5PDs4EnPM+fAM7uzJjEsdbutNZ+7XleivtLph/6foKCdco8LyM9PxY4HnjJc1zfT4AYYzKB04GHPa8N+m6Cnc9/tyk5804/YFuj1zmeYxJcellrd3qe7wJ6BTIYAWNMFjAR+BJ9P0HDM2y2FMgFPgQ2AkXW2lrPJfodFzj/BH4B1Htep6HvJphY4ANjzBJjzLWeYz7/3RbR0QZEgpG11hpjVCcmgIwxCcDLwI+ttSWuA8DR9xNY1to6YIIxJgV4FRgZ2IgEwBhzBpBrrV1ijDk2wOFI82Zaa7cbY3oCHxpj1jQ+6avfbeo58852oH+j15meYxJcdhtj+gB4HnMDHE+3ZYyJxCVmz1hrX/Ec1vcTZKy1RcAcYBqQYoxp+Ae7fscFxgzgLGPMZtz0meOBu9F3EzSstds9j7m4f9hMwQ+/25SceWcRMMyzYiYKuAh4I8AxycHeAK70PL8SeD2AsXRbnjkyjwCrrbX/aHRK308QMMZkeHrMMMbEAifh5gXOAc73XKbvJwCstb+01mZaa7Nwf898Yq29FH03QcEYE2+MSWx4DpwMrMAPv9u0Q4CXjDGn4eYChAOPWmv/GNiIujdjzHPAsUA6sBv4HfAa8CIwANgCXGCtbbpoQPzMGDMT+Az4lv3zZn6Fm3em7yfAjDHjcZOWw3H/QH/RWnu7MWYwrremB/ANcJm1tipwkXZvnmHNn1lrz9B3Exw838OrnpcRwLPW2j8aY9Lw8e82JWciIiIiQUTDmiIiIiJBRMmZiIiISBBRciYiIiISRJSciYiIiAQRJWciIiIiQUTJmYh0C8aYOmPM0kY/Ptt43RiTZYxZ4av2RKR70/ZNItJdVFhrJwQ6CBGR1qjnTES6NWPMZmPMX40x3xpjvjLGDPUczzLGfGKMWW6M+dgYM8BzvJcx5lVjzDLPz3RPU+HGmIeMMSuNMR94qu+LiLSZkjMR6S5imwxrXtjoXLG1dhzwb9xOIAD/Ap6w1o4HngHu8Ry/B5hnrT0MOBxY6Tk+DLjXWjsGKALO8+unEZEuSzsEiEi3YIwps9YmNHN8M3C8tTbbs2H7LmttmjEmH+hjra3xHN9prU03xuQBmY23zzHGZAEfWmuHeV7/DxBprf1DJ3w0Eeli1HMmIgK2hedt0Xivwzo0p1dE2knJmYgIXNjocYHn+XzgIs/zS3GbuQN8DPwQwBgTboxJ7qwgRaR70L/sRKS7iDXGLG30+j1rbUM5jVRjzHJc79fFnmM3AY8ZY34O5AHf9Ry/GXjQGHM1rofsh8BOfwcvIt2H5pyJSLfmmXM2yVqbH+hYRERAw5oiIiIiQUU9ZyIiIiJBRD1nIiIiIkFEyZmIiIhIEFFyJiLdjmdrJmuMaXXFujHmKmPM550Rl4gIKDkTkSDn2fuy2hiT3uT4N54EKytAobUpyRMR8ZaSMxEJBZvYX38MY8w4IC5w4YiI+I+SMxEJBU8BVzR6fSXwZOMLjDHJxpgnjTF5xpgtxpjfGGPCPOfCjTF/N8bkG2OygdObufcRY8xOY8x2Y8wfjDHhHQnYGNPXGPOGMWaPMWaDMeaaRuemGGMWG2NKjDG7jTH/8ByPMcY8bYwpMMYUGWMWGWN6dSQOEQk9Ss5EJBQsBJKMMaM8SdNFwNNNrvkXkAwMBo7BJXMNVf2vAc4AJgKTgPOb3Ps4UAsM9VxzMvD9Dsb8PJAD9PW835+MMcd7zt0N3G2tTQKGAC96jl/p+Qz9gTTgOqCig3GISIhRciYioaKh9+wkYDWwveFEo4Ttl9baUmvtZuBO4HLPJRcA/7TWbrPW7gH+3OjeXsBpwI+tteXW2lzgLvbvq9lmxpj+wAzgf6y1ldbapcDD7O/9qwGGGmPSrbVl1tqFjY6nAUOttXXW2iXW2pL2xiEioUnJmYiEiqeAS4CraDKkCaQDkcCWRse2AP08z/sC25qcazDQc+9Oz1BiEfAA0LMDsfYF9lhrS1uI52pgOLDGM3R5huf4U8D7wPPGmB3GmL8aYyI7EIeIhCAlZyISEqy1W3ALA04DXmlyOh/X6zSw0bEB7O9d24kbKmx8rsE23Ibn6dbaFM9PkrV2TAfC3QH0MMYkNhePtXa9tfZiXAL4F+AlY0y8tbbGWvt7a+1oYDpuKPYKRKRbUXImIqHkauB4a21544PW2jrcvK0/GmMSjTEDgZ+wf17ai8CPjDGZxphU4NZG9+4EPgDuNMYkGWPCjDFDjDHHtCGuaM9k/hhjTAwuCZsP/NlzbLwn9qcBjDGXGWMyrLX1QJGnjXpjzHHGmHGeYdoSXMJZ34Y4RKQLUHImIiHDWrvRWru4hdM3AeVANvA58CzwqOfcQ7jhwmXA1xzc83YFEAWsAgqBl4A+bQitDDdxv+HneFzpjyxcL9qrwO+stR95rp8FrDTGlOEWB1xkra0AenveuwQ3r24ebqhTRLoRbXwuIiIiEkTUcyYiIiISRJSciYiIiAQRJWciIiIiQUTJmYiIiEgQiQh0AL6Snp5us7KyAh2GiIiISKuWLFmSb63NaO5cl0nOsrKyWLy4pRX2IiIiIsHDGLOlpXMa1hQREREJIkrORERERIKIkjMRERGRINJl5pw1p6amhpycHCorKwMdit/FxMSQmZlJZGRkoEMRERGRDujSyVlOTg6JiYlkZWVhjAl0OH5jraWgoICcnBwGDRoU6HBERESkA7r0sGZlZSVpaWldOjEDMMaQlpbWLXoIRUREurounZwBXT4xa9BdPqeIiEhX1+WTMxEREZFQouTMH/bugYJsqCyhqLCQ++67r81NnHbaaRQVFfk+NhEREQlqSs58rb4OinOgqhj2bKRo49fc9+9/ga0/4LLa2tpDNvPOO++QkpLix0BFREQkGHXp1ZqN/f7NlazaUeLTNkf3TeJ3Z4458GB5Ltg6SBsGdVXcev2v2bhpExPGjSYyOoaYuARSU3uwZs0a1q1bx9lnn822bduorKzk5ptv5tprrwX2b0dVVlbGqaeeysyZM5k/fz79+vXj9ddfJzY21qefRURERIKDes58qa4WyvIgJhmiEyAujTvuupchgwezdO6b/O2XN/D1kiXc/X+/ZN3qFQA8+uijLFmyhMWLF3PPPfdQUFBwULPr16/nhhtuYOXKlaSkpPDyyy939icTERGRTtJtes4O6uHyh/Ldrtcssc/+Y8aACYP0YZC8gSlHTGBQRgzsXgWxqdxz14O8+sZbAGzbto3169eTlpZ2QLODBg1iwoQJABxxxBFs3rzZ/59FREREAqLbJGd+V1cDZfkQmwqRLQw5RsYSn5wGPUdDeR5zP3qPj95/hwXzPiYuJZ1jjz222Vpl0dHR+56Hh4dTUVHhr08hIiIiAaZhTV8p2w3UQ2LvAw4nJiZSWlp64LUR0ZCcSXFYCqnJicSF17BmzRoWLlzYefGKiIhIUFLPmS/UVkN5PsSlQUTMAafS0tKYMWMGY8eOJTY2ll69eu07N+u0M7n/3/cw6oiZjBg9jqlTp3Z25CIiIhJkjLU20DH4xKRJk+zixYsPOLZ69WpGjRrls/eotxZrITysSTX+oq2utlnP0RAR1bZGi3NcYtdnvJub1gG+/rwiIiLiH8aYJdbaSc2d07Cml+rrLWt2lpJX2mROWG0l7C2A+PS2J2YAUQmAheq9PolTREREQpuSMy+FhRlio8Ip2lvDAb2NpbuAMEjo1eK9hxSV4B6ryzoco4iIiIQ+JWdtkBIbSXVdPXur69yBmgqoKISEdAiPbF+j4REQEQtVPkrO8jfA0ud805aIiIh0OiVnbZAUG0mYMRTtrXEHSneCCYf4dvaaNYhOgOryg7Z4ape5f4LXfqhhUhERkRAVkOTMGDPLGLPWGLPBGHNrM+cHGmM+NsYsN8bMNcZkBiLOpsLDDEkxERRX1GCry6GyGBIyXO9XR0QlAPUdT6ishQ0fARb2bOxYWyIiIhIQnZ6cGWPCgXuBU4HRwMXGmNFNLvs78KS1djxwO/Dnzo2yZSlxUdTW11NXtMPTa9az4436at5ZXbVLGAHy13WsLREREQmIQPScTQE2WGuzrbXVwPPA7CbXjAY+8Tyf08z5gEmIiSAxrIqI2jJI7AVh4R1vNDzC1UerLiMhIaH97dRUQFgEYCB/fcfjEhERkU4XiOSsH7Ct0escz7HGlgHnep6fAyQaY9KaXIMx5lpjzGJjzOK8vDy/BNtUGNA3rJAaG05dbLrvGo7yzDvriNpKGDANUgYoORMREQlRwbpDwM+AfxtjrgI+BbYDdU0vstY+CDwIrgjtIVt891bY9W3HI6uvJbq2gioiqet7OOFn/u2Ql996663079+fG264AYDbbruNiIgI5syZQ2FhITU1NfzhD39g9slHw9789sdVW+2GNYefAtnzNKwpIiISogLRc7Yd6N/odabn2D7W2h3W2nOttROBX3uOFXVahC2yUFeNNWHUEUFVbeurKy+88EJefPHFfa9ffPFFrrzySl599VW+/vpr5syZw09/+lNsZPz+92iPKs9cs2EnQ/owKNgA9T5Y/SkiIiKdKhA9Z4uAYcaYQbik7CLgksYXGGPSgT3W2nrgl8CjHX7XU+/ocBNUFEHhJkzKAIpr4sgvrWZUXT0R4S3nuBMnTiQ3N5cdO3aQl5dHamoqvXv35pZbbuHTTz8lLCyM7du3szt/D70jotsfW2WJm2+WPtwlZzV7oXQHJAfFQlcRERHxUqcnZ9baWmPMjcD7QDjwqLV2pTHmdmCxtfYN4Fjgz8YYixvWvKGz4zyIta6uWXg0xPYgJaKevNIqiipqSE84dFL1ne98h5deeoldu3Zx4YUX8swzz5CXl8eSJUuIjIwkKyuLyspKSEl072MtGHPINg9QX++K2EbGuvvSh7vj+euUnImIiISYgMw5s9a+A7zT5NhvGz1/CXips+M6JFvvisVGJYBxWznFRLrtnFpLzi688EKuueYa8vPzmTdvHi+++CI9e/YkMjKSOXPmsGXLFndhlGdos2bv/ufeqC4D6t1OAwBpw9xj/noYcnzbPqeIiIgEVLAuCAg+YeGQ3P+AQylxkewqrqS6to6oiJZLaowZM4bS0lL69etHnz59uPTSSznzzDMZN24ckyZNYuTIke7C6ET3WF3WtuSsshhMGER4tpBK6AnRyVqxKSIiEoKUnHVASqxLzor21tAz6dD1zr79dv9K0fT0dBYsWNDsdWXZi90QpbcbqVsLVSWeHr1qd8wYN+9MKzZFRERCjvbW7ICoiHDioyIo3FuDte1cZdnUvn02vWyvttKV0IhJPvB4+jD1nImIiIQgJWcdlBIXSVVtHZU1B5Vha5+oBLB1rtq/N6pK3GN00oHH04e51ZpVpb6JS0RERDpFl0/OfNaj1YLk2EgMhqKKGt802NZ9NitLICIWGx554PGGFZsFG3wTl4iIiHSKLp2cxcTEUFBQ4NcELSI8jMSYCIp8NbQZEQXhUd4lZ/W1UF2GjU6koKCAmJiY/ecar9gUERGRkNGlFwRkZmaSk5ODv/fd3Ftdx57yaqryooiO9MFG6HuLoGYHJFUeut5Z9V635VOCISYhmczMRjXNegwCE67kTEREJMR06eQsMjKSQYMG+f199lbXMukPH3HWYX2547yxHW9w6XPw5nVw3RfQ+xDtvfpDWPsO/HwjhDf5KiOiITVLKzZFRERCTJce1uwscVERnDKmN+98u5OqWh8sDMia4R63fNHyNfX1sOFDGHriwYlZA63YFBERCTlKznxk9oS+lFTWMnetD4ZQUwZA8gDY/HnL1+z4BsrzYPgpLV+TPgz2bIR6H60kFREREb9TcuYjM4emkxYfxetLt/umwayZruespUUG6993uwIMPbHlNtKHuzpoxdt8E5OIiIj4nZIzH4kID+OM8X34aHUuJZU+KKuRNQP2FkDemubPr3sfMidDXI+W29CKTRERkZCj5MyHZk/sR3VtPe+v2NXxxgZ65p01N7RZugt2LoVhJx+6jYZaZ0rOREREQoaSMx+a2D+FAT3ieH3pjo43lpoFSf2aXxSw/kP3eKj5ZgDxaRDbQys2RUREQoiSMx8yxjB7Ql/mb8wnt6Syo425eWebPz943tn6913i1suLsh3pw9VzJiIiEkKUnPnY7Al9qbfw5vKdHW9s4Ay3IrNxclVbDRvnwrCTDl2gtkH6UChQciYiIhIqlJz52NCeiYzpm8Rr3/hg1WbWTPe4pdG8s60LoLoUhrUypNkgfTiU7YaKoo7HIyIiIn6n5MwPzj8ik2+3F7M8p6hjDfUYDAm9YXOjeWfrP4DwaBh8jHdtaAN0ERGRkKLkzA/OOyKTuKhwnpi/pWMNNTfvbN377lhUvHdtqJyGiIhISFFy5gdJMZGce3g/3ly+g4Kyqo41ljUDynbBnmz3U7C+9RIajaUOhLBIrdgUEREJEUrO/OSKaVlU19bzwuIOVucf6Jl3tvlzWPeBez68DclZeKQbHlVyJiIiEhICkpwZY2YZY9YaYzYYY25t5vwAY8wcY8w3xpjlxpjTAhFnRwzvlcj0IWk8s3ArtXX17W8ofRjE93TJ2fr33TBlj8Ftb0PDmiIiIiGh05MzY0w4cC9wKjAauNgYM7rJZb8BXrTWTgQuAu7r3Ch944ppWWwvquCj1bntb8QYN7S5aZ5L0ForPNuc9GFuSLSutv1xiIiISKcIRM/ZFGCDtTbbWlsNPA/MbnKNBZI8z5MBH5Tc73wnjupJ3+QYnlywuWMNDZzhymHUVbdtvlmD9OFQXwNFHVygICIiIn4XiOSsH9B4IlaO51hjtwGXGWNygHeAm5pryBhzrTFmsTFmcV5enj9i7ZCI8DAunTqQ+RsLWL+7tP0NNdQ7i0qEAdPafv++FZuadyYiIhLsgnVBwMXA49baTOA04CljzEGxWmsftNZOstZOysjI6PQgvXHR5P5ERYTx5IIO9FpljHT1zoadCBFRbb8/fah71LwzERGRoBeI5Gw70L/R60zPscauBl4EsNYuAGKA9E6JzsfSEqI5c3xfXv46h5LKmvY1Ygx89x04/R/tuz821S0qUM+ZiIhI0AtEcrYIGGaMGWSMicJN+H+jyTVbgRMAjDGjcMlZ8I1beunK6QPZW13Hy0ty2t9I2hCI69H++7ViU0REJCR0enJmra0FbgTeB1bjVmWuNMbcbow5y3PZT4FrjDHLgOeAq6xtKJEfesZnpjChfwpPLdhCfX2APkb6MG2ALiIiEgIiAvGm1tp3cBP9Gx/7baPnq4AZnR2XP101PYsfv7CUzzfkc/TwAMyPSx8OewugvADi0zr//UVERMQrwbogoMs5dVxv0hOieGL+5sAE0LBiU71nIiIiQU3JWSeJjgjn4ikD+GRtLlsL9nZ+AOnaAF1ERCQUKDnrRJccOYAwY3j6ywAUg00ZAOHRWrEpIiIS5JScdaI+ybHMGtObFxZto6K6rnPfPCzcrfhUz5kEwsY5kL8h0FGIiIQEJWed7IppAymuqOH1pU1Lu3UCrdiUQHn5apj3l0BHISISEpScdbIpg3owsnciTyzYQqdXB0kfDns2QW112+4r2AglIbm9qQSDqjK3Urh4W+vXioiIkrPOZozhyulZrN5ZwuIthZ375mnDwNZB4Sbv76kqg0dOhucugtAtNSeB1JCUFQegt1hEJAQpOQuA2RP6khQT0fllNdqzYnPRQ7A3H3Yug/Uf+Ccu6dqKPMlZ6Q6o7+S5liIiIUjJWQDERUVwwaT+vLdiF7tLKjvvjfclZ16u2KwqhS/ugcHHutWe8/6q3jNpu+Kt7rG+FspyAxuLiEgIUHIWIJdPG0idtTzz5dbOe9PoREjs433P2VcPQcUeOP5/YeYtsH0xZM/1a4jSBRU1mmtWoqFNEZHWKDkLkIFp8Rw3oifPfrmV6tr6zntjb1dsVpXC/Htg6EmQOQkmXAqJfeHTv/k/RulaireB8fyqKc4JbCwiIiFAyVkAXTk9i/yyKp5csLnz3jR9uBvWbG148ssHoKIQjv2lex0RDTNuhi1fwOYv/B+ndB1FW6HXGPdcPWciIq1SchZARw9L5/iRPfn7B2vZUlDeOW+aNgwqi6E8r+VrKktg/r9g2CmQecT+40dcCfE94dO/+j9O6TqKtkGfwyAiVis2RUS8oOQsgIwx/PGcsUSGhXHry992Tt0zb1ZsfvkAVBbBsbceeDwyFqbf5OadbVvkrwilK6mtgrJdkDwAkvtBiYY1RURao+QswPokx/LL00axILuA5xd1QpHO9OHusaUVm5XFsOBfMPxU6Hf4wecnfQ9ie2jumXinYY5ZSn9I6qeeMxERLyg5CwIXT+nPtMFp/Ont1ewsrvDvmyX1c8NLLfWcLbzfJWjH/k/z56MTYNr1sP592LHUb2FKF9FQgDa5PyRnas6ZiIgXlJwFAWMMd5w3jpr6en7z6gr/Dm+GhUH60OZXbFYUwcJ7YcRp0Hdiy21MuRZikv3fe7brW/jPTHjtev++j/hPQxmNhp6z0l1QVxPYmEREgpySsyAxMC2en508go/X5PLGMj/vY9mwYrOpLxt6zW49+FxjMclw5HWw5i3Yvcr38dXXw4J74aHjYfe3sOIVqOnEYr3iO0VbXRmNpH5uzhkWSncGOioRkaCm5CyIfHfGICb0T+G2N1aSX1blvzdKGwaFWw5MeCqKYMF9MPIMt7KuNUdeB1EJ8NnffRtb6S545jx4/1cw9EQ4+36orXAlPCT0FG9z9fHCIyEp03NMQ5siIoei5CyIhIcZ/nr+eMqqavn9m37okWqQPgywsCd7/7GF90GVF71mDeJ6wOTvu16ttuzVeShr34P/TIctC+D0f8BFz8Lo2RAeDRs+8s17SOcq2uaGNMHTc4bmnYmItELJWZAZ3iuRm44fxpvLdvDhqt3+eZOmKzYrCmHhf2DUmdB7nPftTLsRImLgszs7Fk9NBbz9U3juQtfL8oN5MPlqMAai4iBrJqz/sGPvIYFRvNUtBgA3tAlKzkREWhGQ5MwYM8sYs9YYs8EYc1BXjTHmLmPMUs/POmNMUQDCDJjrjhnCyN6J/PrVbymu8MPk6bQh7rGhx2vBvVBVAsd42WvWICHDldZY/iLs2dS+WHatgAePhUUPw9Qb4JqPIWPEgdcMPdEtYCjc3L73kMCor4OSHft7zmKSIDpJw5oiIq3o9OTMGBMO3AucCowGLjbGjG58jbX2FmvtBGvtBOBfwCudHWcgRUWE8dfzx5NfVsWf3l7thzeId70ZBeth7x5XPmPUWdB7bNvbmn4ThEXA53e17b76ejfH7aHjXM/dZa/ArD+5baKaGnaSe9TQZmgp3Qn1tft7zsD1nqnnTETkkALRczYF2GCtzbbWVgPPA7MPcf3FwHOdElkQGZ+ZwjVHD+aFxdv4fH2+798gfZgb1lzwb6gu9X6uWVNJfeDwy2Hps/vLJhxKbbWbW/b0OfD+L2HI8fDD+TD0hJbvSRsKKQNhvZKzkNK4jEaD5H7a/FxEpBWBSM76AY3/Fs/xHDuIMWYgMAj4pIXz1xpjFhtjFuflHWKvyBB1y4nDGZQez62vLKe8qta3jacNg7y1bqum0Wfv35i6PWb82D1+cXfz5+tqYeMn8PoN8Pehbm7ZjqVw2t/h4uchPv3Q7Rvjes82feq2A5LQULTVPSYP2H9MPWciIq0K9gUBFwEvWWvrmjtprX3QWjvJWjspIyOjk0Pzv5jIcP5y3nhyCiv42/trfdt4+jCo2QvV5e3vNWuQ0h8mXAxfP+lKYYAbttz8Bbz1E7hzBDx1Dqx83W0Ldcl/4WfrYco1LvHyxtAToaYcti7oWKzSeYo9ydkBPWeZUJ6nJFtE5BAiAvCe24FGv63J9BxrzkXADX6PKIhNGdSDK6YN5IkFmzljfB8mZfXwTcMNKzbHnAM9R3W8vZm3wDfPwAe/gfgMWPmqm3MUEQsjToWx58LQkyAypn3tDzoawqPcqs3Bx3Y8XvG/om3uv4XI2P3HGq/Y7DE4MHGJiAS5QPScLQKGGWMGGWOicAnYG00vMsaMBFKBbt9V8otZI+mXEst1Ty9hY16ZbxrNnAwTLoMTf+eb9noMhnHfgW//61Ze9jsCznsEfr4BvvOYK9PR3sQM3CKGgdO1KCCUFG87cDEA7K91phWbIiIt6vTkzFpbC9wIvA+sBl601q40xtxujDmr0aUXAc9bv240GRoSoiN4/LtTsBYufehLtu3Z2/FGo+Lg7HshNavjbTWY9We44CmXkF30DIw7322U7itDT4K8Nd4tPJDAa1yAtkHDLgGadyYi0qKAzDmz1r5jrR1urR1irf2j59hvrbVvNLrmNmttBydDdR1Deybw9PePpKKmjosfWsjO4opAh3SwuB4w+iy396Y/DD3RPar3LPhZ61ZlNu05S+rrHrViU0SkRcG+IEAaGdUniSe/N4WivTVc+tCX5JV2s0nVGSPcX/ZKzoJfeb7bEzVlwIHHo+Igtod6zkREDkHJWYg5rH8Kj313MjuLK7n8kS8pLK8OdEidxxjXe5Y919VLk+C1r4xG/4PPJffTnDMRkUNQchaCJmf14KErJpGdX84Vj35FSaUftngKVsNOguoy2LYw0JHIoewrozHg4HNJmeo5ExE5BCVnIWrmsHTuv+xwVu8s4XuPLWJvtY+L1AarQUdDWKSGNoNdc7sDNNAuASIih6TkLIQdP7IX91w8ka+3FvL9JxZTWdNsrd6uJToRBkzVVk7BrngbRCc3vzgkqR9UFrkCyCIichAlZyHutHF9+Pt3DmNBdgE/fHoJ1bX1gQ7J/4adBLkrNW8pmDVXRqNBsqechr4/EZFmKTnrAs49PJM/nD2WOWvzuPn5b6it6+IJ2tCT3KOGNoNXcwVoG+zbJUBDmyIizVFy1kVceuRA/veM0by7Yhc/f2k59fVduHZvz1GQ2Bc2fBjoSKQlh+w50y4BIiKHEoi9NcVPrp45iIrqWv7+wTrio8P5v9ljMd5uLB5KjIFhJ8LK16CuBsIjAx2RNFZRBFXFLfecJfYFjFZsioi0QD1nXcwNxw3lumOG8PTCrdzx3hq67O5XQ0+CqhLY9lWgI5GmihtWajZTRgMgIgoSemrFpohIC5ScdTHGGP5n1ggumzqAB+Zlc9/cjYEOyT8GHwNhERraDEaHKqPRIKlf8PWc5W+ABfe6radERAJIyVkXZIzh9rPGcu7Efvzt/bU8/sWmQIfkezHJ0P9IldQIRg09Z8kt9JxBcO4SMOeP8P6voKCL/oNGREKGkrMuKizM8Nfzx3PKmF7c9uYq/rt4W6BD8r2hJ8Lub6FkZ6AjkcaKtkJELMSnt3xNwy4BwdJLVVEEa952z7UKWEQCTMlZFxYRHsY9F0/kqGHp/M/Ly3n32y6WxAzzlNTY+HFg45ADFW9ztcwOtRgluZ/bhquyuPPiOpRVr0FdFUQl6r8nEQk4JWddXHREOA9cfgQTB6Tyo+e/Ye7a3ECH5Du9xkJCb1iveWdB5VBlNBrsq3UWJEOby56H9BEw4WLY/DnUVAY6IhHpxpScdQNxURE8etVkhvdK5Lqnl/BldkGgQ/INY9zQZvYcqOsme4uGgqKtLZfRaBBMuwTsyYatC+Cwi9x/TzV73WsRkQBRctZNJMdG8uT3ptAvJZarn1jM8pyiQIfkG8NOdENj2xcHOhIBqN4Le/NbLqPRIJh2CVj2AmBg/AWQNRPCozS0KSIBpeSsG0lLiOaZ708lNT6SKx79irW7SgMdUscNPg5MuPdDm9bCzmVQUejfuLqrhtplrSVnib3d9xbonjNrYdlzMOho15sXFQ8DpsKGTwIbl4h0a0rOupneyTE8c/VUosLDuOyRL9mQWxbokDomNgUyJ7de76y2GpY+B/fPhAeOhr8Phxcug9VvQm1Vp4TaLRRvdY+tDWuGhUNin8DPOdu6EIq2wIRL9h8bcgLkrtQqYBEJGCVn3dCAtDie+f6RWGs5//75LN68J9AhdcywE11vWOnug89VFMHnd8Hd4+G166C+Dk6/EyZ/3/3F/MJlcOcIeOsW2Ppl8JR2CFXeFKBtkNwv8LsELHsWIuNh5Bn7jw090T1qaFNEAiQgyZkxZpYxZq0xZoMx5tYWrrnAGLPKGLPSGPNsZ8fY1Q3rlcgrP5xBalwUlz78Je+tCOFegqENJTUaDUUVboF3b4W7xsBHt0HGCLjsZbh+gUvMZv0ZfrIGLn3J9ZQsfQ4ePRnumQhz/qxCpO1VvM3t3JDYp/VrA71LQE2F25919FkQnbD/eK8xbhXwBiVnIhIYnb7xuTEmHLgXOAnIARYZY96w1q5qdM0w4JfADGttoTGmZ2fH2R0MSIvj5R9O5/tPLOKHz3zN784YzVUzBgU6rLbrPR7ie7qhzYzhMP9fsOp1MGEw9nyYdgP0GX/wfeERrlbasJOgssQNcS5/Hub9BebdAZlTYOYtMPK0zv9MoapoGyT1dcOWrUnuB2vfcb2Vh6qJ5i9r33H7sx528YHHjYEhx8O6d11PqzefRUTEhwLRczYF2GCtzbbWVgPPA7ObXHMNcK+1thDAWtuFinMFlx7xUTx7zVROGuV2EvjzO6uprw+xob2wMBh6Aqx8FR463vV4TLsRbl4O5z7QfGLWVEwSTLwUrnwTblkBJ94GFXvgxcth8xd+/whdRtHWQ2/b1FhSJtRWwt4AlXZZ9ryLIeuog88NPcEtGtmxtNPDEhHpcHJmjIk3xoR5ng83xpxljIk8xC39gMZ7CeV4jjU2HBhujPnCGLPQGDOrhfe+1hiz2BizOC8vryMfo1uLiQznP5cdwRXTBvLAp9nc/MJSqmrrAh1W2xx+BfQeB6f8CW5ZCSf/n+uZaY/kTNdjds0nkDoIXrwi8HOjQkWxFwVoGzR8P4H4sy3d7ZL48Re45L6pwccBRls5iUhA+KLn7FMgxhjTD/gAuBx4vINtRgDDgGOBi4GHjDEpTS+y1j5orZ1krZ2UkZHRwbfs3sLDDL8/awy3njqSN5ft4MpHv6K4oibQYXlv4HT4waduCDMmyTdtxiTDRc+61ZzPX+rmKEnL6mqgdGfrZTQaBHKXgG//C7bu4CHNBvFp0HeiFgWEovUfwts/1WpbCWm+SM6MtXYvcC5wn7X2O8CYQ1y/HWj8T+tMz7HGcoA3rLU11tpNwDpcsiZ+ZIzhumOGcPdFE1iypZDv3D+fHUXdPCHJGA7nPgg7l7oVnVrN2bKS7WDrWy+j0SCQuwQsex76HeG+35YMPQFyFqkmXqj5/J+w6GH492RY+B/tHiIhySfJmTFmGnAp8Lbn2KFm0C4ChhljBhljooCLgDeaXPMartcMY0w6bpgz2wexihdmT+jHE9+dws6iSs69bz6rd5YEOqTAGnkaHPtLV6z0ywcCHU3waksZDYC4dFeNv7N3Cdj1Lez+tuVeswZDTnDJZva8zolLOq6mAnK+cguBBhwJ790KDx0L2xYFOjKRNvFFcvZj3MrKV621K40xg4E5LV1sra0FbgTeB1YDL3ruu90Yc5bnsveBAmPMKk9bP7fWdpENIUPD9KHp/PeH0wC44P4FfLqum8/pO/oXMOJ0eP9XsOnTQEcTnIo9yZm3PWdhYW5lZ2f3nC17HsIiYex5h74uczJEJ2toM5Rs+xLqqt0+qZe+BBc8CeUF8MhJ8ObNsDfEazpKt9Hh5MxaO89ae5a19i+ehQH51toftXLPO9ba4dbaIdbaP3qO/dZa+4bnubXW/sRaO9paO85a+3xH45S2G9k7iVeun07flFiufOwr/vzuaqpr6wMdVmCEhcE590PaEPjvVW5VohyooeesYbjSG0mZnTvnrK4Wlr8Iw0+BuB6HvjY8AgYf7bZy0nB2aNj0qauzN2CqK4kyejbc+JWbi/r1U/DvSfDNM/o+Jej5YrXms8aYJGNMPLACWGWM+XnHQ5Ng0DcllldvmM5FkwfwwLxszvvPfLLzQnzLp/aKSXILBOpq3M4CWiBwoKKtrnhrRLT39yT369yes+w5UJ7b+pBmgyEnuGHX/HX+jUt8Y9On0PdwiE7cfyw6EU75o1swlDYUXr8eHjsVdq9quR2RAPPFsOZoa20JcDbwLjAIt2JTuoi4qAj+fO447r/sCLYV7uWMf33Oi4u2Ybvjvz7Th8G5D8HO5W6YpDv+GbSkeKv3880aJPWD0h2u2GtnWPosxPaAYSd7d/3QE9yjSmoEv6pS2P6128S+Ob3Hwnffg7P+DXlr4YGj4MPfdt5/eyJt4IvkLNJT1+xsPCssAf2N1QXNGtubd28+ivGZyfzi5eXc+Ow3FO8NoXIbvjJiFhz3a1j+Aiy8L9DRBI+ibd6X0WiQ3A/qa6GsE+pMVxTBmrdh3PkQEeXdPSkDIH24tnIKBVsWuPIoLSVn4KYnHH453LQExl0AX9wN6z/ovBhFvOSL5OwBYDMQD3xqjBkIdPPlfV1Xn+RYnvn+VH4xawTvr9zFqXd/ylebuuEk26N+6jbL/uB/tZoPoL7ezR3zdjFAgyTP/LTOmHe26nWoq3KTxdtiyAmw5QsNYwe7TfMgPBr6T2n92rgecMZdbmHI1oX+j02kjXyxIOAea20/a+1pnon8W4DjfBCbBKnwMMP1xw7l5R9OJyoijIseXMCdH6ylpq4bLRZoWCCQPswtECjcEuiIAqtst1sl19Zhzc7cJWDZ864XrO/hbbtv6Alum6kt2sYrqG3+zCVmkbHeXR8ZA30nwLav/BqWSHv4YkFAsjHmHw3bKBlj7sT1okkXd1j/FN760VGce3gm//pkAxc8sICtBXsDHVbniU50CwTq6+D5S9w8tO5qXxmNNg5rdtYuAXs2wdb5rtesrZusD5zhemQ2fOKf2KTj9u5x//8dakizOf2PhB1fQ221f+ISaSdfDGs+CpQCF3h+SoDHfNCuhICE6Aj+/p3D+NfFE9mQW8apd3/KI59vora79KKlDYHzH4XCzW6C8ZOz3eTx7rZQoKG0SFt7zmJTITLO/ys2l78AGBh/YdvvjYpz24Op3lnw2vIFYNuenGVOdr2iu771S1gi7eWL5GyItfZ31tpsz8/vgcE+aFdCyJmH9eXdm49iUlYP/u+tVZx93xcszykKdFidY9iJcMsKOPE2yF0DT58H/5kBS5/rPv8ib0jO2jrnzBjXe+bPXQKsdbs7DDq6bTXYGht6AuStCcwm7dK6TZ+6JL+tQ9b9j3SPORralODii+Sswhgzs+GFMWYGoJmz3VBmahyPf3cy915yOLklVZx97xfc9sZKSiu7wYrO2FSYeQv8eDnMvs9t+/PadXD3YW5FWGVxoCP0r+Jt7s8gOqHt9/q71tm2L13Ppre1zZozpKGkhnrPgtKmz2DANO9X4TZI6uOG4rd96Z+4RNopwgdtXAc8aYxJ9rwuBK70QbsSgowxnD6+D0cNT///9u47Pqoye/z458wkmfReSKMk9N4EbJQACnZXXXt3XV1ddXXd1S3urqv7Vfdn3a/f3VXXXlgbNrCDYpcSehEIJYFASIBU0p/fH88MBAwwSSaZSXLer72vO3Pnzp0ne3Fy8pRzePDDdTz3zWbmrijkT6cP4ZRhPZCWzvfpbIJcMOpiGHmR/UX+9aM2l9Lnf4cxl8OE61vfexPIWpNGwyM6o3VDhsZAbaV7q2h+X1MB6+ZAcAQMOr117QNIHgRRabadY9rh623vVlu54IRb7WIT5b2KIti1puWrcD0yx8HWb3zbJqXaqM3BmTFmGTBCRKLdz8tE5BagG8+OVtGhwfzlzKH8ZHQGv5u9ghteXsLkAUn89cyhZMaH+7t57U/EDnf2mwbbl8I3/wvf/hO++xfMuA/G/czfLfSt0nybfb01YtKhfIetvOAM9u49u/Pg35OgxsusPeOva12vnocI9M2BNe/aElBOX/xd28TnD0DuC7b3p/fxvr12V+epddvnxNa9P3M8rHzdDll3xT+cVKfks28Yd5UAj1uBR3x1bdV5jciM5e0bjue5b7bw0EfrmP7w59w0tR8/OzGLYGc36SFIGwnnPAVT/wTv/Qre/41dSJCd4++W+YYxtuestT9PdDpgoLzQ+963Rc/YnrGpf7JltUIi3VuE3buaPA6JaFlJqcPJngq5L9rVfd7k0vJWTQWsmm0fr35Lg7OW2vyFLVDfY0Tr3p95jN3nf6fBmQoY7fXbsYuPXamWCHI6uPqEPnxy2yQm90/mgQ/WcepjX/D1xmJ/N61jxWbCec9C0kB47Uoo2ejvFvnGvj1QV9nyxQAe+3OdeTnvrL4Glr4EA0+BE2+FY66xQ1qDToPsKfaXbfIgG+iFx/smMAPImgzi8H0pp9Vv2WHY+CxY/Y5N6Ku8t2mBDWhb25uZMtQuJtB8ZyqAtFdw1s3yCChvpMaE8a9Lx/Cfy8dSWdPARU9+xzXPLWJjdyqk7oq0udFE4JULoboLFNNobRoNj5ZWCVj7HlSVwJgrWvd5rRUeb1cD+npRQO6LkNDPlgSr2AH5mrHea6UFdoi7dyuHNMEOpaeP0eBMBZRWB2ciUi4iZc1s5UCaD9uoupipg1L49LZJ/HbGQL7NK+Hkhxfw53dWsbuym6SdiO8D5z0HJRvgzWs7f09Ja9NoeLS0SsDiZ+0Kuyw/DAv3nWaHNat8VLKseL2djD7qEug/A4JCDwxxqqPb9IXdtzS/2aEyx8GO5VDbjZJoq4DW6uDMGBNljIluZosyxvh4tqzqakKDnVw/OZvPbp/MBeMyef6bzUz6+3yeXJBHTX2Dv5vX/rIm2YUBP7wP8+/xd2vaxlMdoLWrNV1Rds6QNz1nJRvtMNaYy/yzqrHvVJsmJW++b66X+yKI06b5cEXa4E+HNr23aQGEJ0Dy4LZdJ3M8NNbD9lzftEupNuomM7JVoEqMdHHPWcP44JaJjOkVx71z1zD9oQXMXVGI6epZ9sf9DEZfBl88CCvf8HdrWm9vvp14HxbX+mt4m+ts8bM2mBl1aes/qy3SRkNojG9KOTXU2+S4/U+GqBR7bMjZgT20ufTlwKkja4x7vtkJbQ/UM5osClAqAGhwpgJC/5Qonr1yHM9fNY6wYCe/eGkJ5/3rG5bm7/V309qPCJzyIGROgLdusCk3OqPSfDuk2ZYcdt5UCfAsBBgwE6J6tP6z2sIZZBcGbPy07SW6NnxsC8aPuuTAsf1Dm2+17drtYccKeOt6+PIhf7fE2rPJ/ptp65Am2PmECf103pkKGBqcqYAysX8Sc28+kf/5yTA2l1Rx1uNf8ctXctlSUunvprWPoBA4/wU7NDPrIptQ09eMgdVvw/8dB+//1vbY+NLera1fDODhTc/Z2jnuhQBXtu2z2qrvNJv2o2hN266T+yJEJEO/kw4c2z+0+XbgDW0uecHuN84LjNqx+/ObTfLN9TLH2zJOgfCzqW5PgzMVcJwO4cJxPfns9sncOKUvH6/ewdQHP+ePb62kqLza383zvchkuPBlO8n8v5faHiJfyf8enj4ZXr3MlpD67l/w8nmwb6/vPsPTc9YW0RlQVQx1R7i/noUA2VPa9llt1XeaHVr9/onWX6OiCH74wKYAOTTxbiAObdbtg+WzICTKBuO78/zdIhucRaW2PvnxoTLH2eA/EH421e1pcKYCVqQriF+fPIAFt0/h/GMyefn7rUx64DMe/GgdZV2tXmfqCDjrcfsLee6v2/7X++48ePVy+M90W1fy9Mfg5mVw5uN2hdt/pvvml1BNuc1z5oueMzj8ooCSjbDpcztHz+Fs22e1VXSanS+4+FkobGUhlOX/tRPQmw5pevQ/OfCGNte8a4P7k/5qn2/0wZy7tjDG/jvufWLbhtOb8hRB13lnKgD4JTgTkRkisk5ENojIHc28foWI7BKRpe7tGn+0UwWG5OhQ7j17GJ/cOomcQcn8Y94GJj0wn6e+yKO6rgut7Bx6Dpx4Gyx5Hr5/snXXqNoNH9wJ/zsO1n8Ek++EXy6x9SCdQTYYuOwtqNwFT06FLV+3rc173Ss129xzdpTgbMlz7oUAzQQz/jD5DjtP6f3ftjyQNsYOEWaMg6QBP37dFWV759YE0KrNJc9DXG8YfTnE9oKNPlqt2lq71kFlkW/mm3kk9reLPTQ4UwGgw4MzEXECjwMzgcHAhSLS3Dro/xpjRrq3pzq0kSog9UmM4PGLRvPujScwND2Ge+asYeqDn/PaonwaGrvIPJEpf4D+M+GDO2y+q7JC7+aI1VXDV4/BoyPt0OXIC+GmXBtEHFpTsvcJcM2ndp7bc2dA7kutb29b02h4eMrmNDfvrL7WtnHATIhObdvn+EpYHOT8EbZ+DavebNl7CxZB8ToYfYQVp0POtvPaAiFQKNloSySNdqcvyc6xQ4oNfuy93j/fzIfBmcNhV23qogAVAPyRj2wcsMEYkwcgIrOAM4HVfmiL6oSGZcTwwtXj+XJ9Mfd/sJbbX1/Ok1/k8euTBjB9cAriq2EOf3A44CdP2GHH166wx8QB4Yl2hWJUD4hMafK4hx1u+vw+Oxeo73SYfjekHCXvU0I2XPOxHfp8+xdQsh5y7mp5SoL91QHaGJxFu/NWN7dic90cOx+toysCHM3oy2DR0/DRXTagDgn37n25L9hyQUPOPvw5/U8Gp8sG6L2O9U17Wyv3BXcutovs8+wcWPyMDTL91bbNC+y/ubhevr1u5niY/zf731RojG+vrVQL+CM4SwfymzwvAMY3c945IjIR+AH4lTEm/9ATRORa4FqAnj3b+MtBdTon9EvkuOzjeX/lDv7fR+u49oXFDE6N5pc5fTl5SA8cjk4apIVGw9Uf2Tk1FTugfKd7794Kl9lhSdNkyKvHMLj0rZZNlg+Lg0vegLm3w5cP22z1P3nCFgr3Vmk+OEPsqsO2CA6zPXnN9ZwtesYOmwZaoXiHE2beD8/MhK8egSm/O/p7aith5Zs2MHNFHf48VxT0m26HNmfc55+Eu2B7x5a+bINFT69ln4n2D4aN8/wTnDU22v82Bp3m+2tnjgOMDTz7TvX99ZXyUqBm8n8XeMUYUyMiPweeA370zWyMeQJ4AmDs2LFdZFxLtYTDIZw6PJWThqQwO3cb/zd/A9e/tIT+KZHcMKUvpw1Pw9kZg7TQmCP/8mmotwFaxQ47pJk5vnW/wJ3BcNrDdu7Th7+Dp2fARf890JN1NHvz7ZCkL4KH6HQo237wMc9CgCl/8P9CgOb0Os7OFfzqURh58dF7cla/DbXl3s2dG3K2rSOa/53/eqh++NDmYht92YFjYbGQPtYGZzm/7/g27VwB1Xt9l0KjqfQxNvDM/16DM+VX/vhzbBvQdPZwhvvYfsaYEmOMJ5/AU8CYDmqb6qSCnQ5+OjaTT26dxKMXjMQYuHnWUqY9ZOek1TUEyMRqX3EG2Z6MtFH2F3dbgiMRmHA9XDjLruB8Mge2fufdRHdfpNHwiMn48YKAJc+7FwJc7JvPaA/T7wYEPvrD0c/NfRHis6GnF8GWZ2hz9Vuta9dXj8Fn97fuvR5LnrfpKvpOP/h4do5va4y2hKeeZluKnR+OKwqShwTGXD/VrfkjOFsI9BORPiISAlwAvNP0BBFpOuv3DKCN2R5VdxHkdHDmyHQ+vGUi/7x4NGHBTm5/fTlT/t9nvPTdlu5Rt7O1+p9sh1MdwfD0SfDIMHjnl3be0+F+Ce/Nb3saDY/o9IOLn9fX2ooA/Wd435PnDzEZcOKtdgjSM1G9OSUbYctXttfMm3mRnqHN1iSkXfcBfPxH+OxvrV9ZWbrNVjEYebH9Y6Cp7Bw7rH6kn7e9bFpgs/m31+KQzHF2WLNRvyuU/3R4cGaMqQduBD7EBl2vGmNWicjdInKG+7SbRGSViCwDbgKu6Oh2qs7N4RBmDktlzk0n8J/Lx5IQ6eL3s1cy+e+f8exXmyjd18XypPlKyhD4+ed2qDNtJKx62y5MeCDL9qjNu8em32ios8OpFTtsYlhfiEm3w1W17moQ6+bYodtAWwjQnON+aSeoH6kCQ+6LdshsxIXeX3fwWS1ftVlWaBd5pAyD+CyYc9uRk/seztKXbQDW3BBs+hhwRXd8vrOGevvvz5erNA+VOd4OPbe1AoRSbeCXOWfGmLnA3EOO3dXk8Z3AnR3dLtX1iAhTB6WQMzCZLzcU849PN/Dnd1dzz5w1TMhKYNqgZKYOSiEz3suVdt1BeDyMvcpuDfV2+GrjPLt98RAs+LstdJ4+2p7vs56zJuk0kvq7KwJkdo65P8FhcNK98OqldiXjuJ8d/LqnyHnf6S3r8Rkw48DQpjfzzhobYfbPbUb/c5+2q19fONsu+JjSgq/UxkbIfd7O64rv8+PXnUE2QNo43w5/d9QK6cKlNnDq0w5Dmh6Z4+y+4HvoMbT9PkepI9AKAapbEBFO7JfEq9cdy+xfHMc1J2axo6yaP7+7mhMfmM+MRxbw0EfrWF6wl8aukjPNF5xB9pfV5DvskOdv8uD8F2H4T2HPFntOj2G++az9VQIK7Ny3vM8CoyKAtwadbgOWeff8eBh44zzbA3ak3GbNaenQ5teP2gUUM++3AW52Dgw91xYrL17v/edu+symSWm6EOBQ2VOgdKsdrm2tko3w4rmw/DXvfr5Nn9t9e8w384jrbVcfa74z5UeBulpTqXYzqmcco3rGccfMgWwqruST1Tv5eM1O/nf+Bh6bt4GUaBfTBqUwbXAKx2Un4ArqJMFBRwiLtUHIoNPt89oq7/N7HY2nSkDpNjuvSByBUxHAGyIw43741wkw/1449cEDr+U+b3PV9Tu55dcdfJZdtVnwPfSccPjzChbbwHDwWTCqSRB48t9g/ccw51a47B3vermWPG9TrQw8wophT2qTjfMgsZX1Lb982M5r2/CxfZzzB5ts+HBt3LQAUoZCRGLrPs8bIvYPEl0UoPxIe85Ut9YnMYKfTczi1Z8fy+I/TOfB80Ywumccs3O3ceUzCzn+vvk889WmrlUmypd8FZiBe9K/2FqguS8G/kKA5qQMhmOutslpd6y0xyqLYd37tsh5UEjLr+kZ2lw1+/DnVJfBG1fZlZWnP3pwcBOVAtPusoHN8leP/nmVJbDmPTs3Ljj08OfFZ9leptbOO6vaDStesyWhzvkP1O+DWRfCU9Mg7/Mfn19fA1u/bd/5Zh6Z42zvbcWu9v8spZqhwZlSbnERIZwzJoN/XjKGJX+czjNXHEO/5Ej+8u7q/as9a+u7WEqOQOIMttUPcl/oPAsBmjP5TgiNtSW4jDlykXNveDO0Oec2Owx5zlO2d/NQY66yuck+/J0tVH8ky2dBY93BvW+Hk51jSzu1ppTTkuegvhrGXwfDzoUbvofTH7PDv8+fYUuLFSw6cH7BInt+ew5peniKoBfo0KbyDw3OlGpGaLCTKQOTeeXaCbx8zXjSYsP4/eyVTH3oM15blE99V8ubFihi0m3S0+gMW/y7MwqPt8Nzm7+wE/mXvGADo+RBrb+mZ9Vmc8HCslmw4lWYdMfhhz0dDrsCd98e+OTPh/8cY+yQZsYxRy8BBjY4q62AgoXe/BQHNNTD90/ZXjDP5ziDYczl8MslcPL/wM5V8NRUeOUi+9gz1N3ruJZ9VmukjrQpZXRoU/mJBmdKHcVxfRN5/bpjeebKY4gNC+H215dz0sMLeHvpNl084GueeWedaSFAc8ZcYVNZvHsz7FrT9rlz+4c23zr4eMlG22vW63iY+OsjXyN1uE02vPhZm2S4OQULYdfaIy8EaKr3iTZJcEuHNtfNsQs/xl/349eCQ+HYX8DNy9xB7pfwz+Ph2/+zQVNzPYO+FhxqU8nktzDoVMpHNDhTygsiwpQBybxz4/H8+9IxhAQ5uHnWUmY++gUfrNyB8Sabvjq6uF6dbyFAczx1N6tLISgMhv6kbddzRdmexNVvHRjarK+FN64GR5CtiepNMDv5ThsAv/er5ociFz9n06QM8bK9YbGQMbblwdl3T9i8cP1nHP4cVyRMvB1uXgon3GLbO/DUln1OW2SOt2lk6ms77jOVctPgTKkWEBFOHtKDuTedyD8uHEVdYyPXvbiYUx/7kme/2kRReSuSfaoDjrsJLn/vQFqNzqz38TY57Ym32VqpbTXk7IOHNuf9Fbbnwhn/sFUKvOGKhJkPQNEq2xPVVHUZrHrT1gp1RXrfruwc2NaCUk47VsCWL+GYn3kXUIbHw7Q/wx1b4YRbvW9XW2UcY+e47VjRcZ+plJsGZ0q1gsMhnD4ijY9umciD542g0Rj+/O5qJvztUy584lte/m4reyr1L+4Wi0y2QU1XcdI9MOl231yr6dDmhk/h68dgzJUw+IyjvvUgg06DAafAZ/fZRQQeK9+Auiq7erIlsnMAcyAH2dF8928IDm95zregkLbVkG0pz6IAnXem/ECDM6XaIMjp4JwxGXxwy0Q+uXUiv8zpx86yan43ewXH3PsJVzzzPW8sLqCsWstFqTbyDG2uehNmXwdJA20Os9aY6S6IPvf2AwXulzxvi357Kj94K200uGK8G9r0pM8Yfr7NoxbIolNtaTINzpQfaBJapXykb3IUv5oexS3T+rG6sIx3lxXy7rLt3PbaMkJmO5gyIInTR6QxZUAyES79T0+1wpCz7GR6pwsue6v1eeZie9r5Zx//0Sa4jett51fNuL/lpZicQZDlZSknT/qMcde2rt0dLXMcbP3G361Q3ZD+hlDKx0SEIWkxDEmL4bczBpCbv5d3l21nzvJCPly1kxCng/FZ8eQMTGbqwBR6JmhdT+Wl/jNs8tfjb7FF6ttiwvU2Dcfc39hSTE6XLcvVGtk5sOZdKNkAif2aP6e59BmBLnM8rHwdSgu8n9enlA9ocKZUOxIRRveMY3TPOP5w6mAWbt7Np2t2Mm9tEX95dzV/eXc1fZMjyRmYTM7AZMb0iiPYqbMN1GGERsNNub65ljMYTn8E/jMdlr4Ew86zk+9b46BSTocJzjzpM055oHWf4Q+Zx9h9/ncanKkOpcGZUh3E6RAmZCUwISuB3586mM3FlcxbW8T8dUU889UmnliQR1RoEJP6JzF1UDJTBiQTG96Kcj9KeStznM3JtvhZ73ObNSeut+3R2zgPxv+8+XO++/fR02cEmpShdvFC/vd2FatSHUSDM6X8pHdiBFed0IerTuhDRU09X67fxby1Rcxbu4v3lhcS4nQwfUgKFx7Tk+OyE3A4WjgXSClvnPw/NmBqa1mk7BxY+orNC3ZoDdEdK2DLVzD9r50rubAzGNLH6KIA1eE0OFMqAES6gpgxNJUZQ1NpbDQs31bK20u3MTt3G3OWF5IRF8b5YzM5d2wGqTFh/m6u6kpCwmHAzLZfJzsHFj5l87D1PuHg11qbPiMQZI6Drx6F2qrWL8BQqoV0cotSAcbhEEZmxvKn04fw7Z1TeezCUfSMD+fBj3/g+PvmcfWzC/lo1Q7qtL6nCiSHK+XUmdJnNCdzvC1cv91Hc/2U8oL2nCkVwEKDnZwxIo0zRqSxpaSSVxfl89qiAj5dW0RSlItzx2Rw3pgM+iRGIC1NgaCUL4VG216mjfNg6l0HjnvSZxxuLlqgy/AsCvi2ayVIVgFNukpNwLFjx5pFixb5uxlKtbv6hkY+W7eLWQvzmb+uiIZGQ0JECIPTohmSFuPeR9MnIULnqamO9fkDMP9v8Js8u/KzoR4eHQEJWXD5u/5uXes9MdnWSb3hezsPTSkfEJHFxpixzb2mPWdKdTJBTgfTBqcwbXAKO8uq+WDlDlZuK2XV9jL+82UedQ32D67wECeDUm2gNjjVBm79e0TiCupEE7JV55KdA/PvhbzPbLH3zpg+ozmT7oBXzofcF2DsVf5ujeoG/BKcicgM4FHACTxljLnvMOedA7wOHGOM0W4xpQ6REh3K5cf13v+8tr6R9UXlrNpexurtZazaXsobiwt4vrYBgBCng0GpUQzPiGVEZiwjMmLISorEqT1syhfSRtki7xvn2eCsM6bPaE7/kyFzAnx2Pwy/QBcGqHbX4cGZiDiBx4HpQAGwUETeMcasPuS8KOBmQNcwK+WlkCDH/uoEHo2Nhq27q1i5vZQVBaUsK9jLm0sKeOHbLQBEhDgZlhHDCHfANjwjhvTYMJ3DplrO4YSsybaUkyd9xkn3dK70Gc0RgWl/hmdmwPf/hhN+5e8WqS7OHz1n44ANxpg8ABGZBZwJrD7kvL8C9wO3d2zzlOpaHA6hd2IEvRMjOG14GgANjYa8XRUsKyhlecFeluXv5ZmvNlPrXgGaERfGxeN7ccExmcRFaCJc1QLZObD6bVsWKjgcRl3i7xb5Rq9jod/J8OXDNnFvZ1x5qjoNfwRn6UB+k+cFwPimJ4jIaCDTGDNHRA4bnInItcC1AD179myHpirVNTkdQr+UKPqlRHHuGFuWpqa+gbWF5Swv2MvcFTu4/4O1PPLJD5wxIo3Lj+vN0PSYo1xVKSBrit1v/RrGXNm1gpipd8G/ToCvHoNpf/J3a1QXFnALAkTEATwEXHG0c40xTwBPgF2t2b4tU6prcwU57Ty0zFguPbY3P+ws57mvN/Pmkm28triAMb3iuOzYXswcmkpIkKZIVIcR1wvis2H3xs6bPuNwegy1NUi//af92aJ6+LtFqovyxzfsNiCzyfMM9zGPKGAo8JmIbAYmAO+ISLPLTZVS7aN/ShT3nj2Mb383lbtOG0xJRQ03z1rK8ffP4+GPf2BnWbW/m6gC1YTrYdy1kDzI3y3xvSl3QmOdTRuiVDvp8DxnIhIE/ABMxQZlC4GLjDGrDnP+Z8Cvj7ZaU/OcKdW+GhsNC9bv4vlvtjB/XRFOEU4aksLQ9Bgy48LJiAsjMz6chIgQXUygurY5t9li8TcutAXflWqFgMpzZoypF5EbgQ+xqTSeNsasEpG7gUXGmHc6uk1KqaNzOITJA5KZPCCZzcWVvPjtFt5aup25K3YcdF54iJOMuDAy4sLJdAdsGXFhjOkVT1KUy0+tV8qHJt4OS1+2CXfPecrfrVFdkFYIUEq1SUVNPQV7qsjfve/g/Z59FOyuorymHrCLEHIGJnP+2EwmD0giyKnz1lQn9und8MWD8PMvIHW4v1ujOqEj9ZxpcKaUajfGGMr21bOppJL3VxbyxuJtFFfUkBTl4pzRGZw3NoPspEh/N1Opltu315amyhwHF7/m79aoTkiDM6VUQKhz1wV9dVE+89bauqDH9I7jp2MzOWVYKhGugFtArtThffkIfPInuGKuFkVXLabBmVIq4BSVV/Pmkm28ujCfvOJKIkKcnD4ijdNHpNEvJZKkSJcuLFCBrbYK/jHalqi66kNbSUApL2lwppQKWMYYFm/Zw38X5jNnRSFV7jqgUa4gspIi6JMYQVZSJFlJEWQlRtInMYKwkE5eDkh1HYuegfdugQtnwYCZ/mmDMbDufdj6DWRPgd4ngjPYP21RXtPgTCnVKVTU1LNkyx7ydlWQV1zJpuJK8nZVsm3vvoPOS4sJpXdiBHERIUS5goh0BREZGkRUaLB9HmqPRYXaLSMunNBgDehUO2iog8fHQ5ALrvuy4+uIFi6Hj34PmxYAAhhblWHAqTD4DFvrNEhXSQciDc6UUp3avtoGNu0P1mzgtrmkktJ9dVRU11NRU7+/x605Ua4gZg7rwVmj0pnQJwGHQ4eflA+tfBNevxLOfgJGnN8xn1m+E+b9FXJfhLBYmPw7GHEBbPocVr8DP3wANWXgiob+M2yg1ncaBId1TPvUUWlwppTq8uobGqmsaaC8po6Kmnoqquspr66ndF8dX24o5v0VhVTWNpAaE8oZI9P4yagMBvSI8nezVVfQ2AhPToZ9e+DGxRAU0n6fVbcPvnncFmCvr7FlpCb++sc1TOtrIO9zWPM2rJ1j2xYcDv1OgkGnQ2I/CE+EiETtWfMTDc6UUt3evtoGPl6zk7dyt/H5D7toaDQMSo3m7FFpnDkynZToUH83UXVmGz6BF8+BmQ+0T01RY2DlG/DJn6E0HwaeBtPvhoTso7+3oQ42fwlr3oE170Fl0cGvh0TZIC0i8UDA5nkcGgMh4Taw82wh4bYHrukxpw9WWjfUQ9Fq2L4Eti2Bip1gGpvZjN03Nth9cChkTYGBp0Ji/06zMEODM6WUaqK4oob3lm1n9tLtLMvfiwgcn53IqcNTGZ4RQ7/kKC3urlrGGHjudNj8hR1KjEiCyGS7RSRDZApEJtl9RLJ9HBxhe62Cw448Vy1/IXx4JxQshB7D4OS/QZ+JrWtnYwMULoWy7VBZDFXFdr//cYl7vwsa672/bnA4xPWGuD4Q79my7Bad8ePgzRjYnQfbFttAbPsSO3+u3j2/NDTGroIVJ4jDbo4mj8VhgzBxQFUJ7Fhh3xefbRdmDDwVMse3bA5g+Q4oXGbbATDpdu/f2woanCml1GHk7argraXbeSt3G1t3VwEQ7BT6JkcxODWawWnRdp8aTUy4roBTR1C+E5a+ZHt8KorsVuneV+898nsdQRAUemALdu9FbOARmQJT74IRF3bMogNjoLrUzlur2we1lXZfV+XeDjm2bw/s2Qy7N8GeTVBfffDPFtvLBmwxGfa87bn2+gBBYZA6AtJHQ9pou4/PalkPWOk2WDfXrlrdtMAWpw9PsPPtBpxiV7GGRBz42fZsskFY4TLYsdw+btqjmDEOrvm4rf8vHpEGZ0opdRTGGDbuqmRNYRmrC8tYvd3ud5XX7D8nPTaMwWnRDEqNJiM2jKQoF0lRLpKjXMRHhGhJKnV49TW2N6piJ1Tsso/r9tmeovoa9+OaA8/rq6Gu2j7PGAfH/RJcnaSaRmMjlBfaAGh3ng3YdufZrTQfYjKbBGJjIGmgb4ZFParL7DDzurmw/iMbBAaF2hQjdftsMFZTZs8VJyQPgh7DbRmuHsOhx1Dbc9fONDhTSqlWKiqvZk1h+f5gbfX2UvKKKzn0q9MhEB/hOihgS4py0ScxglGZsWQnReoqUaU6WkMdbPnaBmobPrUrW5sGYsmDbS+lH2hwppRSPlRd18Cu8hp2VdRQVGb3u8qqD35ebrf6RvsdGxUaxMjMWEZmxjKqZywjM+OIj2jHVX1KqYB2pOBMC9kppVQLhQY7yYwPJzM+/IjnNTYa8ooryd26h9z8vSzdupfH52/AHa/ROyHcHazFMTQ9mqTIUOIjQ4gIcWrpKqW6Me05U0qpDlRZU8+KbaUszd9rg7ateylqMq8NIMTpID4ihLiIEBKa7sNDiI8MoUd0KKkxdouPCNFATqlOSHvOlFIqQES4gpiQlcCErATALkQoLK1m7Y4ySipq2V1Zy+6qWnY3eZy/p4rdlbWUV/84tUFIkGN/oJYaE2b3sWGkRoeSHhdGelwY0aG6ylSpzkSDM6WU8iMRIS02jLTYo5fVqa1vZHdlLTvLqiks3UdhafWBbe8+vt+0m51l1fvnuXnEhAWTERfm3sIP2YcRpcGbUgFFgzOllOokQoIc9IgJpUdMKCMyY5s9p6HRUFJRw/bSarbv3UfBniryd9t93q5KFvxQzL66g+uQJkaGkDMwmZlDUzm+b6Im4FXKzzQ4U0qpLsTpEJKjQ0mODmVkMwGcMYbdlbUU7Nnn3qpYtb2MuSt28OqiAqJCg5g2KIUZQ3swqX8SocEdkPBUKXUQDc6UUqobERESIl0kRLoO6n2rqW/gqw3FvL9iBx+t3sns3G2EhziZMjCZU4amMnlAEhEu/ZWhVEfQ1ZpKKaUOUtfQyLd5JcxdsYOPVu2gpLIWV5CDSf2TGJwWTWKkTbCbGGmT7SZGuggL0R42pVoi4JLQisgM4FHACTxljLnvkNevA24AGoAK4FpjzOojXVODM6WU8r2GRsPCzbt5f0Uhn6wpYtvefc2eF+kKIjEyZH/QFhUatL+KgoEmjw3u/wHgECE52kVaTCg9PKtNNUWI6gYCKjgTESfwAzAdKAAWAhc2Db5EJNoYU+Z+fAbwC2PMjCNdV4MzpZRqf3UNjZRU1FLsqYLg3hc32RdX1FJRXY8IeMIrT6Dlibfsa0J9QyO7Kmqoazj4d5HLnSKkR0woaTFh9IgJpV9KJEPSYshKjNA6pqrTC7Q8Z+OADcaYPAARmQWcCewPzjyBmVsEB/7IUkop5UfBzgMrRn2lsdFQXFlD4V5PapB97CitZntpNTtK9/HdISlCQoMdDOwRzdD0aIamxTAkLYb+PSJxBenQquoa/BGcpQP5TZ4XAOMPPUlEbgBuBUKAnOYuJCLXAtcC9OzZ0+cNVUop1f4cDiE5KpTkqFBGZDZ/Tn1DI5uKK1m5vZRV28pYub2Ut3O38+K3WwEIcgj9U6IYkhZN3+RI4twVFeLCg4ltsndq8XnVCfhjWPNcYIYx5hr380uB8caYGw9z/kXAycaYy490XR3WVEqp7qWx0ZDvTgWyclspK7eXsWpbKSWVtYd9T0xY8EEBW3yEi8TIEOIj7JYY6TrosS50UO0l0IY1twFN/zbKcB87nFnAP9u1RUoppTodh0PolRBBr4QIThmWCtg8bhU19eytqmNPVS17qurYW1XLnkr7uOmxovIa1u0op7iyltr6xmY/IyzYSUJkCNdNyuaSCb068sdT3Zg/grOFQD8R6YMNyi4ALmp6goj0M8asdz89FViPUkopdRQiQlRoMFGhwWTGh3v1HmMMlbUN7K6opbiyZn9d05LKWkoqali0ZQ9/emcVQ9Njmk3sq5SvdXhwZoypF5EbgQ+xqTSeNsasEpG7gUXGmHeAG0VkGlAH7AGOOKSplFJKtZaIEOkKItIVRM+EHwd0pfvqOOXRL7hlVi5zbjpRk/GqdqdJaJVSSqmj+C6vhAue/Jafjsnk/nOH+7s5qgs40pwzTRSjlFJKHcX4rASun5TNfxfl88HKQn83R3VxGpwppZRSXrhlWn+GZ8Rwx5sr2FFa7e/mqC5MgzOllFLKCyFBDh45fyQ1dY3c9tpSGhu7xrQgFXg0OFNKKaW8lJUUyV2nD+arDSU8/dUmfzdHdVEanCmllFItcMExmUwfnMIDH6xj9fayo79BqRbS4EwppZRqARHh/nOGExMezM2zcqmua/B3k1QXo8GZUkop1ULxESE8eN4I1hdV8D9z1/i7OaqL0eBMKaWUaoWJ/ZO46vg+PPfNFuavLfJ3c1QXosGZUkop1Uq/mTGAgT2iuP31ZRRX1Pi7OaqL0OBMKaWUaqXQYCePXDCSsup6fvv6crpK1R3lXxqcKaWUUm0wsEc0d8wYyKdri3jx2y3+bo7qAjQ4U0oppdroiuN6M7F/En98exU3vLSEDUXl/m6S6sQ0OFNKKaXayOEQ/u/i0dyU05fP1hVx0sMLuPW/S9laUuXvpqlOSLrK+PjYsWPNokWL/N0MpZRS3VxJRQ3/XpDHc19vpqHRcN7YTG6a2pfUmDB/N00FEBFZbIwZ2+xrGpwppZRSvrezrJrH52/gle+3IiJcMr4X10/OJinK5e+mqQCgwZlSSinlJ/m7q/jHvPW8sWQbIU4HVxzfm59PzCI2PMTfTVN+pMGZUkop5Wd5uyp45JP1vLt8OxEhQUzsn8ix2Ykcm5VAdlIEIuLvJqoOpMGZUkopFSDW7ijj6S838eX6YraXVgOQHOXi2OwEjstO4NisRDLjwzRY6+I0OFNKKaUCjDGGrbur+HpjCd9sLOGbvBJ2ldsqA+mxYRybncCxWQmMyIylZ3w4IUGaYKEr0eBMKaWUCnDGGDbuquCbjSU2YMsrYW9VHQAOgYy4cHonRpCVGEHvBM/jSNLjwnA6tJetszlScBbU0Y0BEJEZwKOAE3jKGHPfIa/fClwD1AO7gKuMMZp2WSmlVJclIvRNjqJvchSXHtubxkbD2h3lrNtZxqbiKjYVV7K5uJLXt+yhoqZ+//uCnULP+HBSY8IwGBobodEY9wYNjQZjDA3Gvmaww6iZ8WFkxIWTGRdOZnwYmXHhxIYH63BqAOjw4ExEnMDjwHSgAFgoIu8YY1Y3OS0XGGuMqRKR64EHgPM7uq1KKaWUvzgcwuC0aAanRR903BhDcUXt/mAtz73fWV6NQwSnCCIQ7HTgcD92OgSH2M0Yw87yapbm76V0X91B1450BZER5w7a4sNIjQklMdJFQqSLhIgQEiNdxEeE6BBrO/NHz9k4YIMxJg9ARGYBZwL7gzNjzPwm538LXNKhLVRKKaUClIiQFOUiKcrFuD7xbbpWWXUd+burKNiz76B9/u4qvt5YTFVtQ7Pviw4NIjHS5Q7cQuwW4SIxykViRAiJUe5gLspFlCuoRb1xxhjqGgxOh3Tb4Vp/BGfpQH6T5wXA+COcfzXwfnMviMi1wLUAPXv29FX7lFJKqW4hOjSYIWkxDEmL+dFrxhgqauopqailpLKGXeV2X1JRS0lFDcWVtRSX17C+qIJv8mr2z487VEiQg8SIEBLcvW4GqK5roKa+kZq6BqrrGqiua6Sm3u6r6xswBoIcQlpsGBlxdsg1Iy6MjHjP43CSo1w4WhC8GWM6zZCtX+aceUtELgHGApOae90Y8wTwBNgFAR3YNKWUUqpLExGiQoOJCg2md2LEUc+va2hkT2UtuypsAFfcZF/s3u+urMXhEEKDHMSGBeOKchEa7CQ02EFosBNXkMP93EllTb3tydtTxbx1RftXsnqEOB2kx4WRFOWisdFQ29BIbX3jgX19I3VNjtU1GCJCnMRFhBDv2cLt/qBj7uHbPl78zO3FH8HZNiCzyfMM97GDiMg04PfAJGNMzaGvK6WUUipwBDsdJEeHkhwd2i7Xr65roGDPPgr2VO0P2gr27GNXWQ0hQQ4iQ4MIcToICXIc2Ac5CHY/DnYIFTUN7KmqZXdlLSUVtazfWcGeqtofDd9mJUUw77bJ7fJzeMMfwdlCoJ+I9MEGZRcAFzU9QURGAf8GZhhjijq+iUoppZQKJKHBTvomR9I3OdLn166ua2B3Ze3+zd86PDgzxtSLyI3Ah9hUGk8bY1aJyN3AImPMO8DfgUjgNff48FZjzBkd3VallFJKdX2hwU7SYsNIiw3zd1MAP805M8bMBeYecuyuJo+ndXijlFJKKaUCgCYqUUoppZQKIBqcKaWUUkoFEA3OlFJKKaUCiAZnSimllFIBRIMzpZRSSqkAosGZUkoppVQA0eBMKaWUUiqAaHCmlFJKKRVAxJiuUS9cRHYBWzrgoxKB4g74HNVyem8Cm96fwKX3JrDp/Qlcbbk3vYwxSc290GWCs44iIouMMWP93Q71Y3pvApven8Cl9yaw6f0JXO11b3RYUymllFIqgGhwppRSSikVQDQ4a7kn/N0AdVh6bwKb3p/ApfcmsOn9CVztcm90zplSSimlVADRnjOllFJKqQCiwZlSSimlVADR4MxLIjJDRNaJyAYRucPf7enuRORpESkSkZVNjsWLyMcist69j/NnG7srEckUkfkislpEVonIze7jen8CgIiEisj3IrLMfX/+4j7eR0S+c3/H/VdEQvzd1u5KRJwikisi77mf670JECKyWURWiMhSEVnkPubz7zYNzrwgIk7gcWAmMBi4UEQG+7dV3d6zwIxDjt0BfGqM6Qd86n6uOl49cJsxZjAwAbjB/d+L3p/AUAPkGGNGACOBGSIyAbgfeNgY0xfYA1ztvyZ2ezcDa5o813sTWKYYY0Y2yW/m8+82Dc68Mw7YYIzJM8bUArOAM/3cpm7NGLMA2H3I4TOB59yPnwPO6sg2KcsYU2iMWeJ+XI79JZOO3p+AYKwK99Ng92aAHOB193G9P34iIhnAqcBT7ueC3ptA5/PvNg3OvJMO5Dd5XuA+pgJLijGm0P14B5Diz8YoEJHewCjgO/T+BAz3sNlSoAj4GNgI7DXG1LtP0e84/3kE+A3Q6H6egN6bQGKAj0RksYhc6z7m8++2oLZeQKlAZIwxIqJ5YvxIRCKBN4BbjDFltgPA0vvjX8aYBmCkiMQCs4GB/m2RAhCR04AiY8xiEZns5+ao5p1gjNkmIsnAxyKytumLvvpu054z72wDMps8z3AfU4Flp4ikArj3RX5uT7clIsHYwOwlY8yb7sN6fwKMMWYvMB84FogVEc8f7Pod5x/HA2eIyGbs9Jkc4FH03gQMY8w2974I+4fNONrhu02DM+8sBPq5V8yEABcA7/i5TerH3gEudz++HHjbj23pttxzZP4DrDHGPNTkJb0/AUBEktw9ZohIGDAdOy9wPnCu+zS9P35gjLnTGJNhjOmN/T0zzxhzMXpvAoKIRIhIlOcxcBKwknb4btMKAV4SkVOwcwGcwNPGmHv926LuTUReASYDicBO4E/AW8CrQE9gC/BTY8yhiwZUOxORE4AvgBUcmDfzO+y8M70/fiYiw7GTlp3YP9BfNcbcLSJZ2N6aeCAXuMQYU+O/lnZv7mHNXxtjTtN7Exjc92G2+2kQ8LIx5l4RScDH320anCmllFJKBRAd1lRKKaWUCiAanCmllFJKBRANzpRSSimlAogGZ0oppZRSAUSDM6WUUkqpAKLBmVKqWxCRBhFZ2mTzWeF1EektIit9dT2lVPem5ZuUUt3FPmPMSH83QimljkZ7zpRS3ZqIbBaRB0RkhYh8LyJ93cd7i8g8EVkuIp+KSE/38RQRmS0iy9zbce5LOUXkSRFZJSIfubPvK6VUi2lwppTqLsIOGdY8v8lrpcaYYcD/YiuBAPwDeM4YMxx4CXjMffwx4HNjzAhgNLDKfbwf8LgxZgiwFzinXX8apVSXpRUClFLdgohUGGMimzm+GcgxxuS5C7bvMMYkiEgxkGqMqXMfLzTGJIrILiCjafkcEekNfGyM6ed+/lsg2BhzTwf8aEqpLkZ7zpRSCsxhHrdE01qHDeicXqVUK2lwppRScH6T/Tfux18DF7gfX4wt5g7wKXA9gIg4RSSmoxqplOoe9C87pVR3ESYiS5s8/8AY40mnESciy7G9Xxe6j/0SeEZEbgd2AVe6j98MPCEiV2N7yK4HCtu78Uqp7kPnnCmlujX3nLOxxphif7dFKaVAhzWVUkoppQKK9pwppZRSSgUQ7TlTSimllAogGpwppZRSSgUQDc6UUkoppQKIBmdKKaWUUgFEgzOllFJKqQDy/wFyfLMSe3ZbuQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OBSERVATION**\n",
        "\n",
        "#**1. AS WE CAN SEE FROM THE ABOVE PLOT LOSS IT GETTING DECREASED WITH INCREASING IN THE NUMBER OF EPOCHS**\n",
        "\n",
        "#**2. AS WE CAN SEE THAT ACCURACY GETTING INCREASED WITH INCREASING IN THE NUMBER OF EPOCHS**"
      ],
      "metadata": {
        "id": "LoLfbnJbPdJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL PERFORMENCE**"
      ],
      "metadata": {
        "id": "kK02wdKCQinH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = ['Model', 'Epochs', 'Train Accuracy', 'Test Accuracy']\n",
        "table.add_row(['MODEL WITH DENSE LAYER ', 50, 0.9135, 0.8768])\n",
        "table.add_row(['MODEL WITH-OUT DENSE LAYER', 50, 0.9335,  0.8926])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFCcWRniMcD7",
        "outputId": "db72a1a0-3da6-4ce6-9a1b-7f07f16a38e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+--------+----------------+---------------+\n",
            "|           Model            | Epochs | Train Accuracy | Test Accuracy |\n",
            "+----------------------------+--------+----------------+---------------+\n",
            "|  MODEL WITH DENSE LAYER    |   50   |     0.9135     |     0.8768    |\n",
            "| MODEL WITH-OUT DENSE LAYER |   50   |     0.9335     |     0.8926    |\n",
            "+----------------------------+--------+----------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# i think performence can further improve by increasing the number of epochs\n",
        "# but i dont have computational power with gpu so i limited till 50"
      ],
      "metadata": {
        "id": "4W100L-_Fg6C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duU-Z3W6QhKR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}